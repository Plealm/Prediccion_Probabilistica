{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b99d120",
   "metadata": {},
   "source": [
    "# Analisis Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b278203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS COMPREHENSIVO DE MODELOS DE PREDICCI√ìN PROBABIL√çSTICA\n",
      "================================================================================\n",
      "\n",
      "Cargando datos...\n",
      "‚úì Estacionario: 1320 filas\n",
      "  Columnas: ['Paso', 'Valores de AR', 'Valores MA', 'Distribuci√≥n', 'Varianza error', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario']\n",
      "‚úì No Estacionario: 840 filas\n",
      "  Columnas: ['Paso', 'Tipo de Modelo', 'Valores de AR', 'Valores MA', 'Distribuci√≥n', 'Varianza error', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario']\n",
      "‚úì No Lineal: 840 filas\n",
      "  Columnas: ['Paso', 'Tipo de Modelo', 'Distribuci√≥n', 'Varianza error', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario']\n",
      "‚úì Tipos de datos convertidos\n",
      "‚úì Filas despu√©s de limpieza: 2600\n",
      "\n",
      "‚úì Datos combinados: 2600 observaciones totales\n",
      "‚úì Columnas finales: ['Paso', 'Valores de AR', 'Valores MA', 'Distribuci√≥n', 'Varianza', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario', 'Tipo de Modelo']\n",
      "\n",
      "================================================================================\n",
      "INICIANDO AN√ÅLISIS COMPREHENSIVO DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "1. Analizando caracter√≠sticas del proceso generador...\n",
      "  - Analizando efecto de estacionaridad...\n",
      "  - Analizando efecto de no linealidad...\n",
      "  - Analizando efecto del tipo de modelo...\n",
      "\n",
      "2. Analizando efecto de distribuciones...\n",
      "  - Analizando efectos de distribuciones...\n",
      "  - Analizando efectos de varianza...\n",
      "\n",
      "3. Analizando horizonte de predicci√≥n...\n",
      "  - Analizando deterioro por horizonte...\n",
      "  - Analizando consistencia de ranking...\n",
      "\n",
      "4. Analizando interacciones complejas...\n",
      "  - Analizando interacciones Escenario √ó Distribuci√≥n...\n",
      "  - Analizando interacci√≥n triple...\n",
      "\n",
      "5. Analizando robustez y estabilidad...\n",
      "  - Calculando m√©tricas de robustez...\n",
      "  - Identificando peores casos...\n",
      "\n",
      "6. Realizando tests de Diebold-Mariano...\n",
      "  - Realizando tests de Diebold-Mariano...\n",
      "  - Realizando tests pareados de Diebold-Mariano...\n",
      "  - Creando matriz de p-valores...\n",
      "  - Analizando dominancia estad√≠stica...\n",
      "  - Analizando DM por escenario...\n",
      "\n",
      "7. Generando perfiles por modelo...\n",
      "  - Generando perfiles individuales...\n",
      "    > Analizando AREPD...\n",
      "    > Analizando AV-MCPS...\n",
      "    > Analizando Block Bootstrapping...\n",
      "    > Analizando DeepAR...\n",
      "    > Analizando EnCQR-LSTM...\n",
      "    > Analizando LSPM...\n",
      "    > Analizando LSPMW...\n",
      "    > Analizando MCPS...\n",
      "    > Analizando Sieve Bootstrap...\n",
      "\n",
      "8. Generando recomendaciones...\n",
      "  - Generando recomendaciones estrat√©gicas...\n",
      "================================================================================\n",
      "RECOMENDACIONES Y CONCLUSIONES\n",
      "================================================================================\n",
      "\n",
      "1. MODELO CAMPE√ìN GENERAL\n",
      "----------------------------------------\n",
      "Mejor rendimiento promedio: Block Bootstrapping\n",
      "ECRPS: 0.557547\n",
      "Desviaci√≥n Est√°ndar: 0.293355\n",
      "\n",
      "Peor rendimiento promedio: AREPD\n",
      "ECRPS: 3.083010\n",
      "\n",
      "2. RECOMENDACIONES POR ESCENARIO\n",
      "----------------------------------------\n",
      "\n",
      "Estacionario_Lineal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.539612\n",
      "\n",
      "No_Estacionario_Lineal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.542499\n",
      "\n",
      "No_Lineal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.603341\n",
      "\n",
      "3. RECOMENDACIONES POR DISTRIBUCI√ìN DE ERRORES\n",
      "----------------------------------------\n",
      "\n",
      "Distribuci√≥n normal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.567834\n",
      "\n",
      "Distribuci√≥n uniform:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.585653\n",
      "\n",
      "Distribuci√≥n exponential:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.514920\n",
      "\n",
      "Distribuci√≥n t-student:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.547263\n",
      "\n",
      "Distribuci√≥n mixture:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.572066\n",
      "\n",
      "4. MODELOS M√ÅS ROBUSTOS (MENOR VARIABILIDAD)\n",
      "----------------------------------------\n",
      "1. Block Bootstrapping: CV = 0.5262\n",
      "2. LSPMW: CV = 0.7688\n",
      "3. LSPM: CV = 0.7896\n",
      "\n",
      "5. RECOMENDACIONES POR HORIZONTE DE PREDICCI√ìN\n",
      "----------------------------------------\n",
      "\n",
      "Paso 1.0:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.548020\n",
      "\n",
      "Paso 3.0:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.560285\n",
      "\n",
      "Paso 5.0:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.554843\n",
      "\n",
      "6. ESTRATEGIA DE ENSAMBLE SUGERIDA\n",
      "----------------------------------------\n",
      "Combinar los siguientes modelos:\n",
      "1. Block Bootstrapping (ECRPS: 0.557547)\n",
      "2. Sieve Bootstrap (ECRPS: 0.614997)\n",
      "3. LSPM (ECRPS: 0.811114)\n",
      "\n",
      "Justificaci√≥n:\n",
      "  - Estos modelos muestran el mejor rendimiento promedio\n",
      "  - Un ensamble puede capturar fortalezas complementarias\n",
      "  - Reduce el riesgo de seleccionar un modelo sub√≥ptimo\n",
      "\n",
      "7. MODELOS CON DOMINANCIA ESTAD√çSTICA\n",
      "----------------------------------------\n",
      "Modelos estad√≠sticamente superiores (test Diebold-Mariano):\n",
      "1. Block Bootstrapping: 7 victorias significativas\n",
      "2. LSPM: 6 victorias significativas\n",
      "3. LSPMW: 5 victorias significativas\n",
      "4. Sieve Bootstrap: 5 victorias significativas\n",
      "5. DeepAR: 3 victorias significativas\n",
      "\n",
      "8. REGLAS DE DECISI√ìN SUGERIDAS\n",
      "----------------------------------------\n",
      "\n",
      "SI el proceso es ESTACIONARIO y LINEAL:\n",
      "  ‚Üí Primera opci√≥n: Block Bootstrapping\n",
      "  ‚Üí Segunda opci√≥n: Sieve Bootstrap\n",
      "\n",
      "SI el proceso es NO ESTACIONARIO y LINEAL:\n",
      "  ‚Üí Primera opci√≥n: Block Bootstrapping\n",
      "  ‚Üí Segunda opci√≥n: Sieve Bootstrap\n",
      "\n",
      "SI el proceso es NO LINEAL:\n",
      "  ‚Üí Primera opci√≥n: Block Bootstrapping\n",
      "  ‚Üí Segunda opci√≥n: DeepAR\n",
      "\n",
      "SI la distribuci√≥n de errores:\n",
      "  ‚Ä¢ Es normal ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es uniform ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es exponential ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es t-student ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es mixture ‚Üí Usar Block Bootstrapping\n",
      "\n",
      "SI el nivel de varianza:\n",
      "  ‚Ä¢ Es bajo (0.2) ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es alto (3.0) ‚Üí Usar Block Bootstrapping\n",
      "\n",
      "9. CONCLUSIONES PRINCIPALES\n",
      "----------------------------------------\n",
      "\n",
      "‚Ä¢ El modelo Block Bootstrapping muestra el mejor rendimiento general\n",
      "  con ECRPS promedio de 0.557547\n",
      "\n",
      "‚Ä¢ El modelo m√°s robusto (menor CV) es Block Bootstrapping\n",
      "\n",
      "‚Ä¢ Block Bootstrapping es consistentemente superior en procesos\n",
      "  estacionarios y no estacionarios\n",
      "\n",
      "‚Ä¢ Para procesos no lineales: Block Bootstrapping es la mejor opci√≥n\n",
      "\n",
      "‚Ä¢ Se recomienda implementar un ENSAMBLE de los top 3 modelos\n",
      "  para maximizar robustez y rendimiento\n",
      "\n",
      "10. CONSIDERACIONES PR√ÅCTICAS\n",
      "----------------------------------------\n",
      "\n",
      "Factores a considerar en la selecci√≥n:\n",
      "  1. Costo computacional vs ganancia en precisi√≥n\n",
      "  2. Robustez ante cambios en la distribuci√≥n de errores\n",
      "  3. Consistencia a trav√©s de horizontes de predicci√≥n\n",
      "  4. Facilidad de interpretaci√≥n y explicabilidad\n",
      "  5. Disponibilidad de recursos para implementaci√≥n\n",
      "\n",
      "Trade-offs identificados:\n",
      "  ‚Ä¢ Algunos modelos son especialistas en escenarios espec√≠ficos\n",
      "  ‚Ä¢ Otros modelos son generalistas con buen rendimiento global\n",
      "\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS COMPLETO. Resultados guardados en: resultados_analisis_completo/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "‚úì An√°lisis completado exitosamente\n",
      "‚úì Todos los resultados guardados en: resultados_analisis_completo/\n",
      "================================================================================\n",
      "\n",
      "Archivos generados:\n",
      "  üìä An√°lisis de caracter√≠sticas del DGP\n",
      "  üìà Efectos de distribuci√≥n y varianza\n",
      "  üéØ An√°lisis de horizonte de predicci√≥n\n",
      "  üîÑ Interacciones complejas\n",
      "  üí™ M√©tricas de robustez\n",
      "  üìâ Tests de Diebold-Mariano\n",
      "  üë§ Perfiles individuales por modelo\n",
      "  üí° Recomendaciones estrat√©gicas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DieboldMarianoTest:\n",
    "    \"\"\"\n",
    "    Implementaci√≥n del test de Diebold-Mariano para comparar pron√≥sticos\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def dm_test(errors1, errors2, h=1, crit=\"MSE\", power=2):\n",
    "        \"\"\"\n",
    "        Realiza el test de Diebold-Mariano\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        errors1 : array-like\n",
    "            Errores del primer modelo\n",
    "        errors2 : array-like\n",
    "            Errores del segundo modelo\n",
    "        h : int\n",
    "            Horizonte de predicci√≥n (para ajustar autocorrelaci√≥n)\n",
    "        crit : str\n",
    "            Criterio de p√©rdida: \"MSE\", \"MAE\", \"MAPE\"\n",
    "        power : int\n",
    "            Potencia para la funci√≥n de p√©rdida\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dm_stat : float\n",
    "            Estad√≠stico DM\n",
    "        p_value : float\n",
    "            P-valor (two-tailed)\n",
    "        \"\"\"\n",
    "        errors1 = np.array(errors1)\n",
    "        errors2 = np.array(errors2)\n",
    "        \n",
    "        # Calcular diferencias de p√©rdida\n",
    "        if crit == \"MSE\":\n",
    "            loss_diff = errors1**2 - errors2**2\n",
    "        elif crit == \"MAE\":\n",
    "            loss_diff = np.abs(errors1) - np.abs(errors2)\n",
    "        elif crit == \"MAPE\":\n",
    "            loss_diff = np.abs(errors1) - np.abs(errors2)\n",
    "        else:\n",
    "            loss_diff = errors1**power - errors2**power\n",
    "        \n",
    "        # Media de las diferencias\n",
    "        mean_diff = np.mean(loss_diff)\n",
    "        \n",
    "        # Varianza de las diferencias (ajustada por autocorrelaci√≥n)\n",
    "        n = len(loss_diff)\n",
    "        \n",
    "        # Calcular varianza con correcci√≥n de Newey-West\n",
    "        gamma0 = np.var(loss_diff, ddof=1)\n",
    "        \n",
    "        if h > 1:\n",
    "            gamma_sum = 0\n",
    "            for k in range(1, h):\n",
    "                gamma_k = np.cov(loss_diff[:-k], loss_diff[k:])[0, 1]\n",
    "                gamma_sum += (1 - k/h) * gamma_k\n",
    "            variance = (gamma0 + 2 * gamma_sum) / n\n",
    "        else:\n",
    "            variance = gamma0 / n\n",
    "        \n",
    "        # Estad√≠stico DM\n",
    "        dm_stat = mean_diff / np.sqrt(variance) if variance > 0 else 0\n",
    "        \n",
    "        # P-valor (two-tailed)\n",
    "        p_value = 2 * (1 - stats.norm.cdf(np.abs(dm_stat)))\n",
    "        \n",
    "        return dm_stat, p_value\n",
    "\n",
    "\n",
    "class ModelPerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    Clase para an√°lisis exhaustivo de rendimiento de modelos de predicci√≥n\n",
    "    en diferentes escenarios de simulaci√≥n.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el analizador cargando los datos de los tres escenarios.\n",
    "        \"\"\"\n",
    "        self.models = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "                      'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "        \n",
    "        # Cargar datos con las rutas especificadas\n",
    "        print(\"Cargando datos...\")\n",
    "        \n",
    "        try:\n",
    "            self.df_estacionario = pd.read_excel(\"./Datos/estacionario.xlsx\")\n",
    "            self.df_estacionario['Escenario'] = 'Estacionario_Lineal'\n",
    "            print(f\"‚úì Estacionario: {len(self.df_estacionario)} filas\")\n",
    "            print(f\"  Columnas: {self.df_estacionario.columns.tolist()}\")\n",
    "            \n",
    "            self.df_no_estacionario = pd.read_excel(\"./Datos/no_estacionario.xlsx\")\n",
    "            self.df_no_estacionario['Escenario'] = 'No_Estacionario_Lineal'\n",
    "            print(f\"‚úì No Estacionario: {len(self.df_no_estacionario)} filas\")\n",
    "            print(f\"  Columnas: {self.df_no_estacionario.columns.tolist()}\")\n",
    "            \n",
    "            self.df_no_lineal = pd.read_excel(\"./Datos/no_lineal.xlsx\")\n",
    "            self.df_no_lineal['Escenario'] = 'No_Lineal'\n",
    "            print(f\"‚úì No Lineal: {len(self.df_no_lineal)} filas\")\n",
    "            print(f\"  Columnas: {self.df_no_lineal.columns.tolist()}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"ERROR: No se encontr√≥ el archivo - {e}\")\n",
    "            print(\"Verifica que los archivos est√©n en la carpeta './Datos/'\")\n",
    "            raise\n",
    "        \n",
    "        # Estandarizar nombres de columnas\n",
    "        self._standardize_columns()\n",
    "        \n",
    "        # Combinar todos los datos\n",
    "        self.df_all = pd.concat([self.df_estacionario, self.df_no_estacionario, \n",
    "                                 self.df_no_lineal], ignore_index=True)\n",
    "        \n",
    "        # Convertir tipos de datos cr√≠ticos\n",
    "        self._convert_data_types()\n",
    "        \n",
    "        print(f\"\\n‚úì Datos combinados: {len(self.df_all)} observaciones totales\")\n",
    "        print(f\"‚úì Columnas finales: {self.df_all.columns.tolist()}\")\n",
    "        \n",
    "    def _standardize_columns(self):\n",
    "        \"\"\"Estandariza nombres de columnas entre datasets\"\"\"\n",
    "        # Para estacionario\n",
    "        if 'Varianza error' in self.df_estacionario.columns:\n",
    "            self.df_estacionario.rename(columns={'Varianza error': 'Varianza'}, inplace=True)\n",
    "        \n",
    "        # Agregar columna 'Tipo de Modelo' si no existe en estacionario\n",
    "        if 'Tipo de Modelo' not in self.df_estacionario.columns:\n",
    "            # Crear tipo de modelo basado en valores AR y MA\n",
    "            def create_model_type(row):\n",
    "                ar_vals = row.get('Valores de AR', '')\n",
    "                ma_vals = row.get('Valores MA', '')\n",
    "                \n",
    "                ar_str = str(ar_vals) if pd.notna(ar_vals) else ''\n",
    "                ma_str = str(ma_vals) if pd.notna(ma_vals) else ''\n",
    "                \n",
    "                # Contar √≥rdenes\n",
    "                ar_order = len([x for x in ar_str.split(',') if x.strip() and x.strip() != '[]']) if ar_str else 0\n",
    "                ma_order = len([x for x in ma_str.split(',') if x.strip() and x.strip() != '[]']) if ma_str else 0\n",
    "                \n",
    "                if ar_order > 0 and ma_order > 0:\n",
    "                    return f'ARMA({ar_order},{ma_order})'\n",
    "                elif ar_order > 0:\n",
    "                    return f'AR({ar_order})'\n",
    "                elif ma_order > 0:\n",
    "                    return f'MA({ma_order})'\n",
    "                else:\n",
    "                    return 'Unknown'\n",
    "            \n",
    "            self.df_estacionario['Tipo de Modelo'] = self.df_estacionario.apply(create_model_type, axis=1)\n",
    "        \n",
    "        # Para no estacionario\n",
    "        if 'Varianza error' in self.df_no_estacionario.columns:\n",
    "            self.df_no_estacionario.rename(columns={'Varianza error': 'Varianza'}, inplace=True)\n",
    "        \n",
    "        # Para no lineal\n",
    "        if 'Varianza error' in self.df_no_lineal.columns:\n",
    "            self.df_no_lineal.rename(columns={'Varianza error': 'Varianza'}, inplace=True)\n",
    "    \n",
    "    def _convert_data_types(self):\n",
    "        \"\"\"Convierte tipos de datos para evitar errores de comparaci√≥n\"\"\"\n",
    "        # Convertir 'Paso' a num√©rico\n",
    "        self.df_all['Paso'] = pd.to_numeric(self.df_all['Paso'], errors='coerce')\n",
    "        \n",
    "        # Convertir 'Varianza' a num√©rico\n",
    "        self.df_all['Varianza'] = pd.to_numeric(self.df_all['Varianza'], errors='coerce')\n",
    "        \n",
    "        # Convertir columnas de modelos a num√©rico\n",
    "        for model in self.models:\n",
    "            self.df_all[model] = pd.to_numeric(self.df_all[model], errors='coerce')\n",
    "        \n",
    "        # Eliminar filas con valores NaN cr√≠ticos\n",
    "        critical_cols = ['Paso', 'Varianza'] + self.models\n",
    "        self.df_all.dropna(subset=critical_cols, inplace=True)\n",
    "        \n",
    "        print(f\"‚úì Tipos de datos convertidos\")\n",
    "        print(f\"‚úì Filas despu√©s de limpieza: {len(self.df_all)}\")\n",
    "        \n",
    "    def generate_full_report(self, output_dir='resultados_analisis'):\n",
    "        \"\"\"\n",
    "        Genera reporte completo respondiendo a todas las preguntas clave.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INICIANDO AN√ÅLISIS COMPREHENSIVO DE MODELOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Crear archivo de reporte\n",
    "        report_file = f\"{output_dir}/reporte_completo.txt\"\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"REPORTE COMPLETO DE AN√ÅLISIS DE MODELOS DE PREDICCI√ìN\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # 1. AN√ÅLISIS POR CARACTER√çSTICAS DEL DGP\n",
    "        print(\"\\n1. Analizando caracter√≠sticas del proceso generador...\")\n",
    "        self.analyze_dgp_characteristics(output_dir)\n",
    "        \n",
    "        # 2. AN√ÅLISIS POR DISTRIBUCI√ìN DE ERRORES\n",
    "        print(\"\\n2. Analizando efecto de distribuciones...\")\n",
    "        self.analyze_distribution_effects(output_dir)\n",
    "        \n",
    "        # 3. AN√ÅLISIS POR HORIZONTE DE PREDICCI√ìN\n",
    "        print(\"\\n3. Analizando horizonte de predicci√≥n...\")\n",
    "        self.analyze_horizon_effects(output_dir)\n",
    "        \n",
    "        # 4. AN√ÅLISIS DE INTERACCIONES COMPLEJAS\n",
    "        print(\"\\n4. Analizando interacciones complejas...\")\n",
    "        self.analyze_interactions(output_dir)\n",
    "        \n",
    "        # 5. AN√ÅLISIS DE ROBUSTEZ Y ESTABILIDAD\n",
    "        print(\"\\n5. Analizando robustez y estabilidad...\")\n",
    "        self.analyze_robustness(output_dir)\n",
    "        \n",
    "        # 6. AN√ÅLISIS DE SIGNIFICANCIA ESTAD√çSTICA (DIEBOLD-MARIANO)\n",
    "        print(\"\\n6. Realizando tests de Diebold-Mariano...\")\n",
    "        self.analyze_statistical_significance_dm(output_dir)\n",
    "        \n",
    "        # 7. AN√ÅLISIS POR MODELO INDIVIDUAL\n",
    "        print(\"\\n7. Generando perfiles por modelo...\")\n",
    "        self.analyze_individual_models(output_dir)\n",
    "        \n",
    "        # 8. RECOMENDACIONES Y CONCLUSIONES\n",
    "        print(\"\\n8. Generando recomendaciones...\")\n",
    "        self.generate_recommendations(output_dir)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"AN√ÅLISIS COMPLETO. Resultados guardados en: {output_dir}/\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "    def analyze_dgp_characteristics(self, output_dir):\n",
    "        \"\"\"\n",
    "        1. AN√ÅLISIS DE CARACTER√çSTICAS DEL PROCESO GENERADOR\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # 1.1 Efecto de estacionaridad\n",
    "        print(\"  - Analizando efecto de estacionaridad...\")\n",
    "        for model in self.models:\n",
    "            est_mean = self.df_estacionario[model].mean()\n",
    "            no_est_mean = self.df_no_estacionario[model].mean()\n",
    "            diff = no_est_mean - est_mean\n",
    "            pct_change = (diff / est_mean) * 100 if est_mean != 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'Modelo': model,\n",
    "                'ECRPS_Estacionario': est_mean,\n",
    "                'ECRPS_No_Estacionario': no_est_mean,\n",
    "                'Diferencia': diff,\n",
    "                'Cambio_%': pct_change\n",
    "            })\n",
    "        \n",
    "        df_estacionaridad = pd.DataFrame(results)\n",
    "        df_estacionaridad = df_estacionaridad.sort_values('Cambio_%')\n",
    "        df_estacionaridad.to_csv(f'{output_dir}/1_efecto_estacionaridad.csv', index=False)\n",
    "        \n",
    "        # Visualizaci√≥n\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Gr√°fico de barras comparativas\n",
    "        x = np.arange(len(self.models))\n",
    "        width = 0.35\n",
    "        axes[0].bar(x - width/2, df_estacionaridad['ECRPS_Estacionario'], \n",
    "                   width, label='Estacionario', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, df_estacionaridad['ECRPS_No_Estacionario'], \n",
    "                   width, label='No Estacionario', alpha=0.8)\n",
    "        axes[0].set_xlabel('Modelo')\n",
    "        axes[0].set_ylabel('ECRPS Promedio')\n",
    "        axes[0].set_title('Rendimiento: Estacionario vs No Estacionario')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(df_estacionaridad['Modelo'], rotation=45, ha='right')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gr√°fico de cambio porcentual\n",
    "        colors = ['green' if x < 0 else 'red' for x in df_estacionaridad['Cambio_%']]\n",
    "        axes[1].barh(df_estacionaridad['Modelo'], df_estacionaridad['Cambio_%'], color=colors, alpha=0.7)\n",
    "        axes[1].set_xlabel('Cambio Porcentual (%)')\n",
    "        axes[1].set_title('Impacto de No Estacionaridad\\n(Negativo = Mejor en No Estacionario)')\n",
    "        axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/1_estacionaridad.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 1.2 Efecto de no linealidad\n",
    "        print(\"  - Analizando efecto de no linealidad...\")\n",
    "        results_nl = []\n",
    "        for model in self.models:\n",
    "            lin_mean = self.df_estacionario[model].mean()\n",
    "            nl_mean = self.df_no_lineal[model].mean()\n",
    "            diff = nl_mean - lin_mean\n",
    "            pct_change = (diff / lin_mean) * 100 if lin_mean != 0 else 0\n",
    "            \n",
    "            results_nl.append({\n",
    "                'Modelo': model,\n",
    "                'ECRPS_Lineal': lin_mean,\n",
    "                'ECRPS_No_Lineal': nl_mean,\n",
    "                'Diferencia': diff,\n",
    "                'Cambio_%': pct_change\n",
    "            })\n",
    "        \n",
    "        df_linealidad = pd.DataFrame(results_nl)\n",
    "        df_linealidad = df_linealidad.sort_values('Cambio_%')\n",
    "        df_linealidad.to_csv(f'{output_dir}/1_efecto_no_linealidad.csv', index=False)\n",
    "        \n",
    "        # Visualizaci√≥n no linealidad\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        x = np.arange(len(self.models))\n",
    "        axes[0].bar(x - width/2, df_linealidad['ECRPS_Lineal'], \n",
    "                   width, label='Lineal', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, df_linealidad['ECRPS_No_Lineal'], \n",
    "                   width, label='No Lineal', alpha=0.8)\n",
    "        axes[0].set_xlabel('Modelo')\n",
    "        axes[0].set_ylabel('ECRPS Promedio')\n",
    "        axes[0].set_title('Rendimiento: Lineal vs No Lineal')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(df_linealidad['Modelo'], rotation=45, ha='right')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        colors = ['green' if x < 0 else 'red' for x in df_linealidad['Cambio_%']]\n",
    "        axes[1].barh(df_linealidad['Modelo'], df_linealidad['Cambio_%'], color=colors, alpha=0.7)\n",
    "        axes[1].set_xlabel('Cambio Porcentual (%)')\n",
    "        axes[1].set_title('Impacto de No Linealidad\\n(Negativo = Mejor en No Lineal)')\n",
    "        axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/1_no_linealidad.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 1.3 An√°lisis por tipo de modelo\n",
    "        print(\"  - Analizando efecto del tipo de modelo...\")\n",
    "        self.analyze_model_type_effect(output_dir)\n",
    "        \n",
    "    def analyze_model_type_effect(self, output_dir):\n",
    "        \"\"\"Analiza el efecto del tipo de modelo en el rendimiento\"\"\"\n",
    "        \n",
    "        # An√°lisis para datos estacionarios\n",
    "        if 'Tipo de Modelo' in self.df_estacionario.columns:\n",
    "            results_type = []\n",
    "            for model in self.models:\n",
    "                for model_type in self.df_estacionario['Tipo de Modelo'].unique():\n",
    "                    subset = self.df_estacionario[self.df_estacionario['Tipo de Modelo'] == model_type]\n",
    "                    if len(subset) > 0:\n",
    "                        results_type.append({\n",
    "                            'Modelo_Predictor': model,\n",
    "                            'Tipo_Proceso': model_type,\n",
    "                            'ECRPS_Mean': subset[model].mean(),\n",
    "                            'ECRPS_Std': subset[model].std(),\n",
    "                            'N_Obs': len(subset)\n",
    "                        })\n",
    "            \n",
    "            df_type = pd.DataFrame(results_type)\n",
    "            df_type.to_csv(f'{output_dir}/1_efecto_tipo_modelo.csv', index=False)\n",
    "            \n",
    "            # Crear heatmap para tipos m√°s comunes\n",
    "            common_types = df_type['Tipo_Proceso'].value_counts().head(10).index\n",
    "            df_type_filtered = df_type[df_type['Tipo_Proceso'].isin(common_types)]\n",
    "            \n",
    "            if len(df_type_filtered) > 0:\n",
    "                pivot = df_type_filtered.pivot_table(\n",
    "                    index='Modelo_Predictor', \n",
    "                    columns='Tipo_Proceso', \n",
    "                    values='ECRPS_Mean'\n",
    "                )\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(14, 8))\n",
    "                sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax, \n",
    "                           cbar_kws={'label': 'ECRPS'})\n",
    "                ax.set_title('Rendimiento por Modelo Predictor y Tipo de Proceso', fontsize=14)\n",
    "                ax.set_xlabel('Tipo de Proceso')\n",
    "                ax.set_ylabel('Modelo Predictor')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/1_heatmap_tipo_modelo.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "    def analyze_distribution_effects(self, output_dir):\n",
    "        \"\"\"\n",
    "        2. AN√ÅLISIS DE EFECTOS DE DISTRIBUCI√ìN\n",
    "        \"\"\"\n",
    "        print(\"  - Analizando efectos de distribuciones...\")\n",
    "        \n",
    "        results_dist = []\n",
    "        for model in self.models:\n",
    "            for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "                if pd.notna(dist):\n",
    "                    subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "                    if len(subset) > 0:\n",
    "                        results_dist.append({\n",
    "                            'Modelo': model,\n",
    "                            'Distribuci√≥n': dist,\n",
    "                            'ECRPS_Mean': subset[model].mean(),\n",
    "                            'ECRPS_Std': subset[model].std(),\n",
    "                            'ECRPS_Min': subset[model].min(),\n",
    "                            'ECRPS_Max': subset[model].max()\n",
    "                        })\n",
    "        \n",
    "        df_dist = pd.DataFrame(results_dist)\n",
    "        df_dist.to_csv(f'{output_dir}/2_efecto_distribucion.csv', index=False)\n",
    "        \n",
    "        # Heatmap\n",
    "        if len(df_dist) > 0:\n",
    "            pivot = df_dist.pivot(index='Modelo', columns='Distribuci√≥n', values='ECRPS_Mean')\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax, cbar_kws={'label': 'ECRPS'})\n",
    "            ax.set_title('Rendimiento por Modelo y Distribuci√≥n', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/2_heatmap_distribucion.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # An√°lisis por varianza\n",
    "        print(\"  - Analizando efectos de varianza...\")\n",
    "        results_var = []\n",
    "        varianzas_unicas = sorted([v for v in self.df_all['Varianza'].unique() if pd.notna(v)])\n",
    "        \n",
    "        for model in self.models:\n",
    "            for var in varianzas_unicas:\n",
    "                subset = self.df_all[self.df_all['Varianza'] == var]\n",
    "                if len(subset) > 0:\n",
    "                    results_var.append({\n",
    "                        'Modelo': model,\n",
    "                        'Varianza': var,\n",
    "                        'ECRPS_Mean': subset[model].mean(),\n",
    "                        'ECRPS_Std': subset[model].std()\n",
    "                    })\n",
    "        \n",
    "        df_var = pd.DataFrame(results_var)\n",
    "        df_var.to_csv(f'{output_dir}/2_efecto_varianza.csv', index=False)\n",
    "        \n",
    "        # Gr√°fico de l√≠neas por varianza\n",
    "        if len(df_var) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            for model in self.models:\n",
    "                data = df_var[df_var['Modelo'] == model].sort_values('Varianza')\n",
    "                if len(data) > 0:\n",
    "                    ax.plot(data['Varianza'], data['ECRPS_Mean'], marker='o', label=model, linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Varianza', fontsize=12)\n",
    "            ax.set_ylabel('ECRPS Promedio', fontsize=12)\n",
    "            ax.set_title('Rendimiento seg√∫n Nivel de Varianza', fontsize=14)\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/2_efecto_varianza.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "    def analyze_horizon_effects(self, output_dir):\n",
    "        \"\"\"\n",
    "        3. AN√ÅLISIS DE HORIZONTE DE PREDICCI√ìN\n",
    "        \"\"\"\n",
    "        print(\"  - Analizando deterioro por horizonte...\")\n",
    "        \n",
    "        results_horizon = []\n",
    "        pasos_unicos = sorted([p for p in self.df_all['Paso'].unique() if pd.notna(p)])\n",
    "        \n",
    "        for model in self.models:\n",
    "            for paso in pasos_unicos:\n",
    "                subset = self.df_all[self.df_all['Paso'] == paso]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    std_val = subset[model].std()\n",
    "                    cv_val = std_val / mean_val if mean_val != 0 and pd.notna(mean_val) else 0\n",
    "                    \n",
    "                    results_horizon.append({\n",
    "                        'Modelo': model,\n",
    "                        'Paso': int(paso),\n",
    "                        'ECRPS_Mean': mean_val,\n",
    "                        'ECRPS_Std': std_val,\n",
    "                        'ECRPS_CV': cv_val\n",
    "                    })\n",
    "        \n",
    "        df_horizon = pd.DataFrame(results_horizon)\n",
    "        df_horizon.to_csv(f'{output_dir}/3_efecto_horizonte.csv', index=False)\n",
    "        \n",
    "        # Gr√°fico de deterioro\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # ECRPS promedio por paso\n",
    "        for model in self.models:\n",
    "            data = df_horizon[df_horizon['Modelo'] == model].sort_values('Paso')\n",
    "            if len(data) > 0:\n",
    "                axes[0].plot(data['Paso'], data['ECRPS_Mean'], marker='o', label=model, linewidth=2)\n",
    "        \n",
    "        axes[0].set_xlabel('Paso de Predicci√≥n', fontsize=12)\n",
    "        axes[0].set_ylabel('ECRPS Promedio', fontsize=12)\n",
    "        axes[0].set_title('Deterioro del Rendimiento por Horizonte', fontsize=14)\n",
    "        axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Tasa de deterioro\n",
    "        deterioro = []\n",
    "        for model in self.models:\n",
    "            data = df_horizon[df_horizon['Modelo'] == model].sort_values('Paso')\n",
    "            if len(data) >= 2:\n",
    "                paso_values = data['Paso'].tolist()\n",
    "                ecrps_paso1 = data.iloc[0]['ECRPS_Mean']\n",
    "                ecrps_paso_final = data.iloc[-1]['ECRPS_Mean']\n",
    "                \n",
    "                if pd.notna(ecrps_paso1) and pd.notna(ecrps_paso_final) and ecrps_paso1 != 0:\n",
    "                    tasa = ((ecrps_paso_final - ecrps_paso1) / ecrps_paso1) * 100\n",
    "                    deterioro.append({'Modelo': model, 'Deterioro_%': tasa})\n",
    "        \n",
    "        if deterioro:\n",
    "            df_deterioro = pd.DataFrame(deterioro).sort_values('Deterioro_%')\n",
    "            colors = ['green' if x < df_deterioro['Deterioro_%'].median() else 'red' \n",
    "                     for x in df_deterioro['Deterioro_%']]\n",
    "            axes[1].barh(df_deterioro['Modelo'], df_deterioro['Deterioro_%'], color=colors, alpha=0.7)\n",
    "            axes[1].set_xlabel(f'Deterioro Paso {pasos_unicos[0]}‚Üí{pasos_unicos[-1]} (%)', fontsize=12)\n",
    "            axes[1].set_title('Tasa de Deterioro por Modelo', fontsize=14)\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/3_horizonte_prediccion.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de consistencia de ranking\n",
    "        print(\"  - Analizando consistencia de ranking...\")\n",
    "        ranking_consistency = []\n",
    "        for paso in pasos_unicos:\n",
    "            subset = self.df_all[self.df_all['Paso'] == paso]\n",
    "            if len(subset) > 0:\n",
    "                ranks = subset[self.models].mean().rank()\n",
    "                rank_dict = ranks.to_dict()\n",
    "                rank_dict['Paso'] = int(paso)\n",
    "                ranking_consistency.append(rank_dict)\n",
    "        \n",
    "        df_ranks = pd.DataFrame(ranking_consistency)\n",
    "        df_ranks.to_csv(f'{output_dir}/3_ranking_por_paso.csv', index=False)\n",
    "        \n",
    "    def analyze_interactions(self, output_dir):\n",
    "        \"\"\"\n",
    "        4. AN√ÅLISIS DE INTERACCIONES COMPLEJAS\n",
    "        \"\"\"\n",
    "        print(\"  - Analizando interacciones Escenario √ó Distribuci√≥n...\")\n",
    "        \n",
    "        results_int = []\n",
    "        for model in self.models:\n",
    "            for escenario in self.df_all['Escenario'].unique():\n",
    "                for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "                    subset = self.df_all[(self.df_all['Escenario'] == escenario) & \n",
    "                                        (self.df_all['Distribuci√≥n'] == dist)]\n",
    "                    if len(subset) > 0:\n",
    "                        results_int.append({\n",
    "                            'Modelo': model,\n",
    "                            'Escenario': escenario,\n",
    "                            'Distribuci√≥n': dist,\n",
    "                            'ECRPS_Mean': subset[model].mean()\n",
    "                        })\n",
    "        \n",
    "        df_int = pd.DataFrame(results_int)\n",
    "        df_int.to_csv(f'{output_dir}/4_interacciones.csv', index=False)\n",
    "        \n",
    "        # Heatmap de interacciones para cada modelo\n",
    "        for model in self.models[:3]:  # Solo primeros 3 por espacio\n",
    "            model_data = df_int[df_int['Modelo'] == model]\n",
    "            if len(model_data) > 0:\n",
    "                pivot = model_data.pivot(\n",
    "                    index='Escenario', columns='Distribuci√≥n', values='ECRPS_Mean')\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax)\n",
    "                ax.set_title(f'Interacci√≥n Escenario √ó Distribuci√≥n: {model}', fontsize=12)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/4_interaccion_{model.replace(\" \", \"_\")}.png', \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        # Interacci√≥n triple: Escenario √ó Varianza √ó Paso\n",
    "        print(\"  - Analizando interacci√≥n triple...\")\n",
    "        results_triple = []\n",
    "        \n",
    "        varianzas_unicas = sorted([v for v in self.df_all['Varianza'].unique() if pd.notna(v)])\n",
    "        pasos_unicos = sorted([p for p in self.df_all['Paso'].unique() if pd.notna(p)])\n",
    "        \n",
    "        for model in self.models:\n",
    "            for escenario in self.df_all['Escenario'].unique():\n",
    "                for var in varianzas_unicas:\n",
    "                    for paso in pasos_unicos:\n",
    "                        subset = self.df_all[\n",
    "                            (self.df_all['Escenario'] == escenario) & \n",
    "                            (self.df_all['Varianza'] == var) &\n",
    "                            (self.df_all['Paso'] == paso)\n",
    "                        ]\n",
    "                        if len(subset) > 0:\n",
    "                            results_triple.append({\n",
    "                                'Modelo': model,\n",
    "                                'Escenario': escenario,\n",
    "                                'Varianza': var,\n",
    "                                'Paso': int(paso),\n",
    "                                'ECRPS_Mean': subset[model].mean()\n",
    "                            })\n",
    "        \n",
    "        df_triple = pd.DataFrame(results_triple)\n",
    "        df_triple.to_csv(f'{output_dir}/4_interaccion_triple.csv', index=False)\n",
    "        \n",
    "    def analyze_robustness(self, output_dir):\n",
    "        \"\"\"\n",
    "        5. AN√ÅLISIS DE ROBUSTEZ Y ESTABILIDAD\n",
    "        \"\"\"\n",
    "        print(\"  - Calculando m√©tricas de robustez...\")\n",
    "        \n",
    "        results_robust = []\n",
    "        for model in self.models:\n",
    "            ecrps_values = self.df_all[model]\n",
    "            \n",
    "            results_robust.append({\n",
    "                'Modelo': model,\n",
    "                'ECRPS_Mean': ecrps_values.mean(),\n",
    "                'ECRPS_Std': ecrps_values.std(),\n",
    "                'ECRPS_CV': ecrps_values.std() / ecrps_values.mean() if ecrps_values.mean() != 0 else 0,\n",
    "                'ECRPS_Min': ecrps_values.min(),\n",
    "                'ECRPS_Q25': ecrps_values.quantile(0.25),\n",
    "                'ECRPS_Median': ecrps_values.median(),\n",
    "                'ECRPS_Q75': ecrps_values.quantile(0.75),\n",
    "                'ECRPS_Max': ecrps_values.max(),\n",
    "                'ECRPS_IQR': ecrps_values.quantile(0.75) - ecrps_values.quantile(0.25)\n",
    "            })\n",
    "        \n",
    "        df_robust = pd.DataFrame(results_robust)\n",
    "        df_robust = df_robust.sort_values('ECRPS_CV')\n",
    "        df_robust.to_csv(f'{output_dir}/5_robustez.csv', index=False)\n",
    "        \n",
    "        # Gr√°fico de robustez\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Coeficiente de variaci√≥n\n",
    "        axes[0, 0].barh(df_robust['Modelo'], df_robust['ECRPS_CV'], alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Coeficiente de Variaci√≥n')\n",
    "        axes[0, 0].set_title('Estabilidad (Menor CV = M√°s Estable)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rango intercuart√≠lico\n",
    "        axes[0, 1].barh(df_robust['Modelo'], df_robust['ECRPS_IQR'], alpha=0.7, color='coral')\n",
    "        axes[0, 1].set_xlabel('Rango Intercuart√≠lico')\n",
    "        axes[0, 1].set_title('Variabilidad (Menor IQR = M√°s Consistente)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Boxplot comparativo\n",
    "        data_box = [self.df_all[model] for model in self.models]\n",
    "        bp = axes[1, 0].boxplot(data_box, labels=self.models, patch_artist=True)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "        axes[1, 0].set_ylabel('ECRPS')\n",
    "        axes[1, 0].set_title('Distribuci√≥n de ECRPS por Modelo')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter: Media vs Variabilidad\n",
    "        axes[1, 1].scatter(df_robust['ECRPS_Mean'], df_robust['ECRPS_Std'], \n",
    "                          s=100, alpha=0.6, c=range(len(df_robust)), cmap='viridis')\n",
    "        for idx, row in df_robust.iterrows():\n",
    "            axes[1, 1].annotate(row['Modelo'], \n",
    "                               (row['ECRPS_Mean'], row['ECRPS_Std']),\n",
    "                               fontsize=8, alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('ECRPS Promedio')\n",
    "        axes[1, 1].set_ylabel('Desviaci√≥n Est√°ndar')\n",
    "        axes[1, 1].set_title('Trade-off Rendimiento vs Estabilidad')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/5_robustez.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de peores casos\n",
    "        print(\"  - Identificando peores casos...\")\n",
    "        worst_cases = []\n",
    "        for model in self.models:\n",
    "            df_temp = self.df_all.copy()\n",
    "            df_temp['ECRPS'] = df_temp[model]\n",
    "            worst = df_temp.nlargest(10, 'ECRPS')[\n",
    "                ['Escenario', 'Tipo de Modelo', 'Distribuci√≥n', 'Varianza', 'Paso', 'ECRPS']\n",
    "            ]\n",
    "            worst['Modelo_Predictor'] = model\n",
    "            worst_cases.append(worst)\n",
    "        \n",
    "        df_worst = pd.concat(worst_cases, ignore_index=True)\n",
    "        df_worst.to_csv(f'{output_dir}/5_peores_casos.csv', index=False)\n",
    "        \n",
    "    def analyze_statistical_significance_dm(self, output_dir):\n",
    "        \"\"\"\n",
    "        6. AN√ÅLISIS DE SIGNIFICANCIA ESTAD√çSTICA CON DIEBOLD-MARIANO\n",
    "        \"\"\"\n",
    "        print(\"  - Realizando tests de Diebold-Mariano...\")\n",
    "        \n",
    "        # Test de Friedman por escenario (para comparaci√≥n general)\n",
    "        results_friedman = []\n",
    "        for escenario in self.df_all['Escenario'].unique():\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            data_matrix = subset[self.models].values\n",
    "            \n",
    "            try:\n",
    "                statistic, p_value = friedmanchisquare(*[data_matrix[:, i] for i in range(len(self.models))])\n",
    "                \n",
    "                results_friedman.append({\n",
    "                    'Escenario': escenario,\n",
    "                    'Friedman_Statistic': statistic,\n",
    "                    'P_Value': p_value,\n",
    "                    'Significativo': 'S√≠' if p_value < 0.05 else 'No'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    Advertencia: Error en test de Friedman para {escenario}: {e}\")\n",
    "        \n",
    "        if results_friedman:\n",
    "            df_friedman = pd.DataFrame(results_friedman)\n",
    "            df_friedman.to_csv(f'{output_dir}/6_test_friedman.csv', index=False)\n",
    "        \n",
    "        # Tests de Diebold-Mariano pareados\n",
    "        print(\"  - Realizando tests pareados de Diebold-Mariano...\")\n",
    "        pairs = list(combinations(self.models, 2))\n",
    "        dm_results = []\n",
    "        \n",
    "        for model1, model2 in pairs:\n",
    "            # Calcular errores (usamos ECRPS directamente como m√©trica de p√©rdida)\n",
    "            errors1 = self.df_all[model1].values\n",
    "            errors2 = self.df_all[model2].values\n",
    "            \n",
    "            # Test de Diebold-Mariano\n",
    "            dm_stat, p_value = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "            \n",
    "            mean_diff = self.df_all[model1].mean() - self.df_all[model2].mean()\n",
    "            \n",
    "            # Determinar ganador\n",
    "            if p_value < 0.05:\n",
    "                if mean_diff < 0:\n",
    "                    ganador = model1\n",
    "                else:\n",
    "                    ganador = model2\n",
    "            else:\n",
    "                ganador = 'Empate'\n",
    "            \n",
    "            dm_results.append({\n",
    "                'Modelo_1': model1,\n",
    "                'Modelo_2': model2,\n",
    "                'Diferencia_Media': mean_diff,\n",
    "                'DM_Statistic': dm_stat,\n",
    "                'P_Value': p_value,\n",
    "                'Significativo_0.05': 'S√≠' if p_value < 0.05 else 'No',\n",
    "                'Significativo_0.01': 'S√≠' if p_value < 0.01 else 'No',\n",
    "                'Ganador': ganador\n",
    "            })\n",
    "        \n",
    "        df_dm = pd.DataFrame(dm_results)\n",
    "        df_dm = df_dm.sort_values('P_Value')\n",
    "        df_dm.to_csv(f'{output_dir}/6_tests_diebold_mariano.csv', index=False)\n",
    "        \n",
    "        # Matriz de p-valores (Diebold-Mariano)\n",
    "        print(\"  - Creando matriz de p-valores...\")\n",
    "        p_matrix = np.ones((len(self.models), len(self.models)))\n",
    "        for i, model1 in enumerate(self.models):\n",
    "            for j, model2 in enumerate(self.models):\n",
    "                if i != j:\n",
    "                    errors1 = self.df_all[model1].values\n",
    "                    errors2 = self.df_all[model2].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    p_matrix[i, j] = p_val\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        sns.heatmap(p_matrix, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                   xticklabels=self.models, yticklabels=self.models, \n",
    "                   ax=ax, vmin=0, vmax=0.1, cbar_kws={'label': 'P-valor'})\n",
    "        ax.set_title('Matriz de P-valores (Test de Diebold-Mariano)\\nVerde = Diferencia Significativa', \n",
    "                    fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/6_matriz_pvalores_dm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Dominancia estad√≠stica con Diebold-Mariano\n",
    "        print(\"  - Analizando dominancia estad√≠stica...\")\n",
    "        dominance = []\n",
    "        for model in self.models:\n",
    "            wins = 0\n",
    "            losses = 0\n",
    "            ties = 0\n",
    "            for other_model in self.models:\n",
    "                if model != other_model:\n",
    "                    errors1 = self.df_all[model].values\n",
    "                    errors2 = self.df_all[other_model].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = self.df_all[model].mean() - self.df_all[other_model].mean()\n",
    "                    \n",
    "                    if p_val < 0.05:\n",
    "                        if mean_diff < 0:  # modelo es mejor (menor ECRPS)\n",
    "                            wins += 1\n",
    "                        else:\n",
    "                            losses += 1\n",
    "                    else:\n",
    "                        ties += 1\n",
    "            \n",
    "            dominance.append({\n",
    "                'Modelo': model,\n",
    "                'Victorias_Significativas': wins,\n",
    "                'Derrotas_Significativas': losses,\n",
    "                'Empates': ties,\n",
    "                'Score_Neto': wins - losses\n",
    "            })\n",
    "        \n",
    "        df_dominance = pd.DataFrame(dominance)\n",
    "        df_dominance = df_dominance.sort_values('Score_Neto', ascending=False)\n",
    "        df_dominance.to_csv(f'{output_dir}/6_dominancia_estadistica_dm.csv', index=False)\n",
    "        \n",
    "        # Visualizaci√≥n de dominancia\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(df_dominance))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax.bar(x - width, df_dominance['Victorias_Significativas'], \n",
    "               width, label='Victorias', color='green', alpha=0.7)\n",
    "        ax.bar(x, df_dominance['Empates'], \n",
    "               width, label='Empates', color='gray', alpha=0.7)\n",
    "        ax.bar(x + width, df_dominance['Derrotas_Significativas'], \n",
    "               width, label='Derrotas', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Modelo')\n",
    "        ax.set_ylabel('N√∫mero de Comparaciones')\n",
    "        ax.set_title('Dominancia Estad√≠stica (Test Diebold-Mariano)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df_dominance['Modelo'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/6_dominancia_dm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de Diebold-Mariano por escenario\n",
    "        print(\"  - Analizando DM por escenario...\")\n",
    "        dm_by_scenario = []\n",
    "        for escenario in self.df_all['Escenario'].unique():\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            \n",
    "            for model1, model2 in combinations(self.models, 2):\n",
    "                errors1 = subset[model1].values\n",
    "                errors2 = subset[model2].values\n",
    "                \n",
    "                if len(errors1) > 0 and len(errors2) > 0:\n",
    "                    dm_stat, p_value = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = subset[model1].mean() - subset[model2].mean()\n",
    "                    \n",
    "                    dm_by_scenario.append({\n",
    "                        'Escenario': escenario,\n",
    "                        'Modelo_1': model1,\n",
    "                        'Modelo_2': model2,\n",
    "                        'DM_Statistic': dm_stat,\n",
    "                        'P_Value': p_value,\n",
    "                        'Diferencia_Media': mean_diff,\n",
    "                        'Significativo': 'S√≠' if p_value < 0.05 else 'No'\n",
    "                    })\n",
    "        \n",
    "        df_dm_scenario = pd.DataFrame(dm_by_scenario)\n",
    "        df_dm_scenario.to_csv(f'{output_dir}/6_dm_por_escenario.csv', index=False)\n",
    "    \n",
    "    def analyze_individual_models(self, output_dir):\n",
    "        \"\"\"\n",
    "        7. PERFILES INDIVIDUALES POR MODELO\n",
    "        \"\"\"\n",
    "        print(\"  - Generando perfiles individuales...\")\n",
    "        \n",
    "        for model in self.models:\n",
    "            print(f\"    > Analizando {model}...\")\n",
    "            \n",
    "            # Crear subdirectorio para el modelo\n",
    "            model_dir = f\"{output_dir}/perfiles_modelos/{model.replace(' ', '_')}\"\n",
    "            import os\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Reporte del modelo\n",
    "            report = []\n",
    "            report.append(f\"=\"*80)\n",
    "            report.append(f\"PERFIL DETALLADO: {model}\")\n",
    "            report.append(f\"=\"*80)\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Estad√≠sticas generales\n",
    "            report.append(\"1. ESTAD√çSTICAS GENERALES\")\n",
    "            report.append(\"-\" * 40)\n",
    "            report.append(f\"ECRPS Promedio Global: {self.df_all[model].mean():.6f}\")\n",
    "            report.append(f\"Desviaci√≥n Est√°ndar: {self.df_all[model].std():.6f}\")\n",
    "            cv = self.df_all[model].std()/self.df_all[model].mean() if self.df_all[model].mean() != 0 else 0\n",
    "            report.append(f\"Coeficiente de Variaci√≥n: {cv:.4f}\")\n",
    "            report.append(f\"M√≠nimo: {self.df_all[model].min():.6f}\")\n",
    "            report.append(f\"Mediana: {self.df_all[model].median():.6f}\")\n",
    "            report.append(f\"M√°ximo: {self.df_all[model].max():.6f}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Ranking general\n",
    "            mean_scores = self.df_all[self.models].mean()\n",
    "            ranking = mean_scores.rank().astype(int)\n",
    "            report.append(f\"Ranking General: {ranking[model]}¬∞ de {len(self.models)}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Mejor escenario\n",
    "            report.append(\"2. MEJORES ESCENARIOS\")\n",
    "            report.append(\"-\" * 40)\n",
    "            best_idx = self.df_all[model].idxmin()\n",
    "            best_row = self.df_all.loc[best_idx]\n",
    "            report.append(f\"Mejor ECRPS: {best_row[model]:.6f}\")\n",
    "            report.append(f\"  - Escenario: {best_row['Escenario']}\")\n",
    "            if 'Tipo de Modelo' in best_row:\n",
    "                report.append(f\"  - Tipo Modelo: {best_row['Tipo de Modelo']}\")\n",
    "            report.append(f\"  - Distribuci√≥n: {best_row['Distribuci√≥n']}\")\n",
    "            report.append(f\"  - Varianza: {best_row['Varianza']}\")\n",
    "            report.append(f\"  - Paso: {best_row['Paso']}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Peor escenario\n",
    "            report.append(\"3. PEORES ESCENARIOS\")\n",
    "            report.append(\"-\" * 40)\n",
    "            worst_idx = self.df_all[model].idxmax()\n",
    "            worst_row = self.df_all.loc[worst_idx]\n",
    "            report.append(f\"Peor ECRPS: {worst_row[model]:.6f}\")\n",
    "            report.append(f\"  - Escenario: {worst_row['Escenario']}\")\n",
    "            if 'Tipo de Modelo' in worst_row:\n",
    "                report.append(f\"  - Tipo Modelo: {worst_row['Tipo de Modelo']}\")\n",
    "            report.append(f\"  - Distribuci√≥n: {worst_row['Distribuci√≥n']}\")\n",
    "            report.append(f\"  - Varianza: {worst_row['Varianza']}\")\n",
    "            report.append(f\"  - Paso: {worst_row['Paso']}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Rendimiento por escenario\n",
    "            report.append(\"4. RENDIMIENTO POR ESCENARIO\")\n",
    "            report.append(\"-\" * 40)\n",
    "            for escenario in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']:\n",
    "                subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    rank = subset[self.models].mean().rank()[model]\n",
    "                    report.append(f\"{escenario}:\")\n",
    "                    report.append(f\"  ECRPS: {mean_val:.6f} (Ranking: {int(rank)}¬∞)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Fortalezas y debilidades\n",
    "            report.append(\"5. FORTALEZAS Y DEBILIDADES\")\n",
    "            report.append(\"-\" * 40)\n",
    "            \n",
    "            # Por distribuci√≥n\n",
    "            report.append(\"Por Distribuci√≥n:\")\n",
    "            dist_performance = []\n",
    "            for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "                subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    rank = subset[self.models].mean().rank()[model]\n",
    "                    dist_performance.append((dist, mean_val, rank))\n",
    "            \n",
    "            if dist_performance:\n",
    "                dist_performance.sort(key=lambda x: x[2])\n",
    "                report.append(f\"  Mejor: {dist_performance[0][0]} (Ranking {int(dist_performance[0][2])}¬∞)\")\n",
    "                report.append(f\"  Peor: {dist_performance[-1][0]} (Ranking {int(dist_performance[-1][2])}¬∞)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Por varianza\n",
    "            report.append(\"Por Varianza:\")\n",
    "            var_performance = []\n",
    "            for var in sorted(self.df_all['Varianza'].unique()):\n",
    "                subset = self.df_all[self.df_all['Varianza'] == var]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    rank = subset[self.models].mean().rank()[model]\n",
    "                    var_performance.append((var, mean_val, rank))\n",
    "            \n",
    "            if var_performance:\n",
    "                var_performance.sort(key=lambda x: x[2])\n",
    "                report.append(f\"  Mejor: Varianza {var_performance[0][0]} (Ranking {int(var_performance[0][2])}¬∞)\")\n",
    "                report.append(f\"  Peor: Varianza {var_performance[-1][0]} (Ranking {int(var_performance[-1][2])}¬∞)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Comparaciones con Diebold-Mariano\n",
    "            report.append(\"6. COMPARACIONES ESTAD√çSTICAS (DIEBOLD-MARIANO)\")\n",
    "            report.append(\"-\" * 40)\n",
    "            \n",
    "            wins = 0\n",
    "            losses = 0\n",
    "            for other_model in self.models:\n",
    "                if model != other_model:\n",
    "                    errors1 = self.df_all[model].values\n",
    "                    errors2 = self.df_all[other_model].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = self.df_all[model].mean() - self.df_all[other_model].mean()\n",
    "                    \n",
    "                    if p_val < 0.05:\n",
    "                        if mean_diff < 0:\n",
    "                            wins += 1\n",
    "                        else:\n",
    "                            losses += 1\n",
    "            \n",
    "            report.append(f\"Victorias significativas: {wins}\")\n",
    "            report.append(f\"Derrotas significativas: {losses}\")\n",
    "            report.append(f\"Score neto: {wins - losses}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Guardar reporte\n",
    "            with open(f\"{model_dir}/perfil_{model.replace(' ', '_')}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(report))\n",
    "            \n",
    "            # Visualizaciones del modelo\n",
    "            self._create_model_visualizations(model, model_dir)\n",
    "    \n",
    "    def _create_model_visualizations(self, model, model_dir):\n",
    "        \"\"\"Crea visualizaciones espec√≠ficas para un modelo\"\"\"\n",
    "        \n",
    "        # 1. Distribuci√≥n de ECRPS\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Histograma\n",
    "        axes[0, 0].hist(self.df_all[model], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        axes[0, 0].axvline(self.df_all[model].mean(), color='red', linestyle='--', \n",
    "                          linewidth=2, label=f'Media: {self.df_all[model].mean():.4f}')\n",
    "        axes[0, 0].axvline(self.df_all[model].median(), color='green', linestyle='--', \n",
    "                          linewidth=2, label=f'Mediana: {self.df_all[model].median():.4f}')\n",
    "        axes[0, 0].set_xlabel('ECRPS')\n",
    "        axes[0, 0].set_ylabel('Frecuencia')\n",
    "        axes[0, 0].set_title(f'Distribuci√≥n de ECRPS - {model}')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Boxplot por escenario\n",
    "        data_by_scenario = [self.df_all[self.df_all['Escenario'] == esc][model] \n",
    "                           for esc in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']]\n",
    "        bp = axes[0, 1].boxplot(data_by_scenario, labels=['Est. Lin.', 'No Est. Lin.', 'No Lin.'], \n",
    "                               patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
    "            patch.set_facecolor(color)\n",
    "        axes[0, 1].set_ylabel('ECRPS')\n",
    "        axes[0, 1].set_title(f'ECRPS por Escenario - {model}')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rendimiento por paso\n",
    "        paso_data = []\n",
    "        for p in sorted(self.df_all['Paso'].unique()):\n",
    "            subset = self.df_all[self.df_all['Paso'] == p]\n",
    "            if len(subset) > 0:\n",
    "                paso_data.append((p, subset[model].mean()))\n",
    "        \n",
    "        if paso_data:\n",
    "            pasos, means = zip(*paso_data)\n",
    "            axes[1, 0].plot(pasos, means, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "            axes[1, 0].set_xlabel('Paso de Predicci√≥n')\n",
    "            axes[1, 0].set_ylabel('ECRPS Promedio')\n",
    "            axes[1, 0].set_title(f'Rendimiento por Horizonte - {model}')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Heatmap: Distribuci√≥n √ó Varianza\n",
    "        pivot_data = []\n",
    "        dist_labels = []\n",
    "        var_labels = sorted(self.df_all['Varianza'].unique())\n",
    "        \n",
    "        for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "            row = []\n",
    "            for var in var_labels:\n",
    "                subset = self.df_all[(self.df_all['Distribuci√≥n'] == dist) & \n",
    "                                    (self.df_all['Varianza'] == var)]\n",
    "                if len(subset) > 0:\n",
    "                    row.append(subset[model].mean())\n",
    "                else:\n",
    "                    row.append(np.nan)\n",
    "            if not all(np.isnan(row)):\n",
    "                pivot_data.append(row)\n",
    "                dist_labels.append(dist)\n",
    "        \n",
    "        if pivot_data:\n",
    "            pivot_df = pd.DataFrame(pivot_data, index=dist_labels, columns=var_labels)\n",
    "            \n",
    "            sns.heatmap(pivot_df, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[1, 1],\n",
    "                       cbar_kws={'label': 'ECRPS'})\n",
    "            axes[1, 1].set_title(f'ECRPS: Distribuci√≥n √ó Varianza - {model}')\n",
    "            axes[1, 1].set_xlabel('Varianza')\n",
    "            axes[1, 1].set_ylabel('Distribuci√≥n')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/visualizaciones_{model.replace(\" \", \"_\")}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Comparaci√≥n con otros modelos\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        means = self.df_all[self.models].mean().sort_values()\n",
    "        colors = ['red' if m == model else 'steelblue' for m in means.index]\n",
    "        bars = ax.barh(means.index, means.values, color=colors, alpha=0.7)\n",
    "        \n",
    "        # Destacar el modelo actual\n",
    "        for i, bar in enumerate(bars):\n",
    "            if means.index[i] == model:\n",
    "                bar.set_edgecolor('black')\n",
    "                bar.set_linewidth(3)\n",
    "        \n",
    "        ax.set_xlabel('ECRPS Promedio')\n",
    "        ax.set_title(f'Comparaci√≥n Global - {model} (Destacado en Rojo)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/comparacion_{model.replace(\" \", \"_\")}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_recommendations(self, output_dir):\n",
    "        \"\"\"\n",
    "        8. GENERACI√ìN DE RECOMENDACIONES\n",
    "        \"\"\"\n",
    "        print(\"  - Generando recomendaciones estrat√©gicas...\")\n",
    "        \n",
    "        recommendations = []\n",
    "        recommendations.append(\"=\"*80)\n",
    "        recommendations.append(\"RECOMENDACIONES Y CONCLUSIONES\")\n",
    "        recommendations.append(\"=\"*80)\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 1. Modelo campe√≥n general\n",
    "        overall_best = self.df_all[self.models].mean().idxmin()\n",
    "        overall_worst = self.df_all[self.models].mean().idxmax()\n",
    "        \n",
    "        recommendations.append(\"1. MODELO CAMPE√ìN GENERAL\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(f\"Mejor rendimiento promedio: {overall_best}\")\n",
    "        recommendations.append(f\"ECRPS: {self.df_all[overall_best].mean():.6f}\")\n",
    "        recommendations.append(f\"Desviaci√≥n Est√°ndar: {self.df_all[overall_best].std():.6f}\")\n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(f\"Peor rendimiento promedio: {overall_worst}\")\n",
    "        recommendations.append(f\"ECRPS: {self.df_all[overall_worst].mean():.6f}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 2. Modelos por escenario\n",
    "        recommendations.append(\"2. RECOMENDACIONES POR ESCENARIO\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        for escenario in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']:\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            if len(subset) > 0:\n",
    "                best_model = subset[self.models].mean().idxmin()\n",
    "                best_score = subset[best_model].mean()\n",
    "                \n",
    "                recommendations.append(f\"\\n{escenario}:\")\n",
    "                recommendations.append(f\"  Modelo Recomendado: {best_model}\")\n",
    "                recommendations.append(f\"  ECRPS Promedio: {best_score:.6f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 3. Modelos por distribuci√≥n\n",
    "        recommendations.append(\"3. RECOMENDACIONES POR DISTRIBUCI√ìN DE ERRORES\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "            subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "            if len(subset) > 0:\n",
    "                best_model = subset[self.models].mean().idxmin()\n",
    "                best_score = subset[best_model].mean()\n",
    "                \n",
    "                recommendations.append(f\"\\nDistribuci√≥n {dist}:\")\n",
    "                recommendations.append(f\"  Modelo Recomendado: {best_model}\")\n",
    "                recommendations.append(f\"  ECRPS Promedio: {best_score:.6f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 4. Modelos m√°s robustos\n",
    "        recommendations.append(\"4. MODELOS M√ÅS ROBUSTOS (MENOR VARIABILIDAD)\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        cv_scores = {model: self.df_all[model].std() / self.df_all[model].mean() \n",
    "                    for model in self.models if self.df_all[model].mean() != 0}\n",
    "        cv_sorted = sorted(cv_scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "        for i, (model, cv) in enumerate(cv_sorted[:3], 1):\n",
    "            recommendations.append(f\"{i}. {model}: CV = {cv:.4f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 5. Modelos por horizonte\n",
    "        recommendations.append(\"5. RECOMENDACIONES POR HORIZONTE DE PREDICCI√ìN\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        pasos_unicos = sorted(self.df_all['Paso'].unique())\n",
    "        for paso in [pasos_unicos[0], pasos_unicos[len(pasos_unicos)//2], pasos_unicos[-1]]:\n",
    "            subset = self.df_all[self.df_all['Paso'] == paso]\n",
    "            if len(subset) > 0:\n",
    "                best_model = subset[self.models].mean().idxmin()\n",
    "                best_score = subset[best_model].mean()\n",
    "                \n",
    "                recommendations.append(f\"\\nPaso {paso}:\")\n",
    "                recommendations.append(f\"  Modelo Recomendado: {best_model}\")\n",
    "                recommendations.append(f\"  ECRPS Promedio: {best_score:.6f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 6. Estrategia de ensamble\n",
    "        recommendations.append(\"6. ESTRATEGIA DE ENSAMBLE SUGERIDA\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        # Top 3 modelos complementarios\n",
    "        top3 = self.df_all[self.models].mean().nsmallest(3)\n",
    "        recommendations.append(\"Combinar los siguientes modelos:\")\n",
    "        for i, (model, score) in enumerate(top3.items(), 1):\n",
    "            recommendations.append(f\"{i}. {model} (ECRPS: {score:.6f})\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(\"Justificaci√≥n:\")\n",
    "        recommendations.append(\"  - Estos modelos muestran el mejor rendimiento promedio\")\n",
    "        recommendations.append(\"  - Un ensamble puede capturar fortalezas complementarias\")\n",
    "        recommendations.append(\"  - Reduce el riesgo de seleccionar un modelo sub√≥ptimo\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 7. Modelos con dominancia estad√≠stica\n",
    "        recommendations.append(\"7. MODELOS CON DOMINANCIA ESTAD√çSTICA\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        dominance_scores = []\n",
    "        for model in self.models:\n",
    "            wins = 0\n",
    "            for other_model in self.models:\n",
    "                if model != other_model:\n",
    "                    errors1 = self.df_all[model].values\n",
    "                    errors2 = self.df_all[other_model].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = self.df_all[model].mean() - self.df_all[other_model].mean()\n",
    "                    \n",
    "                    if p_val < 0.05 and mean_diff < 0:\n",
    "                        wins += 1\n",
    "            \n",
    "            dominance_scores.append((model, wins))\n",
    "        \n",
    "        dominance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        recommendations.append(\"Modelos estad√≠sticamente superiores (test Diebold-Mariano):\")\n",
    "        for i, (model, wins) in enumerate(dominance_scores[:5], 1):\n",
    "            recommendations.append(f\"{i}. {model}: {wins} victorias significativas\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 8. Reglas de decisi√≥n\n",
    "        recommendations.append(\"8. REGLAS DE DECISI√ìN SUGERIDAS\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Reglas por escenario\n",
    "        for escenario in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']:\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            if len(subset) > 0:\n",
    "                top2 = subset[self.models].mean().nsmallest(2)\n",
    "                \n",
    "                if escenario == 'Estacionario_Lineal':\n",
    "                    recommendations.append(\"SI el proceso es ESTACIONARIO y LINEAL:\")\n",
    "                elif escenario == 'No_Estacionario_Lineal':\n",
    "                    recommendations.append(\"SI el proceso es NO ESTACIONARIO y LINEAL:\")\n",
    "                else:\n",
    "                    recommendations.append(\"SI el proceso es NO LINEAL:\")\n",
    "                \n",
    "                recommendations.append(f\"  ‚Üí Primera opci√≥n: {top2.index[0]}\")\n",
    "                recommendations.append(f\"  ‚Üí Segunda opci√≥n: {top2.index[1]}\")\n",
    "                recommendations.append(\"\")\n",
    "        \n",
    "        # Reglas por distribuci√≥n\n",
    "        recommendations.append(\"SI la distribuci√≥n de errores:\")\n",
    "        for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "            subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "            if len(subset) > 0:\n",
    "                best = subset[self.models].mean().idxmin()\n",
    "                recommendations.append(f\"  ‚Ä¢ Es {dist} ‚Üí Usar {best}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Reglas por varianza\n",
    "        recommendations.append(\"SI el nivel de varianza:\")\n",
    "        variances = sorted(self.df_all['Varianza'].unique())\n",
    "        if len(variances) >= 2:\n",
    "            low_var = variances[0]\n",
    "            high_var = variances[-1]\n",
    "            \n",
    "            subset_low = self.df_all[self.df_all['Varianza'] == low_var]\n",
    "            subset_high = self.df_all[self.df_all['Varianza'] == high_var]\n",
    "            \n",
    "            best_low = subset_low[self.models].mean().idxmin()\n",
    "            best_high = subset_high[self.models].mean().idxmin()\n",
    "            \n",
    "            recommendations.append(f\"  ‚Ä¢ Es bajo ({low_var}) ‚Üí Usar {best_low}\")\n",
    "            recommendations.append(f\"  ‚Ä¢ Es alto ({high_var}) ‚Üí Usar {best_high}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 9. Conclusiones finales\n",
    "        recommendations.append(\"9. CONCLUSIONES PRINCIPALES\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(f\"‚Ä¢ El modelo {overall_best} muestra el mejor rendimiento general\")\n",
    "        recommendations.append(f\"  con ECRPS promedio de {self.df_all[overall_best].mean():.6f}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # An√°lisis de robustez\n",
    "        most_robust = min(cv_scores.items(), key=lambda x: x[1])[0]\n",
    "        recommendations.append(f\"‚Ä¢ El modelo m√°s robusto (menor CV) es {most_robust}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Comparaci√≥n estacionario vs no estacionario\n",
    "        est_best = self.df_estacionario[self.models].mean().idxmin()\n",
    "        no_est_best = self.df_no_estacionario[self.models].mean().idxmin()\n",
    "        \n",
    "        if est_best == no_est_best:\n",
    "            recommendations.append(f\"‚Ä¢ {est_best} es consistentemente superior en procesos\")\n",
    "            recommendations.append(\"  estacionarios y no estacionarios\")\n",
    "        else:\n",
    "            recommendations.append(f\"‚Ä¢ Para procesos estacionarios: preferir {est_best}\")\n",
    "            recommendations.append(f\"‚Ä¢ Para procesos no estacionarios: preferir {no_est_best}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # An√°lisis de no linealidad\n",
    "        nl_best = self.df_no_lineal[self.models].mean().idxmin()\n",
    "        recommendations.append(f\"‚Ä¢ Para procesos no lineales: {nl_best} es la mejor opci√≥n\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Recomendaci√≥n de ensamble\n",
    "        recommendations.append(\"‚Ä¢ Se recomienda implementar un ENSAMBLE de los top 3 modelos\")\n",
    "        recommendations.append(\"  para maximizar robustez y rendimiento\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Consideraciones pr√°cticas\n",
    "        recommendations.append(\"10. CONSIDERACIONES PR√ÅCTICAS\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(\"Factores a considerar en la selecci√≥n:\")\n",
    "        recommendations.append(\"  1. Costo computacional vs ganancia en precisi√≥n\")\n",
    "        recommendations.append(\"  2. Robustez ante cambios en la distribuci√≥n de errores\")\n",
    "        recommendations.append(\"  3. Consistencia a trav√©s de horizontes de predicci√≥n\")\n",
    "        recommendations.append(\"  4. Facilidad de interpretaci√≥n y explicabilidad\")\n",
    "        recommendations.append(\"  5. Disponibilidad de recursos para implementaci√≥n\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Trade-offs identificados\n",
    "        recommendations.append(\"Trade-offs identificados:\")\n",
    "        \n",
    "        # Mejor vs m√°s robusto\n",
    "        if overall_best != most_robust:\n",
    "            recommendations.append(f\"  ‚Ä¢ Rendimiento vs Robustez: {overall_best} (mejor) vs {most_robust} (m√°s robusto)\")\n",
    "        \n",
    "        # Modelos especializados\n",
    "        recommendations.append(\"  ‚Ä¢ Algunos modelos son especialistas en escenarios espec√≠ficos\")\n",
    "        recommendations.append(\"  ‚Ä¢ Otros modelos son generalistas con buen rendimiento global\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Guardar recomendaciones\n",
    "        with open(f'{output_dir}/8_recomendaciones.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(recommendations))\n",
    "        \n",
    "        print('\\n'.join(recommendations))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO DE EJECUCI√ìN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para ejecutar el an√°lisis completo\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AN√ÅLISIS COMPREHENSIVO DE MODELOS DE PREDICCI√ìN PROBABIL√çSTICA\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Crear analizador\n",
    "    try:\n",
    "        analyzer = ModelPerformanceAnalyzer()\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nERROR: No se encontraron los archivos de datos\")\n",
    "        print(\"Verifica que existan los siguientes archivos:\")\n",
    "        print(\"  - ./Datos/estacionario.xlsx\")\n",
    "        print(\"  - ./Datos/no_estacionario.xlsx\")\n",
    "        print(\"  - ./Datos/no_lineal.xlsx\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR al cargar datos: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Ejecutar an√°lisis completo\n",
    "    output_directory = 'resultados_analisis_completo'\n",
    "    \n",
    "    try:\n",
    "        analyzer.generate_full_report(output_dir=output_directory)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úì An√°lisis completado exitosamente\")\n",
    "        print(f\"‚úì Todos los resultados guardados en: {output_directory}/\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        print(\"Archivos generados:\")\n",
    "        print(\"  üìä An√°lisis de caracter√≠sticas del DGP\")\n",
    "        print(\"  üìà Efectos de distribuci√≥n y varianza\")\n",
    "        print(\"  üéØ An√°lisis de horizonte de predicci√≥n\")\n",
    "        print(\"  üîÑ Interacciones complejas\")\n",
    "        print(\"  üí™ M√©tricas de robustez\")\n",
    "        print(\"  üìâ Tests de Diebold-Mariano\")\n",
    "        print(\"  üë§ Perfiles individuales por modelo\")\n",
    "        print(\"  üí° Recomendaciones estrat√©gicas\")\n",
    "        print(\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR durante el an√°lisis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1784b",
   "metadata": {},
   "source": [
    "# Pre analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f559ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores √∫nicos encontrados en 'Tipo de Modelo':\n",
      "['AR(1)' 'AR(2)' 'MA(1)' 'MA(2)' 'ARMA(1,1)' 'ARMA(2,2)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paso</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Distribuci√≥n</th>\n",
       "      <th>Varianza error</th>\n",
       "      <th>AREPD</th>\n",
       "      <th>AV-MCPS</th>\n",
       "      <th>Block Bootstrapping</th>\n",
       "      <th>DeepAR</th>\n",
       "      <th>EnCQR-LSTM</th>\n",
       "      <th>LSPM</th>\n",
       "      <th>LSPMW</th>\n",
       "      <th>MCPS</th>\n",
       "      <th>Sieve Bootstrap</th>\n",
       "      <th>Mejor Modelo</th>\n",
       "      <th>Escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.294667</td>\n",
       "      <td>0.355344</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.263419</td>\n",
       "      <td>0.306622</td>\n",
       "      <td>0.440706</td>\n",
       "      <td>0.431452</td>\n",
       "      <td>0.285427</td>\n",
       "      <td>0.248691</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.604540</td>\n",
       "      <td>0.307449</td>\n",
       "      <td>0.254264</td>\n",
       "      <td>0.273001</td>\n",
       "      <td>0.565522</td>\n",
       "      <td>0.470424</td>\n",
       "      <td>0.474111</td>\n",
       "      <td>0.285430</td>\n",
       "      <td>0.254193</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.273622</td>\n",
       "      <td>0.276230</td>\n",
       "      <td>0.258388</td>\n",
       "      <td>0.315765</td>\n",
       "      <td>0.269452</td>\n",
       "      <td>0.520070</td>\n",
       "      <td>0.517876</td>\n",
       "      <td>0.337990</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.261423</td>\n",
       "      <td>0.279697</td>\n",
       "      <td>0.254453</td>\n",
       "      <td>0.289443</td>\n",
       "      <td>0.269285</td>\n",
       "      <td>0.287989</td>\n",
       "      <td>0.288111</td>\n",
       "      <td>0.282999</td>\n",
       "      <td>0.254655</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.626252</td>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.254842</td>\n",
       "      <td>0.272827</td>\n",
       "      <td>0.639437</td>\n",
       "      <td>0.763960</td>\n",
       "      <td>0.753066</td>\n",
       "      <td>0.308347</td>\n",
       "      <td>0.254952</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.082513</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>0.953857</td>\n",
       "      <td>1.116455</td>\n",
       "      <td>1.053269</td>\n",
       "      <td>2.030504</td>\n",
       "      <td>2.165650</td>\n",
       "      <td>0.990087</td>\n",
       "      <td>0.954156</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>2</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.903173</td>\n",
       "      <td>0.971148</td>\n",
       "      <td>0.954440</td>\n",
       "      <td>1.005615</td>\n",
       "      <td>1.518301</td>\n",
       "      <td>1.431610</td>\n",
       "      <td>1.522051</td>\n",
       "      <td>1.141614</td>\n",
       "      <td>0.954065</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>3</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.310542</td>\n",
       "      <td>1.021845</td>\n",
       "      <td>0.976235</td>\n",
       "      <td>1.002865</td>\n",
       "      <td>1.615073</td>\n",
       "      <td>1.026140</td>\n",
       "      <td>1.036051</td>\n",
       "      <td>1.484601</td>\n",
       "      <td>0.962417</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>4</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.324103</td>\n",
       "      <td>0.968827</td>\n",
       "      <td>0.961514</td>\n",
       "      <td>0.977739</td>\n",
       "      <td>1.072897</td>\n",
       "      <td>1.453428</td>\n",
       "      <td>1.530595</td>\n",
       "      <td>1.125230</td>\n",
       "      <td>0.960919</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>5</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.522689</td>\n",
       "      <td>1.011090</td>\n",
       "      <td>0.960863</td>\n",
       "      <td>1.001003</td>\n",
       "      <td>1.123328</td>\n",
       "      <td>1.061991</td>\n",
       "      <td>1.038016</td>\n",
       "      <td>1.179879</td>\n",
       "      <td>0.962942</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Paso Tipo de Modelo Distribuci√≥n  Varianza error     AREPD   AV-MCPS  \\\n",
       "0       1          AR(1)       normal             0.2  0.294667  0.355344   \n",
       "2       2          AR(1)       normal             0.2  0.604540  0.307449   \n",
       "4       3          AR(1)       normal             0.2  0.273622  0.276230   \n",
       "6       4          AR(1)       normal             0.2  0.261423  0.279697   \n",
       "8       5          AR(1)       normal             0.2  0.626252  0.273680   \n",
       "...   ...            ...          ...             ...       ...       ...   \n",
       "1309    1      ARMA(2,2)      mixture             3.0  1.082513  0.999066   \n",
       "1311    2      ARMA(2,2)      mixture             3.0  1.903173  0.971148   \n",
       "1313    3      ARMA(2,2)      mixture             3.0  2.310542  1.021845   \n",
       "1315    4      ARMA(2,2)      mixture             3.0  1.324103  0.968827   \n",
       "1317    5      ARMA(2,2)      mixture             3.0  1.522689  1.011090   \n",
       "\n",
       "      Block Bootstrapping    DeepAR  EnCQR-LSTM      LSPM     LSPMW      MCPS  \\\n",
       "0                0.248447  0.263419    0.306622  0.440706  0.431452  0.285427   \n",
       "2                0.254264  0.273001    0.565522  0.470424  0.474111  0.285430   \n",
       "4                0.258388  0.315765    0.269452  0.520070  0.517876  0.337990   \n",
       "6                0.254453  0.289443    0.269285  0.287989  0.288111  0.282999   \n",
       "8                0.254842  0.272827    0.639437  0.763960  0.753066  0.308347   \n",
       "...                   ...       ...         ...       ...       ...       ...   \n",
       "1309             0.953857  1.116455    1.053269  2.030504  2.165650  0.990087   \n",
       "1311             0.954440  1.005615    1.518301  1.431610  1.522051  1.141614   \n",
       "1313             0.976235  1.002865    1.615073  1.026140  1.036051  1.484601   \n",
       "1315             0.961514  0.977739    1.072897  1.453428  1.530595  1.125230   \n",
       "1317             0.960863  1.001003    1.123328  1.061991  1.038016  1.179879   \n",
       "\n",
       "      Sieve Bootstrap         Mejor Modelo            Escenario  \n",
       "0            0.248691  Block Bootstrapping  Estacionario_Lineal  \n",
       "2            0.254193      Sieve Bootstrap  Estacionario_Lineal  \n",
       "4            0.258039      Sieve Bootstrap  Estacionario_Lineal  \n",
       "6            0.254655  Block Bootstrapping  Estacionario_Lineal  \n",
       "8            0.254952  Block Bootstrapping  Estacionario_Lineal  \n",
       "...               ...                  ...                  ...  \n",
       "1309         0.954156  Block Bootstrapping  Estacionario_Lineal  \n",
       "1311         0.954065      Sieve Bootstrap  Estacionario_Lineal  \n",
       "1313         0.962417      Sieve Bootstrap  Estacionario_Lineal  \n",
       "1315         0.960919      Sieve Bootstrap  Estacionario_Lineal  \n",
       "1317         0.962942  Block Bootstrapping  Estacionario_Lineal  \n",
       "\n",
       "[600 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "    \n",
    "estacionario = pd.read_excel(\"./Datos/estacionario.xlsx\")\n",
    "\n",
    "estacionario = estacionario.drop_duplicates()\n",
    "estacionario = estacionario[estacionario[\"Paso\"] != \"Promedio\"]\n",
    "\n",
    "def determinar_tipo_modelo_mejorado(row):\n",
    "    \"\"\"\n",
    "    Determina el tipo de modelo (AR, MA, ARMA) y su orden a partir de los valores\n",
    "    en las columnas 'Valores de AR' y 'Valores MA'.\n",
    "    \"\"\"\n",
    "    ar_str = str(row['Valores de AR'])\n",
    "    ma_str = str(row['Valores MA'])\n",
    "    \n",
    "    # Expresi√≥n regular para encontrar n√∫meros (enteros o decimales, positivos o negativos)\n",
    "    regex_numeros = r'-?\\d+\\.?\\d*'\n",
    "    \n",
    "    # Cuenta cu√°ntos n√∫meros v√°lidos hay en cada string\n",
    "    p = len(re.findall(regex_numeros, ar_str))\n",
    "    q = len(re.findall(regex_numeros, ma_str))\n",
    "    \n",
    "    if p > 0 and q == 0:\n",
    "        return f\"AR({p})\"\n",
    "    elif p == 0 and q > 0:\n",
    "        return f\"MA({q})\"\n",
    "    elif p > 0 and q > 0:\n",
    "        return f\"ARMA({p},{q})\"\n",
    "    else:\n",
    "        return None # O \"Ruido Blanco\" si p=0 y q=0\n",
    "\n",
    "# Aplica la funci√≥n mejorada para crear la columna \"Tipo de Modelo\"\n",
    "estacionario['Tipo de Modelo'] = estacionario.apply(determinar_tipo_modelo_mejorado, axis=1)\n",
    "\n",
    "# Imprime los valores √∫nicos de la columna Tipo de modelo para verificar\n",
    "print(\"Valores √∫nicos encontrados en 'Tipo de Modelo':\")\n",
    "print(estacionario['Tipo de Modelo'].unique())\n",
    "\n",
    "# Ordena las columnas 'Paso' y 'Tipo de modelo' al inicio\n",
    "cols = estacionario.columns.tolist()\n",
    "# Aseguramos que las columnas existan antes de moverlas\n",
    "if 'Paso' in cols:\n",
    "    cols.insert(0, cols.pop(cols.index('Paso')))\n",
    "if 'Tipo de Modelo' in cols:\n",
    "    cols.insert(1, cols.pop(cols.index('Tipo de Modelo')))\n",
    "\n",
    "estacionario = estacionario.reindex(columns=cols)\n",
    "\n",
    "\n",
    "# Borra las columnas originales 'Valores de AR' y 'Valores MA'\n",
    "estacionario = estacionario.drop(columns=['Valores de AR', 'Valores MA'])\n",
    "estacionario[\"Escenario\"] = \"Estacionario_Lineal\"\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "estacionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba423eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paso</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Distribuci√≥n</th>\n",
       "      <th>Varianza error</th>\n",
       "      <th>AREPD</th>\n",
       "      <th>AV-MCPS</th>\n",
       "      <th>Block Bootstrapping</th>\n",
       "      <th>DeepAR</th>\n",
       "      <th>EnCQR-LSTM</th>\n",
       "      <th>LSPM</th>\n",
       "      <th>LSPMW</th>\n",
       "      <th>MCPS</th>\n",
       "      <th>Sieve Bootstrap</th>\n",
       "      <th>Mejor Modelo</th>\n",
       "      <th>Escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.860823</td>\n",
       "      <td>0.258474</td>\n",
       "      <td>0.253635</td>\n",
       "      <td>0.319481</td>\n",
       "      <td>0.488711</td>\n",
       "      <td>0.367279</td>\n",
       "      <td>0.360494</td>\n",
       "      <td>0.270816</td>\n",
       "      <td>0.273828</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.244128</td>\n",
       "      <td>0.528968</td>\n",
       "      <td>0.275061</td>\n",
       "      <td>0.438099</td>\n",
       "      <td>0.322919</td>\n",
       "      <td>0.426187</td>\n",
       "      <td>0.430296</td>\n",
       "      <td>0.576792</td>\n",
       "      <td>0.272952</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.799818</td>\n",
       "      <td>0.864295</td>\n",
       "      <td>0.272406</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.396481</td>\n",
       "      <td>0.642530</td>\n",
       "      <td>0.639134</td>\n",
       "      <td>0.269655</td>\n",
       "      <td>0.275661</td>\n",
       "      <td>MCPS</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.912421</td>\n",
       "      <td>0.481159</td>\n",
       "      <td>0.255186</td>\n",
       "      <td>0.291577</td>\n",
       "      <td>0.495882</td>\n",
       "      <td>0.341570</td>\n",
       "      <td>0.341227</td>\n",
       "      <td>0.533788</td>\n",
       "      <td>0.275948</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.822771</td>\n",
       "      <td>0.792130</td>\n",
       "      <td>0.257461</td>\n",
       "      <td>0.658698</td>\n",
       "      <td>1.291283</td>\n",
       "      <td>0.981902</td>\n",
       "      <td>0.969842</td>\n",
       "      <td>1.455485</td>\n",
       "      <td>0.338116</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76.766114</td>\n",
       "      <td>5.668568</td>\n",
       "      <td>0.965836</td>\n",
       "      <td>7.254422</td>\n",
       "      <td>13.176312</td>\n",
       "      <td>4.421885</td>\n",
       "      <td>4.173484</td>\n",
       "      <td>20.231134</td>\n",
       "      <td>2.612414</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>2</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.630681</td>\n",
       "      <td>6.161741</td>\n",
       "      <td>0.974398</td>\n",
       "      <td>8.767931</td>\n",
       "      <td>9.287902</td>\n",
       "      <td>1.733689</td>\n",
       "      <td>1.596168</td>\n",
       "      <td>21.251698</td>\n",
       "      <td>1.956761</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>3</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86.539087</td>\n",
       "      <td>10.452450</td>\n",
       "      <td>0.982561</td>\n",
       "      <td>24.631292</td>\n",
       "      <td>18.639842</td>\n",
       "      <td>6.195609</td>\n",
       "      <td>5.953120</td>\n",
       "      <td>23.480752</td>\n",
       "      <td>3.623684</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>4</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>93.057798</td>\n",
       "      <td>13.911382</td>\n",
       "      <td>0.958507</td>\n",
       "      <td>27.567728</td>\n",
       "      <td>17.852720</td>\n",
       "      <td>7.288522</td>\n",
       "      <td>6.952830</td>\n",
       "      <td>28.286507</td>\n",
       "      <td>4.681807</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>5</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.120410</td>\n",
       "      <td>13.899543</td>\n",
       "      <td>0.955009</td>\n",
       "      <td>17.064311</td>\n",
       "      <td>8.945594</td>\n",
       "      <td>4.716275</td>\n",
       "      <td>4.320090</td>\n",
       "      <td>31.056370</td>\n",
       "      <td>4.177879</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Paso Tipo de Modelo Distribuci√≥n  Varianza error      AREPD    AV-MCPS  \\\n",
       "0      1   ARIMA(0,1,0)       normal             0.2   1.860823   0.258474   \n",
       "1      2   ARIMA(0,1,0)       normal             0.2   1.244128   0.528968   \n",
       "2      3   ARIMA(0,1,0)       normal             0.2   1.799818   0.864295   \n",
       "3      4   ARIMA(0,1,0)       normal             0.2   1.912421   0.481159   \n",
       "4      5   ARIMA(0,1,0)       normal             0.2   2.822771   0.792130   \n",
       "..   ...            ...          ...             ...        ...        ...   \n",
       "834    1   ARIMA(2,1,2)      mixture             3.0  76.766114   5.668568   \n",
       "835    2   ARIMA(2,1,2)      mixture             3.0  80.630681   6.161741   \n",
       "836    3   ARIMA(2,1,2)      mixture             3.0  86.539087  10.452450   \n",
       "837    4   ARIMA(2,1,2)      mixture             3.0  93.057798  13.911382   \n",
       "838    5   ARIMA(2,1,2)      mixture             3.0  99.120410  13.899543   \n",
       "\n",
       "     Block Bootstrapping     DeepAR  EnCQR-LSTM      LSPM     LSPMW  \\\n",
       "0               0.253635   0.319481    0.488711  0.367279  0.360494   \n",
       "1               0.275061   0.438099    0.322919  0.426187  0.430296   \n",
       "2               0.272406   0.291500    0.396481  0.642530  0.639134   \n",
       "3               0.255186   0.291577    0.495882  0.341570  0.341227   \n",
       "4               0.257461   0.658698    1.291283  0.981902  0.969842   \n",
       "..                   ...        ...         ...       ...       ...   \n",
       "834             0.965836   7.254422   13.176312  4.421885  4.173484   \n",
       "835             0.974398   8.767931    9.287902  1.733689  1.596168   \n",
       "836             0.982561  24.631292   18.639842  6.195609  5.953120   \n",
       "837             0.958507  27.567728   17.852720  7.288522  6.952830   \n",
       "838             0.955009  17.064311    8.945594  4.716275  4.320090   \n",
       "\n",
       "          MCPS  Sieve Bootstrap         Mejor Modelo               Escenario  \n",
       "0     0.270816         0.273828  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "1     0.576792         0.272952      Sieve Bootstrap  No_Estacionario_Lineal  \n",
       "2     0.269655         0.275661                 MCPS  No_Estacionario_Lineal  \n",
       "3     0.533788         0.275948  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "4     1.455485         0.338116  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "..         ...              ...                  ...                     ...  \n",
       "834  20.231134         2.612414  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "835  21.251698         1.956761  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "836  23.480752         3.623684  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "837  28.286507         4.681807  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "838  31.056370         4.177879  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "\n",
       "[700 rows x 15 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_estacionario = pd.read_excel(\"./Datos/no_estacionario.xlsx\")\n",
    "no_estacionario.drop(columns=['Valores de AR', 'Valores MA'], inplace=True)\n",
    "no_estacionario[\"Escenario\"] = \"No_Estacionario_Lineal\"\n",
    "no_estacionario = no_estacionario[no_estacionario[\"Paso\"] != \"Promedio\"]\n",
    "no_estacionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e7ce88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paso</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Distribuci√≥n</th>\n",
       "      <th>Varianza error</th>\n",
       "      <th>AREPD</th>\n",
       "      <th>AV-MCPS</th>\n",
       "      <th>Block Bootstrapping</th>\n",
       "      <th>DeepAR</th>\n",
       "      <th>EnCQR-LSTM</th>\n",
       "      <th>LSPM</th>\n",
       "      <th>LSPMW</th>\n",
       "      <th>MCPS</th>\n",
       "      <th>Sieve Bootstrap</th>\n",
       "      <th>Mejor Modelo</th>\n",
       "      <th>Escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.257043</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.251524</td>\n",
       "      <td>0.263274</td>\n",
       "      <td>0.257984</td>\n",
       "      <td>0.285655</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>0.257015</td>\n",
       "      <td>0.251188</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.305723</td>\n",
       "      <td>0.383340</td>\n",
       "      <td>0.288529</td>\n",
       "      <td>0.297164</td>\n",
       "      <td>0.324101</td>\n",
       "      <td>0.316846</td>\n",
       "      <td>0.319675</td>\n",
       "      <td>0.347319</td>\n",
       "      <td>0.290022</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.292055</td>\n",
       "      <td>0.258555</td>\n",
       "      <td>0.287265</td>\n",
       "      <td>0.275374</td>\n",
       "      <td>0.278881</td>\n",
       "      <td>0.320347</td>\n",
       "      <td>0.320181</td>\n",
       "      <td>0.270736</td>\n",
       "      <td>0.262183</td>\n",
       "      <td>AV-MCPS</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.298469</td>\n",
       "      <td>0.269290</td>\n",
       "      <td>0.263802</td>\n",
       "      <td>0.255605</td>\n",
       "      <td>0.270449</td>\n",
       "      <td>0.290893</td>\n",
       "      <td>0.290581</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.258734</td>\n",
       "      <td>DeepAR</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.298007</td>\n",
       "      <td>0.368342</td>\n",
       "      <td>0.501202</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>0.326254</td>\n",
       "      <td>0.329508</td>\n",
       "      <td>0.423889</td>\n",
       "      <td>0.442319</td>\n",
       "      <td>AREPD</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.164445</td>\n",
       "      <td>0.992519</td>\n",
       "      <td>0.962026</td>\n",
       "      <td>0.989297</td>\n",
       "      <td>1.046459</td>\n",
       "      <td>0.971555</td>\n",
       "      <td>1.076860</td>\n",
       "      <td>1.003262</td>\n",
       "      <td>0.961513</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>2</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.191648</td>\n",
       "      <td>1.034591</td>\n",
       "      <td>0.986347</td>\n",
       "      <td>1.077081</td>\n",
       "      <td>0.972263</td>\n",
       "      <td>0.957417</td>\n",
       "      <td>0.986525</td>\n",
       "      <td>0.963721</td>\n",
       "      <td>0.984072</td>\n",
       "      <td>LSPM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>3</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.193252</td>\n",
       "      <td>1.387456</td>\n",
       "      <td>1.012627</td>\n",
       "      <td>0.981861</td>\n",
       "      <td>0.955903</td>\n",
       "      <td>0.987603</td>\n",
       "      <td>0.977201</td>\n",
       "      <td>1.041540</td>\n",
       "      <td>1.009812</td>\n",
       "      <td>EnCQR-LSTM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>4</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.229893</td>\n",
       "      <td>1.182221</td>\n",
       "      <td>1.124342</td>\n",
       "      <td>0.983326</td>\n",
       "      <td>0.960088</td>\n",
       "      <td>1.036372</td>\n",
       "      <td>0.978720</td>\n",
       "      <td>1.029875</td>\n",
       "      <td>1.103310</td>\n",
       "      <td>EnCQR-LSTM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>5</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.149135</td>\n",
       "      <td>1.068179</td>\n",
       "      <td>1.046778</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.961731</td>\n",
       "      <td>0.961208</td>\n",
       "      <td>0.977492</td>\n",
       "      <td>1.069917</td>\n",
       "      <td>1.059325</td>\n",
       "      <td>LSPM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Paso Tipo de Modelo Distribuci√≥n  Varianza error     AREPD   AV-MCPS  \\\n",
       "0      1     SETAR(2,1)       normal             0.2  0.257043  0.253521   \n",
       "1      2     SETAR(2,1)       normal             0.2  0.305723  0.383340   \n",
       "2      3     SETAR(2,1)       normal             0.2  0.292055  0.258555   \n",
       "3      4     SETAR(2,1)       normal             0.2  0.298469  0.269290   \n",
       "4      5     SETAR(2,1)       normal             0.2  0.298007  0.368342   \n",
       "..   ...            ...          ...             ...       ...       ...   \n",
       "834    1     SETAR(2,3)      mixture             3.0  1.164445  0.992519   \n",
       "835    2     SETAR(2,3)      mixture             3.0  1.191648  1.034591   \n",
       "836    3     SETAR(2,3)      mixture             3.0  1.193252  1.387456   \n",
       "837    4     SETAR(2,3)      mixture             3.0  1.229893  1.182221   \n",
       "838    5     SETAR(2,3)      mixture             3.0  1.149135  1.068179   \n",
       "\n",
       "     Block Bootstrapping    DeepAR  EnCQR-LSTM      LSPM     LSPMW      MCPS  \\\n",
       "0               0.251524  0.263274    0.257984  0.285655  0.282110  0.257015   \n",
       "1               0.288529  0.297164    0.324101  0.316846  0.319675  0.347319   \n",
       "2               0.287265  0.275374    0.278881  0.320347  0.320181  0.270736   \n",
       "3               0.263802  0.255605    0.270449  0.290893  0.290581  0.329900   \n",
       "4               0.501202  0.323900    0.348571  0.326254  0.329508  0.423889   \n",
       "..                   ...       ...         ...       ...       ...       ...   \n",
       "834             0.962026  0.989297    1.046459  0.971555  1.076860  1.003262   \n",
       "835             0.986347  1.077081    0.972263  0.957417  0.986525  0.963721   \n",
       "836             1.012627  0.981861    0.955903  0.987603  0.977201  1.041540   \n",
       "837             1.124342  0.983326    0.960088  1.036372  0.978720  1.029875   \n",
       "838             1.046778  0.999776    0.961731  0.961208  0.977492  1.069917   \n",
       "\n",
       "     Sieve Bootstrap         Mejor Modelo               Escenario  \n",
       "0           0.251188      Sieve Bootstrap  No_Lineal_Estacionario  \n",
       "1           0.290022  Block Bootstrapping  No_Lineal_Estacionario  \n",
       "2           0.262183              AV-MCPS  No_Lineal_Estacionario  \n",
       "3           0.258734               DeepAR  No_Lineal_Estacionario  \n",
       "4           0.442319                AREPD  No_Lineal_Estacionario  \n",
       "..               ...                  ...                     ...  \n",
       "834         0.961513      Sieve Bootstrap  No_Lineal_Estacionario  \n",
       "835         0.984072                 LSPM  No_Lineal_Estacionario  \n",
       "836         1.009812           EnCQR-LSTM  No_Lineal_Estacionario  \n",
       "837         1.103310           EnCQR-LSTM  No_Lineal_Estacionario  \n",
       "838         1.059325                 LSPM  No_Lineal_Estacionario  \n",
       "\n",
       "[700 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_lineal = pd.read_excel(\"./Datos/no_lineal.xlsx\")\n",
    "no_lineal = no_lineal[no_lineal[\"Paso\"] != \"Promedio\"]\n",
    "no_lineal[\"Escenario\"] = \"No_Lineal_Estacionario\"\n",
    "no_lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f2c36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une los tres DataFrames en uno solo uno debajo de otro\n",
    "df_all = pd.concat([estacionario, no_estacionario, no_lineal], ignore_index=True)\n",
    "# Guarda el DataFrame combinado en un archivo Excel\n",
    "df_all.to_excel(\"./Datos/datos_combinados.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf7f35",
   "metadata": {},
   "source": [
    "# Analisis con la correcion del profe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac9a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio 'resultados_completos_media_mediana' creado.\n",
      "‚úì Datos cargados exitosamente.\n",
      "\n",
      "================================================================================\n",
      "--- INICIANDO AN√ÅLISIS BASADO EN LA MEAN ---\n",
      "================================================================================\n",
      " -> 1. Analizando por estacionariedad (mean)...\n",
      " -> 2. Analizando por linealidad (mean)...\n",
      " -> 3. Generando heatmaps generales (mean)...\n",
      " -> 4. Analizando ECRPS vs Varianza (mean)...\n",
      " -> 5. Analizando ECRPS vs Paso (mean)...\n",
      " -> Analizando robustez y estabilidad (basado en mean)...\n",
      "\n",
      "================================================================================\n",
      "--- INICIANDO AN√ÅLISIS BASADO EN LA MEDIAN ---\n",
      "================================================================================\n",
      " -> 1. Analizando por estacionariedad (median)...\n",
      " -> 2. Analizando por linealidad (median)...\n",
      " -> 3. Generando heatmaps generales (median)...\n",
      " -> 4. Analizando ECRPS vs Varianza (median)...\n",
      " -> 5. Analizando ECRPS vs Paso (median)...\n",
      " -> Analizando robustez y estabilidad (basado en median)...\n",
      "\n",
      "================================================================================\n",
      "--- INICIANDO AN√ÅLISIS INDEPENDIENTES DE AGREGACI√ìN ---\n",
      "================================================================================\n",
      " -> Generando gr√°ficos de densidad individuales (an√°lisis √∫nico)...\n",
      " -> Realizando Test de Diebold-Mariano (an√°lisis √∫nico)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pleal\\AppData\\Local\\Temp\\ipykernel_16608\\4105642524.py:199: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annot_matrix = result_matrix.applymap(lambda x: {1: 'Gana', -1: 'Pierde', 0: 'Empate'}[x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì An√°lisis completo. Resultados guardados en la carpeta 'resultados_completos_media_mediana'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "CARPETA_RESULTADOS = \"resultados_completos_media_mediana\"\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "ESCENARIOS_ESTACIONARIOS = ['Estacionario_Lineal', 'No_Lineal_Estacionario']\n",
    "ESCENARIOS_NO_ESTACIONARIOS = ['No_Estacionario_Lineal']\n",
    "ESCENARIOS_LINEALES = ['Estacionario_Lineal', 'No_Estacionario_Lineal']\n",
    "ESCENARIOS_NO_LINEALES = ['No_Lineal_Estacionario']\n",
    "\n",
    "# ============================================================================\n",
    "# CLASE PARA TEST ESTAD√çSTICO\n",
    "# ============================================================================\n",
    "class DieboldMarianoTest:\n",
    "    @staticmethod\n",
    "    def dm_test(errors1, errors2, h=1, power=2):\n",
    "        # Implementaci√≥n del test... (sin cambios)\n",
    "        errors1, errors2 = np.array(errors1), np.array(errors2)\n",
    "        loss_diff = (errors1**power) - (errors2**power)\n",
    "        mean_diff = np.mean(loss_diff)\n",
    "        n = len(loss_diff)\n",
    "        gamma0 = np.var(loss_diff, ddof=1)\n",
    "        if h > 1:\n",
    "            gamma_sum = sum((1 - k/h) * np.cov(loss_diff[:-k], loss_diff[k:])[0, 1] for k in range(1, h))\n",
    "            variance = (gamma0 + 2 * gamma_sum) / n\n",
    "        else:\n",
    "            variance = gamma0 / n\n",
    "        dm_stat = mean_diff / np.sqrt(variance) if variance > 0 else 0\n",
    "        p_value = 2 * (1 - stats.norm.cdf(np.abs(dm_stat)))\n",
    "        return dm_stat, p_value\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE AN√ÅLISIS Y VISUALIZACI√ìN (MODIFICADAS)\n",
    "# ============================================================================\n",
    "def crear_directorio_resultados(nombre_carpeta):\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "        print(f\"Directorio '{nombre_carpeta}' creado.\")\n",
    "\n",
    "def guardar_grafico(nombre_archivo):\n",
    "    ruta_completa = os.path.join(CARPETA_RESULTADOS, nombre_archivo)\n",
    "    plt.savefig(ruta_completa, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def graficar_comparacion_barras(promedios1, promedios2, orden, etiqueta1, etiqueta2, agg_method, nombre_archivo):\n",
    "    \"\"\"Grafica la comparaci√≥n de barras para media o mediana.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    x = np.arange(len(orden))\n",
    "    width = 0.35\n",
    "    bars1 = ax.bar(x - width/2, promedios1[orden], width, label=etiqueta1, alpha=0.8, color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, promedios2[orden], width, label=etiqueta2, alpha=0.8, color='#e74c3c')\n",
    "    \n",
    "    ylabel = f'ECRPS {\"Promedio\" if agg_method == \"mean\" else \"Mediano\"} (menor es mejor)'\n",
    "    titulo = f'Comparaci√≥n de Desempe√±o ({agg_method.capitalize()})'\n",
    "    \n",
    "    ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(orden, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2., height, f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(nombre_archivo)\n",
    "\n",
    "def generar_heatmap(data, agg_method, titulo_sufijo, nombre_archivo, figsize=(14, 8)):\n",
    "    \"\"\"Genera un heatmap basado en media o mediana.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cbar_label = f'ECRPS {\"Promedio\" if agg_method == \"mean\" else \"Mediano\"}'\n",
    "    titulo = f'Heatmap: {titulo_sufijo} ({agg_method.capitalize()})'\n",
    "    \n",
    "    sns.heatmap(data, annot=True, fmt='.3f', cmap='RdYlGn_r',\n",
    "                cbar_kws={'label': cbar_label},\n",
    "                linewidths=0.5, linecolor='gray', ax=ax)\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(data.index.name, fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(nombre_archivo)\n",
    "\n",
    "def graficar_evolucion_metrica_por_tipo(df, metrica_eje_x, agg_method, xlabel, nombre_archivo_sufijo):\n",
    "    \"\"\"Genera gr√°ficos de evoluci√≥n para cada tipo de modelo, usando media o mediana.\"\"\"\n",
    "    valores_unicos = sorted(df[metrica_eje_x].unique())\n",
    "    tipos_modelo_unicos = df['Tipo de Modelo'].unique()\n",
    "    \n",
    "    for tipo in tipos_modelo_unicos:\n",
    "        df_tipo = df[df['Tipo de Modelo'] == tipo]\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        for modelo in MODELOS:\n",
    "            agregados = [df_tipo[df_tipo[metrica_eje_x] == val][modelo].agg(agg_method) for val in valores_unicos]\n",
    "            if not all(np.isnan(agregados)):\n",
    "                ax.plot(valores_unicos, agregados, marker='o', linewidth=2, markersize=8, label=modelo, alpha=0.8)\n",
    "        \n",
    "        ylabel = f'ECRPS {\"Promedio\" if agg_method == \"mean\" else \"Mediano\"}'\n",
    "        titulo = f'ECRPS vs {metrica_eje_x} ({agg_method.capitalize()}) - Tipo: {tipo}'\n",
    "        \n",
    "        ax.set_xlabel(xlabel, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(titulo, fontsize=13, fontweight='bold', pad=15)\n",
    "        ax.legend(fontsize=9, loc='best', ncol=2)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        if metrica_eje_x == 'Paso':\n",
    "            ax.set_xticks(valores_unicos)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        nombre_archivo_tipo = f'ecrps_vs_{nombre_archivo_sufijo}_tipo_{tipo.replace(\" \", \"_\").lower()}_{agg_method}.png'\n",
    "        guardar_grafico(nombre_archivo_tipo)\n",
    "\n",
    "def analizar_robustez_estabilidad(df, agg_method):\n",
    "    \"\"\"Calcula y grafica m√©tricas de robustez y estabilidad.\"\"\"\n",
    "    print(f\" -> Analizando robustez y estabilidad (basado en {agg_method})...\")\n",
    "    \n",
    "    if agg_method == 'mean':\n",
    "        # An√°lisis basado en la media (como antes)\n",
    "        metrics = [{'Modelo': m, 'Centralidad': df[m].mean(), 'Dispersion': df[m].std()} for m in MODELOS]\n",
    "        df_robust = pd.DataFrame(metrics)\n",
    "        label_centralidad = 'ECRPS Promedio (Rendimiento)'\n",
    "        label_dispersion = 'Desviaci√≥n Est√°ndar (Estabilidad)'\n",
    "        titulo_compromiso = 'Compromiso Rendimiento vs. Estabilidad (Media vs Std)'\n",
    "        \n",
    "    else: # agg_method == 'median'\n",
    "        # An√°lisis basado en la mediana (m√°s robusto a outliers)\n",
    "        metrics = [{'Modelo': m, 'Centralidad': df[m].median(), 'Dispersion': df[m].quantile(0.75) - df[m].quantile(0.25)} for m in MODELOS]\n",
    "        df_robust = pd.DataFrame(metrics)\n",
    "        label_centralidad = 'ECRPS Mediano (Rendimiento T√≠pico)'\n",
    "        label_dispersion = 'Rango Intercuart√≠lico (IQR - Estabilidad Robusta)'\n",
    "        titulo_compromiso = 'Compromiso Rendimiento vs. Estabilidad (Mediana vs IQR)'\n",
    "\n",
    "    # Gr√°fico de dispersi√≥n Rendimiento vs Estabilidad\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df_robust, x='Centralidad', y='Dispersion', hue='Modelo', s=150, alpha=0.8, ax=ax)\n",
    "    for _, row in df_robust.iterrows():\n",
    "        ax.text(row['Centralidad'], row['Dispersion'], row['Modelo'], fontsize=9, ha='left', va='bottom')\n",
    "    ax.set_xlabel(label_centralidad, fontweight='bold')\n",
    "    ax.set_ylabel(label_dispersion, fontweight='bold')\n",
    "    ax.set_title(titulo_compromiso, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(title='Modelos', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(f\"6_compromiso_rendimiento_estabilidad_{agg_method}.png\")\n",
    "\n",
    "# --- Funciones que no dependen de la agregaci√≥n (se ejecutan una sola vez) ---\n",
    "def graficar_densidades_individuales(df):\n",
    "    \"\"\"Crea un gr√°fico de densidad individual para cada modelo.\"\"\"\n",
    "    print(\" -> Generando gr√°ficos de densidad individuales (an√°lisis √∫nico)...\")\n",
    "    all_ecrps_values = df[MODELOS].values.flatten()\n",
    "    xlim_max = np.quantile(all_ecrps_values[~np.isnan(all_ecrps_values)], 0.995)\n",
    "    for modelo in MODELOS:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        sns.kdeplot(df[modelo].dropna(), fill=True, color='teal', ax=ax, lw=2.5)\n",
    "        mean_val, median_val = df[modelo].mean(), df[modelo].median()\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', label=f'Media: {mean_val:.3f}')\n",
    "        ax.axvline(median_val, color='green', linestyle=':', label=f'Mediana: {median_val:.3f}')\n",
    "        ax.set_title(f'Distribuci√≥n del ECRPS - Modelo: {modelo}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('ECRPS', fontweight='bold')\n",
    "        ax.set_ylabel('Densidad', fontweight='bold')\n",
    "        ax.set_xlim(left=0, right=xlim_max)\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        guardar_grafico(f\"7_densidad_{modelo.replace(' ', '_').lower()}.png\")\n",
    "\n",
    "def realizar_test_diebold_mariano(df):\n",
    "    \"\"\"Realiza el test de Diebold-Mariano con correcci√≥n de Bonferroni.\"\"\"\n",
    "    print(\" -> Realizando Test de Diebold-Mariano (an√°lisis √∫nico)...\")\n",
    "    pairs = list(combinations(MODELOS, 2))\n",
    "    alpha_bonferroni = 0.05 / len(pairs)\n",
    "    dm_results = []\n",
    "    for m1, m2 in pairs:\n",
    "        e1, e2 = df[m1].dropna(), df[m2].dropna()\n",
    "        min_len = min(len(e1), len(e2))\n",
    "        _, p_value = DieboldMarianoTest.dm_test(e1[:min_len], e2[:min_len])\n",
    "        winner = 'Empate' if p_value >= alpha_bonferroni else (m1 if df[m1].mean() < df[m2].mean() else m2)\n",
    "        dm_results.append({'Modelo_1': m1, 'Modelo_2': m2, 'Ganador_Bonferroni': winner})\n",
    "    \n",
    "    # Heatmap de resultados\n",
    "    result_matrix = pd.DataFrame(index=MODELOS, columns=MODELOS, data=0)\n",
    "    for _, row in pd.DataFrame(dm_results).iterrows():\n",
    "        if row['Ganador_Bonferroni'] == row['Modelo_1']:\n",
    "            result_matrix.loc[row['Modelo_1'], row['Modelo_2']], result_matrix.loc[row['Modelo_2'], row['Modelo_1']] = 1, -1\n",
    "        elif row['Ganador_Bonferroni'] == row['Modelo_2']:\n",
    "            result_matrix.loc[row['Modelo_1'], row['Modelo_2']], result_matrix.loc[row['Modelo_2'], row['Modelo_1']] = -1, 1\n",
    "\n",
    "    annot_matrix = result_matrix.applymap(lambda x: {1: 'Gana', -1: 'Pierde', 0: 'Empate'}[x])\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(result_matrix.astype(float), annot=annot_matrix, fmt='s', cmap=['red', 'lightgray', 'green'], cbar=False, ax=ax)\n",
    "    ax.set_title('Resultado Test Diebold-Mariano (con correcci√≥n de Bonferroni)', fontweight='bold')\n",
    "    guardar_grafico(\"8_dm_heatmap_bonferroni.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ============================================================================\n",
    "def main():\n",
    "    crear_directorio_resultados(CARPETA_RESULTADOS)\n",
    "    try:\n",
    "        df = pd.read_excel(RUTA_DATOS)\n",
    "        print(\"‚úì Datos cargados exitosamente.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontr√≥ el archivo en la ruta '{RUTA_DATOS}'.\")\n",
    "        return\n",
    "\n",
    "    # Bucle principal para ejecutar an√°lisis por media y mediana\n",
    "    for agg_method in ['mean', 'median']:\n",
    "        print(f\"\\n{'='*80}\\n--- INICIANDO AN√ÅLISIS BASADO EN LA {agg_method.upper()} ---\\n{'='*80}\")\n",
    "\n",
    "        # --- AN√ÅLISIS 1: ESTACIONARIEDAD ---\n",
    "        print(f\" -> 1. Analizando por estacionariedad ({agg_method})...\")\n",
    "        df_est = df[df['Escenario'].isin(ESCENARIOS_ESTACIONARIOS)]\n",
    "        df_no_est = df[df['Escenario'].isin(ESCENARIOS_NO_ESTACIONARIOS)]\n",
    "        agregados_est = df_est[MODELOS].agg(agg_method)\n",
    "        agregados_no_est = df_no_est[MODELOS].agg(agg_method)\n",
    "        orden_est = (agregados_est + agregados_no_est).sort_values().index\n",
    "        graficar_comparacion_barras(agregados_est, agregados_no_est, orden_est, 'Estacionarios', 'No Estacionarios', agg_method, f'1_comparacion_estacionariedad_{agg_method}.png')\n",
    "        \n",
    "        # --- AN√ÅLISIS 2: LINEALIDAD ---\n",
    "        print(f\" -> 2. Analizando por linealidad ({agg_method})...\")\n",
    "        df_lin = df[df['Escenario'].isin(ESCENARIOS_LINEALES)]\n",
    "        df_no_lin = df[df['Escenario'].isin(ESCENARIOS_NO_LINEALES)]\n",
    "        agregados_lin = df_lin[MODELOS].agg(agg_method)\n",
    "        agregados_no_lin = df_no_lin[MODELOS].agg(agg_method)\n",
    "        orden_lin = (agregados_lin + agregados_no_lin).sort_values().index\n",
    "        graficar_comparacion_barras(agregados_lin, agregados_no_lin, orden_lin, 'Lineales', 'No Lineales', agg_method, f'2_comparacion_linealidad_{agg_method}.png')\n",
    "\n",
    "        # --- AN√ÅLISIS 3: HEATMAPS GENERALES ---\n",
    "        print(f\" -> 3. Generando heatmaps generales ({agg_method})...\")\n",
    "        heatmap_esc_df = df.groupby('Escenario')[MODELOS].agg(agg_method)\n",
    "        generar_heatmap(heatmap_esc_df, agg_method, 'Desempe√±o por Escenario', f'3_heatmap_escenario_{agg_method}.png', figsize=(14, 6))\n",
    "        heatmap_dist_df = df.groupby('Distribuci√≥n')[MODELOS].agg(agg_method)\n",
    "        generar_heatmap(heatmap_dist_df, agg_method, 'Desempe√±o por Distribuci√≥n', f'3_heatmap_distribucion_{agg_method}.png')\n",
    "\n",
    "        # --- AN√ÅLISIS 4 & 5: EVOLUCI√ìN VS VARIANZA Y PASO ---\n",
    "        print(f\" -> 4. Analizando ECRPS vs Varianza ({agg_method})...\")\n",
    "        graficar_evolucion_metrica_por_tipo(df, 'Varianza error', agg_method, 'Varianza error', 'varianza')\n",
    "        print(f\" -> 5. Analizando ECRPS vs Paso ({agg_method})...\")\n",
    "        graficar_evolucion_metrica_por_tipo(df, 'Paso', agg_method, 'Paso (Horizonte)', 'paso')\n",
    "        \n",
    "        # --- AN√ÅLISIS 6: ROBUSTEZ Y ESTABILIDAD ---\n",
    "        analizar_robustez_estabilidad(df, agg_method)\n",
    "\n",
    "    # --- AN√ÅLISIS QUE SE EJECUTAN UNA SOLA VEZ ---\n",
    "    print(f\"\\n{'='*80}\\n--- INICIANDO AN√ÅLISIS INDEPENDIENTES DE AGREGACI√ìN ---\\n{'='*80}\")\n",
    "    # --- AN√ÅLISIS 7: DENSIDAD DE ERRORES ---\n",
    "    graficar_densidades_individuales(df)\n",
    "    \n",
    "    # --- AN√ÅLISIS 8: TEST DE DIEBOLD-MARIANO ---\n",
    "    realizar_test_diebold_mariano(df)\n",
    "    \n",
    "    print(f\"\\n‚úì An√°lisis completo. Resultados guardados en la carpeta '{CARPETA_RESULTADOS}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f9926",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b3b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio 'resultados_meta_modelo' creado.\n",
      "1. Cargando y preparando los datos para el meta-modelo...\n",
      "2. Realizando preprocesamiento (One-Hot Encoding)...\n",
      "\n",
      "3. Entrenando y evaluando los modelos recomendadores...\n",
      "\n",
      "--- Analizando: √Årbol de Decisi√≥n ---\n",
      "Precisi√≥n: 45.83%\n",
      "Reporte de Clasificaci√≥n:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              AREPD       0.00      0.00      0.00         9\n",
      "            AV-MCPS       0.00      0.00      0.00        22\n",
      "Block Bootstrapping       0.50      0.88      0.64       286\n",
      "             DeepAR       0.00      0.00      0.00        31\n",
      "         EnCQR-LSTM       0.18      0.29      0.22        35\n",
      "               LSPM       0.00      0.00      0.00        42\n",
      "              LSPMW       0.00      0.00      0.00        16\n",
      "               MCPS       0.00      0.00      0.00        20\n",
      "    Sieve Bootstrap       0.29      0.09      0.13       139\n",
      "\n",
      "           accuracy                           0.46       600\n",
      "          macro avg       0.11      0.14      0.11       600\n",
      "       weighted avg       0.32      0.46      0.35       600\n",
      "\n",
      "Generando visualizaciones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pleal\\Documents\\Unal\\Tesis\\Codigo\\Prediccion_Probabilistica\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pleal\\Documents\\Unal\\Tesis\\Codigo\\Prediccion_Probabilistica\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pleal\\Documents\\Unal\\Tesis\\Codigo\\Prediccion_Probabilistica\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Gr√°fico guardado en: resultados_meta_modelo\\feature_importance_√°rbol_de_decisi√≥n.png\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\confusion_matrix_√°rbol_de_decisi√≥n.png\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\decision_tree_visualization.png\n",
      "\n",
      "--- Analizando: Gradient Boosting ---\n",
      "Precisi√≥n: 43.17%\n",
      "Reporte de Clasificaci√≥n:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              AREPD       0.00      0.00      0.00         9\n",
      "            AV-MCPS       0.00      0.00      0.00        22\n",
      "Block Bootstrapping       0.58      0.71      0.64       286\n",
      "             DeepAR       0.21      0.26      0.23        31\n",
      "         EnCQR-LSTM       0.21      0.20      0.20        35\n",
      "               LSPM       0.18      0.10      0.12        42\n",
      "              LSPMW       0.12      0.06      0.08        16\n",
      "               MCPS       0.00      0.00      0.00        20\n",
      "    Sieve Bootstrap       0.30      0.26      0.28       139\n",
      "\n",
      "           accuracy                           0.43       600\n",
      "          macro avg       0.18      0.18      0.17       600\n",
      "       weighted avg       0.39      0.43      0.40       600\n",
      "\n",
      "Generando visualizaciones...\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\feature_importance_gradient_boosting.png\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\confusion_matrix_gradient_boosting.png\n",
      "\n",
      "‚úì An√°lisis del meta-modelo completado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "CARPETA_RESULTADOS = \"resultados_meta_modelo\"\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ============================================================================\n",
    "def crear_directorio_resultados(nombre_carpeta):\n",
    "    \"\"\"Crea la carpeta de resultados si no existe.\"\"\"\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "        print(f\"Directorio '{nombre_carpeta}' creado.\")\n",
    "\n",
    "def guardar_grafico(nombre_archivo):\n",
    "    \"\"\"Guarda la figura actual en un archivo y la cierra.\"\"\"\n",
    "    ruta_completa = os.path.join(CARPETA_RESULTADOS, nombre_archivo)\n",
    "    plt.savefig(ruta_completa, dpi=300, bbox_inches='tight')\n",
    "    print(f\" -> Gr√°fico guardado en: {ruta_completa}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Grafica la importancia de las caracter√≠sticas del modelo.\"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    df_importance = pd.DataFrame({\n",
    "        'Caracter√≠stica': feature_names,\n",
    "        'Importancia': importances\n",
    "    }).sort_values(by='Importancia', ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(df_importance['Caracter√≠stica'], df_importance['Importancia'], color='steelblue')\n",
    "    ax.set_xlabel('Importancia')\n",
    "    ax.set_title(f'Importancia de Caracter√≠sticas - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(f\"feature_importance_{model_name.replace(' ', '_').lower()}.png\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, class_labels):\n",
    "    \"\"\"Grafica la matriz de confusi√≥n normalizada.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels, normalize='true')\n",
    "    df_cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicci√≥n del Recomendador', fontweight='bold')\n",
    "    ax.set_ylabel('Mejor Modelo Real', fontweight='bold')\n",
    "    ax.set_title(f'Matriz de Confusi√≥n Normalizada - {model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(f\"confusion_matrix_{model_name.replace(' ', '_').lower()}.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal para crear y analizar el meta-modelo.\"\"\"\n",
    "    crear_directorio_resultados(CARPETA_RESULTADOS)\n",
    "    \n",
    "    # 1. Cargar y preparar los datos\n",
    "    print(\"1. Cargando y preparando los datos para el meta-modelo...\")\n",
    "    try:\n",
    "        df = pd.read_excel(RUTA_DATOS)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontr√≥ el archivo en la ruta '{RUTA_DATOS}'.\")\n",
    "        return\n",
    "\n",
    "    features = ['Escenario', 'Distribuci√≥n', 'Varianza error', 'Paso', 'Tipo de Modelo']\n",
    "    df_meta = df[features + MODELOS].copy()\n",
    "    df_meta['Mejor_Modelo'] = df_meta[MODELOS].idxmin(axis=1)\n",
    "    df_meta.dropna(subset=features, inplace=True)\n",
    "\n",
    "    X = df_meta[features]\n",
    "    y = df_meta['Mejor_Modelo']\n",
    "    \n",
    "    # 2. Preprocesamiento de caracter√≠sticas\n",
    "    print(\"2. Realizando preprocesamiento (One-Hot Encoding)...\")\n",
    "    categorical_features = ['Escenario', 'Distribuci√≥n', 'Tipo de Modelo']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    X_encoded_cats = encoder.fit_transform(X[categorical_features])\n",
    "    \n",
    "    # Obtener los nombres de las nuevas columnas codificadas\n",
    "    encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "    \n",
    "    # Combinar caracter√≠sticas num√©ricas y codificadas\n",
    "    X_numeric = X.drop(columns=categorical_features)\n",
    "    X_processed = np.hstack((X_numeric.values, X_encoded_cats))\n",
    "    \n",
    "    # Nombres de todas las caracter√≠sticas finales\n",
    "    final_feature_names = list(X_numeric.columns) + list(encoded_feature_names)\n",
    "\n",
    "    # 3. Dividir en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 4. Definir y entrenar los modelos\n",
    "    print(\"\\n3. Entrenando y evaluando los modelos recomendadores...\")\n",
    "    models_to_train = {\n",
    "        \"√Årbol de Decisi√≥n\": DecisionTreeClassifier(max_depth=5, min_samples_leaf=20, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42)\n",
    "    }\n",
    "    \n",
    "    class_labels = sorted(y.unique())\n",
    "\n",
    "    for name, model in models_to_train.items():\n",
    "        print(f\"\\n--- Analizando: {name} ---\")\n",
    "        \n",
    "        # Entrenar\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluar y mostrar reporte\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Precisi√≥n: {accuracy:.2%}\")\n",
    "        print(\"Reporte de Clasificaci√≥n:\")\n",
    "        print(classification_report(y_test, y_pred, labels=class_labels))\n",
    "        \n",
    "        # Generar visualizaciones\n",
    "        print(\"Generando visualizaciones...\")\n",
    "        plot_feature_importance(model, final_feature_names, name)\n",
    "        plot_confusion_matrix(y_test, y_pred, name, class_labels)\n",
    "        \n",
    "        # Visualizar el √°rbol de decisi√≥n si corresponde\n",
    "        if name == \"√Årbol de Decisi√≥n\":\n",
    "            fig, ax = plt.subplots(figsize=(25, 15))\n",
    "            plot_tree(model, feature_names=final_feature_names, class_names=class_labels, \n",
    "                      filled=True, rounded=True, fontsize=10, ax=ax)\n",
    "            ax.set_title(\"√Årbol de Decisi√≥n para Recomendaci√≥n de Modelos\", fontsize=20)\n",
    "            guardar_grafico(\"decision_tree_visualization.png\")\n",
    "\n",
    "    print(\"\\n‚úì An√°lisis del meta-modelo completado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1dff2",
   "metadata": {},
   "source": [
    "# Analisis Escalonado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a536136e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà                    AN√ÅLISIS DE MODELOS DE PREDICCI√ìN                          ‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚úì Datos cargados: 2000 filas, 15 columnas\n",
      "\n",
      "Escenarios encontrados: ['Estacionario_Lineal' 'No_Estacionario_Lineal' 'No_Lineal_Estacionario']\n",
      "Modelos a analizar: 9\n",
      "\n",
      "================================================================================\n",
      "INICIANDO AN√ÅLISIS COMPLETO DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[1/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: AREPD en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.6672  0.5196  0.2277  3.9991  0.5410\n",
      "2       120  0.7377  0.5050  0.2284  2.9819  0.5903\n",
      "3       120  0.7196  0.4639  0.2396  2.7209  0.5730\n",
      "4       120  0.7352  0.4171  0.2383  1.9014  0.6127\n",
      "5       120  0.6787  0.3668  0.2513  1.8688  0.5949\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 2\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.9083  0.7029  0.2489  3.9991  0.6457\n",
      "AR(2)             100  0.6557  0.3800  0.2315  2.1000  0.5655\n",
      "ARMA(1,1)         100  0.6724  0.3564  0.2439  1.7552  0.5899\n",
      "ARMA(2,2)         100  0.7003  0.4216  0.2277  2.3105  0.5691\n",
      "MA(1)             100  0.6856  0.4046  0.2284  2.2818  0.5686\n",
      "MA(2)             100  0.6236  0.3263  0.2376  1.5198  0.5398\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: AR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.6368  0.3726  0.2277  2.1000  0.5328\n",
      "mixture         120  0.7733  0.5182  0.2506  2.7428  0.6089\n",
      "normal          120  0.6923  0.3821  0.2540  1.8273  0.5894\n",
      "t-student       120  0.6869  0.4648  0.2439  2.9819  0.5569\n",
      "uniform         120  0.7491  0.5214  0.2594  3.9991  0.6035\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: mixture\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.3089  0.0900  0.2277  0.8361  0.2782\n",
      "0.5               150  0.5154  0.2160  0.3563  1.7337  0.4387\n",
      "1.0               150  0.7237  0.2231  0.5118  1.8273  0.6369\n",
      "3.0               150  1.2827  0.4539  0.8659  3.9991  1.1333\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7843\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    AREPD\n",
      "    1      ARMA(2,2)  exponential             0.2 0.227714\n",
      "    2          MA(1)  exponential             0.2 0.228436\n",
      "    1          MA(1)  exponential             0.2 0.228979\n",
      "    1          AR(2)  exponential             0.2 0.231478\n",
      "    2          MA(2)  exponential             0.2 0.237566\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    AREPD\n",
      "    3          AR(1)      mixture             3.0 2.720889\n",
      "    2          AR(1)      mixture             3.0 2.742800\n",
      "    1          AR(1)    t-student             3.0 2.804116\n",
      "    2          AR(1)    t-student             3.0 2.981865\n",
      "    1          AR(1)      uniform             3.0 3.999106\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2277\n",
      "   Peor rendimiento: 3.9991\n",
      "   Rendimiento promedio: 0.7077\n",
      "   Desviaci√≥n est√°ndar: 0.4574\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3668)\n",
      "   Paso menos estable: 1 (std=0.5196)\n",
      "   Tipo m√°s estable: MA(2) (std=0.3263)\n",
      "   Tipo menos estable: AR(1) (std=0.7029)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\AREPD\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[2/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: AV-MCPS en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.5562  0.2799  0.2247  1.1817  0.4737\n",
      "2       120  0.6416  0.4518  0.2314  4.2618  0.5514\n",
      "3       120  0.6393  0.3772  0.2233  2.6010  0.5444\n",
      "4       120  0.6419  0.3558  0.2328  1.9240  0.5611\n",
      "5       120  0.6282  0.3510  0.2233  1.5973  0.5442\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.6794  0.5279  0.2233  4.2618  0.5576\n",
      "AR(2)             100  0.5872  0.2970  0.2247  1.5801  0.5338\n",
      "ARMA(1,1)         100  0.6200  0.3513  0.2393  1.9240  0.5429\n",
      "ARMA(2,2)         100  0.6195  0.3350  0.2255  1.5674  0.5525\n",
      "MA(1)             100  0.6281  0.3494  0.2361  1.8463  0.5398\n",
      "MA(2)             100  0.5947  0.2958  0.2340  1.3417  0.5471\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: AR(2)\n",
      "‚úì Peor rendimiento en tipo: AR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.5650  0.3060  0.2233  1.5973  0.5065\n",
      "mixture         120  0.6661  0.5039  0.2487  4.2618  0.5618\n",
      "normal          120  0.6336  0.3333  0.2529  1.3774  0.5716\n",
      "t-student       120  0.6074  0.3271  0.2411  1.9240  0.5424\n",
      "uniform         120  0.6353  0.3296  0.2597  1.5801  0.5808\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: mixture\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.2771  0.0417  0.2233  0.4945  0.2670\n",
      "0.5               150  0.4663  0.1134  0.3564  1.1863  0.4242\n",
      "1.0               150  0.6178  0.1072  0.5029  1.0604  0.5851\n",
      "3.0               150  1.1248  0.3441  0.8800  4.2618  1.0285\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.8478\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  AV-MCPS\n",
      "    3          AR(1)  exponential             0.2 0.223298\n",
      "    5          AR(1)  exponential             0.2 0.223340\n",
      "    1          AR(2)  exponential             0.2 0.224732\n",
      "    5      ARMA(2,2)  exponential             0.2 0.225543\n",
      "    1          AR(1)  exponential             0.2 0.229574\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  AV-MCPS\n",
      "    3          MA(1)      mixture             3.0 1.846299\n",
      "    4          AR(1)      mixture             3.0 1.858324\n",
      "    4      ARMA(1,1)    t-student             3.0 1.923993\n",
      "    3          AR(1)      mixture             3.0 2.600968\n",
      "    2          AR(1)      mixture             3.0 4.261827\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2233\n",
      "   Peor rendimiento: 4.2618\n",
      "   Rendimiento promedio: 0.6215\n",
      "   Desviaci√≥n est√°ndar: 0.3676\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=0.2799)\n",
      "   Paso menos estable: 2 (std=0.4518)\n",
      "   Tipo m√°s estable: MA(2) (std=0.2958)\n",
      "   Tipo menos estable: AR(1) (std=0.5279)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\AV-MCPS\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[3/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: Block Bootstrapping en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.5335  0.2659  0.2215  1.0075  0.4531\n",
      "2       120  0.5447  0.2740  0.2247  1.0419  0.4690\n",
      "3       120  0.5391  0.2685  0.2246  1.0223  0.4837\n",
      "4       120  0.5414  0.2682  0.2231  1.0340  0.4847\n",
      "5       120  0.5394  0.2682  0.2227  1.0285  0.4686\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 2\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.5370  0.2683  0.2246  1.0223  0.4582\n",
      "AR(2)             100  0.5404  0.2700  0.2217  1.0419  0.4890\n",
      "ARMA(1,1)         100  0.5375  0.2692  0.2231  1.0391  0.4645\n",
      "ARMA(2,2)         100  0.5381  0.2691  0.2227  1.0365  0.4812\n",
      "MA(1)             100  0.5480  0.2721  0.2233  1.0219  0.4776\n",
      "MA(2)             100  0.5367  0.2664  0.2215  1.0172  0.4573\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: MA(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.4971  0.2466  0.2215  0.9320  0.4548\n",
      "mixture         120  0.5427  0.2704  0.2458  1.0042  0.4929\n",
      "normal          120  0.5585  0.2774  0.2484  1.0391  0.5165\n",
      "t-student       120  0.5290  0.2619  0.2389  0.9845  0.4934\n",
      "uniform         120  0.5708  0.2812  0.2571  1.0419  0.5220\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.2478  0.0143  0.2215  0.2911  0.2480\n",
      "0.5               150  0.3945  0.0226  0.3505  0.4742  0.3952\n",
      "1.0               150  0.5578  0.0436  0.4943  0.9320  0.5565\n",
      "3.0               150  0.9584  0.0506  0.8565  1.0419  0.9672\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.9775\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Block Bootstrapping\n",
      "    1          MA(2)  exponential             0.2             0.221491\n",
      "    1          AR(2)  exponential             0.2             0.221679\n",
      "    5      ARMA(2,2)  exponential             0.2             0.222744\n",
      "    4      ARMA(1,1)  exponential             0.2             0.223081\n",
      "    4          MA(1)  exponential             0.2             0.223256\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Block Bootstrapping\n",
      "    4          AR(2)      uniform             3.0             1.034027\n",
      "    2      ARMA(1,1)      uniform             3.0             1.035206\n",
      "    2      ARMA(2,2)      uniform             3.0             1.036542\n",
      "    2      ARMA(1,1)       normal             3.0             1.039135\n",
      "    2          AR(2)      uniform             3.0             1.041912\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2215\n",
      "   Peor rendimiento: 1.0419\n",
      "   Rendimiento promedio: 0.5396\n",
      "   Desviaci√≥n est√°ndar: 0.2681\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=0.2659)\n",
      "   Paso menos estable: 2 (std=0.2740)\n",
      "   Tipo m√°s estable: MA(2) (std=0.2664)\n",
      "   Tipo menos estable: MA(1) (std=0.2721)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\Block Bootstrapping\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[4/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: DeepAR en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.5962  0.2946  0.2342  1.3207  0.5425\n",
      "2       120  0.6046  0.3102  0.2345  1.2671  0.5343\n",
      "3       120  0.5883  0.3045  0.2329  1.4672  0.5247\n",
      "4       120  0.5832  0.2915  0.2352  1.1570  0.5394\n",
      "5       120  0.5954  0.3214  0.2293  1.7948  0.5200\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 4\n",
      "‚úì Peor rendimiento en paso: 2\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.6403  0.3297  0.2329  1.3207  0.5477\n",
      "AR(2)             100  0.5806  0.2929  0.2369  1.1773  0.5084\n",
      "ARMA(1,1)         100  0.5895  0.3018  0.2399  1.3892  0.5323\n",
      "ARMA(2,2)         100  0.5815  0.2918  0.2293  1.2246  0.5217\n",
      "MA(1)             100  0.5986  0.3239  0.2353  1.7948  0.5396\n",
      "MA(2)             100  0.5707  0.2819  0.2342  1.1213  0.5261\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: AR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.5639  0.2926  0.2293  1.3207  0.5203\n",
      "mixture         120  0.5999  0.3093  0.2540  1.7948  0.5666\n",
      "normal          120  0.5971  0.3006  0.2520  1.2246  0.5421\n",
      "t-student       120  0.5803  0.3032  0.2383  1.4672  0.5368\n",
      "uniform         120  0.6266  0.3140  0.2595  1.3892  0.5877\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.2698  0.0280  0.2293  0.4111  0.2645\n",
      "0.5               150  0.4393  0.0608  0.3624  0.8167  0.4280\n",
      "1.0               150  0.6072  0.0763  0.5171  1.1670  0.5930\n",
      "3.0               150  1.0579  0.1173  0.8963  1.7948  1.0260\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.9545\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error   DeepAR\n",
      "    5      ARMA(2,2)  exponential             0.2 0.229294\n",
      "    3          AR(1)  exponential             0.2 0.232875\n",
      "    1          MA(2)  exponential             0.2 0.234201\n",
      "    3          MA(2)  exponential             0.2 0.234354\n",
      "    2      ARMA(2,2)  exponential             0.2 0.234465\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error   DeepAR\n",
      "    1          AR(1)  exponential             3.0 1.320671\n",
      "    3      ARMA(1,1)      uniform             3.0 1.389231\n",
      "    5          MA(1)    t-student             3.0 1.417213\n",
      "    3          MA(1)    t-student             3.0 1.467214\n",
      "    5          MA(1)      mixture             3.0 1.794838\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2293\n",
      "   Peor rendimiento: 1.7948\n",
      "   Rendimiento promedio: 0.5935\n",
      "   Desviaci√≥n est√°ndar: 0.3037\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 4 (std=0.2915)\n",
      "   Paso menos estable: 5 (std=0.3214)\n",
      "   Tipo m√°s estable: MA(2) (std=0.2819)\n",
      "   Tipo menos estable: AR(1) (std=0.3297)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\DeepAR\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[5/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: EnCQR-LSTM en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.6999  0.4885  0.2348  3.2026  0.5857\n",
      "2       120  0.6868  0.4206  0.2325  2.6865  0.5761\n",
      "3       120  0.6608  0.3804  0.2261  2.3153  0.5767\n",
      "4       120  0.6727  0.3831  0.2350  2.3266  0.5816\n",
      "5       120  0.6492  0.3563  0.2430  2.0073  0.5407\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.8748  0.6049  0.2529  3.2026  0.6909\n",
      "AR(2)             100  0.6087  0.3179  0.2302  1.5927  0.5125\n",
      "ARMA(1,1)         100  0.6615  0.3389  0.2392  1.6104  0.6123\n",
      "ARMA(2,2)         100  0.6642  0.3600  0.2381  1.8465  0.5904\n",
      "MA(1)             100  0.6533  0.3835  0.2325  2.3153  0.5575\n",
      "MA(2)             100  0.5809  0.3025  0.2261  1.6200  0.5139\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: AR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.5997  0.3139  0.2261  1.6104  0.5238\n",
      "mixture         120  0.6964  0.3948  0.2482  1.9049  0.5958\n",
      "normal          120  0.7041  0.4595  0.2534  2.8402  0.5781\n",
      "t-student       120  0.6705  0.4532  0.2443  3.2026  0.5638\n",
      "uniform         120  0.6987  0.3965  0.2626  2.0073  0.6031\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: normal\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.2972  0.0649  0.2261  0.6394  0.2727\n",
      "0.5               150  0.5098  0.1957  0.3585  1.7858  0.4386\n",
      "1.0               150  0.6904  0.2427  0.5057  2.8402  0.6203\n",
      "3.0               150  1.1982  0.3450  0.8699  3.2026  1.0788\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.8055\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  EnCQR-LSTM\n",
      "    3          MA(2)  exponential             0.2    0.226102\n",
      "    3          AR(2)  exponential             0.2    0.230150\n",
      "    2          MA(1)  exponential             0.2    0.232454\n",
      "    1          MA(1)  exponential             0.2    0.234778\n",
      "    4          MA(1)  exponential             0.2    0.235046\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  EnCQR-LSTM\n",
      "    3          MA(1)    t-student             3.0    2.315321\n",
      "    4          AR(1)       normal             3.0    2.326557\n",
      "    2          AR(1)    t-student             3.0    2.686481\n",
      "    1          AR(1)       normal             1.0    2.840212\n",
      "    1          AR(1)    t-student             3.0    3.202591\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2261\n",
      "   Peor rendimiento: 3.2026\n",
      "   Rendimiento promedio: 0.6739\n",
      "   Desviaci√≥n est√°ndar: 0.4074\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3563)\n",
      "   Paso menos estable: 1 (std=0.4885)\n",
      "   Tipo m√°s estable: MA(2) (std=0.3025)\n",
      "   Tipo menos estable: AR(1) (std=0.6049)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\EnCQR-LSTM\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[6/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: LSPM en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.7489  0.4539  0.2273  2.5181  0.5900\n",
      "2       120  0.7253  0.4446  0.2247  2.4129  0.5535\n",
      "3       120  0.7356  0.4841  0.2315  3.4122  0.5899\n",
      "4       120  0.7467  0.4488  0.2233  2.7657  0.6440\n",
      "5       120  0.6969  0.4237  0.2252  2.6441  0.5631\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.7461  0.4451  0.2491  2.4178  0.5894\n",
      "AR(2)             100  0.7068  0.4473  0.2347  2.3155  0.5576\n",
      "ARMA(1,1)         100  0.7573  0.4949  0.2272  2.5181  0.5882\n",
      "ARMA(2,2)         100  0.7748  0.4056  0.2233  2.0305  0.6643\n",
      "MA(1)             100  0.7860  0.5420  0.2237  3.4122  0.6353\n",
      "MA(2)             100  0.6131  0.3242  0.2323  1.7902  0.5137\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: MA(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.6816  0.4499  0.2233  2.4532  0.5223\n",
      "mixture         120  0.7144  0.4137  0.2471  2.7657  0.5735\n",
      "normal          120  0.7354  0.4421  0.2528  2.5181  0.5782\n",
      "t-student       120  0.7304  0.4566  0.2377  3.4122  0.5841\n",
      "uniform         120  0.7917  0.4867  0.2608  2.6441  0.6222\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.3454  0.1265  0.2233  0.9171  0.3005\n",
      "0.5               150  0.5291  0.2001  0.3554  1.8090  0.4635\n",
      "1.0               150  0.7691  0.2759  0.4996  2.4129  0.6853\n",
      "3.0               150  1.2792  0.4341  0.8598  3.4122  1.0999\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7664\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     LSPM\n",
      "    4      ARMA(2,2)  exponential             0.2 0.223260\n",
      "    4          MA(1)  exponential             0.2 0.223718\n",
      "    2          MA(1)  exponential             0.2 0.224717\n",
      "    5      ARMA(2,2)  exponential             0.2 0.225249\n",
      "    2      ARMA(1,1)  exponential             0.2 0.227204\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     LSPM\n",
      "    4      ARMA(1,1)  exponential             3.0 2.453167\n",
      "    1      ARMA(1,1)       normal             3.0 2.518095\n",
      "    5          MA(1)      uniform             3.0 2.644088\n",
      "    4          MA(1)      mixture             3.0 2.765661\n",
      "    3          MA(1)    t-student             3.0 3.412221\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2233\n",
      "   Peor rendimiento: 3.4122\n",
      "   Rendimiento promedio: 0.7307\n",
      "   Desviaci√≥n est√°ndar: 0.4503\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.4237)\n",
      "   Paso menos estable: 3 (std=0.4841)\n",
      "   Tipo m√°s estable: MA(2) (std=0.3242)\n",
      "   Tipo menos estable: MA(1) (std=0.5420)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\LSPM\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[7/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: LSPMW en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.7348  0.4448  0.2277  2.5206  0.6023\n",
      "2       120  0.7840  0.5034  0.2240  3.1395  0.5837\n",
      "3       120  0.7780  0.5178  0.2309  3.5492  0.6361\n",
      "4       120  0.7925  0.4829  0.2235  2.7660  0.7070\n",
      "5       120  0.7286  0.4499  0.2247  2.6385  0.5939\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.7910  0.5007  0.2494  2.6738  0.6158\n",
      "AR(2)             100  0.7386  0.4663  0.2341  2.4281  0.5724\n",
      "ARMA(1,1)         100  0.7984  0.5338  0.2323  2.7660  0.6342\n",
      "ARMA(2,2)         100  0.8006  0.4182  0.2235  2.1657  0.7136\n",
      "MA(1)             100  0.8191  0.5747  0.2237  3.5492  0.7050\n",
      "MA(2)             100  0.6337  0.3320  0.2418  1.8172  0.5377\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: MA(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.7131  0.4948  0.2235  3.1395  0.5441\n",
      "mixture         120  0.7484  0.4503  0.2494  2.7294  0.6391\n",
      "normal          120  0.7752  0.4686  0.2541  2.5206  0.6022\n",
      "t-student       120  0.7571  0.4850  0.2453  3.5492  0.6003\n",
      "uniform         120  0.8241  0.4995  0.2655  2.6738  0.6754\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.3676  0.1589  0.2235  1.1847  0.3079\n",
      "0.5               150  0.5507  0.2203  0.3558  1.7113  0.4763\n",
      "1.0               150  0.8178  0.3359  0.5110  3.1395  0.7160\n",
      "3.0               150  1.3181  0.4719  0.8591  3.5492  1.1032\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7320\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    LSPMW\n",
      "    4      ARMA(2,2)  exponential             0.2 0.223452\n",
      "    4          MA(1)  exponential             0.2 0.223746\n",
      "    2          MA(1)  exponential             0.2 0.224025\n",
      "    5      ARMA(2,2)  exponential             0.2 0.224737\n",
      "    1      ARMA(2,2)  exponential             0.2 0.227696\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    LSPMW\n",
      "    3          AR(1)      uniform             3.0 2.673848\n",
      "    4          MA(1)      mixture             3.0 2.729437\n",
      "    4      ARMA(1,1)  exponential             3.0 2.766004\n",
      "    2          MA(1)  exponential             1.0 3.139542\n",
      "    3          MA(1)    t-student             3.0 3.549239\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2235\n",
      "   Peor rendimiento: 3.5492\n",
      "   Rendimiento promedio: 0.7636\n",
      "   Desviaci√≥n est√°ndar: 0.4798\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=0.4448)\n",
      "   Paso menos estable: 3 (std=0.5178)\n",
      "   Tipo m√°s estable: MA(2) (std=0.3320)\n",
      "   Tipo menos estable: MA(1) (std=0.5747)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\LSPMW\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[8/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: MCPS en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.5586  0.2794  0.2300  1.1299  0.4704\n",
      "2       120  0.6307  0.3402  0.2498  1.8043  0.5713\n",
      "3       120  0.6459  0.3305  0.2416  1.6998  0.5703\n",
      "4       120  0.6678  0.4136  0.2397  2.8281  0.5690\n",
      "5       120  0.6298  0.3583  0.2293  1.8880  0.5345\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.6713  0.4060  0.2358  2.8281  0.5874\n",
      "AR(2)             100  0.6054  0.3225  0.2314  1.6998  0.5359\n",
      "ARMA(1,1)         100  0.6536  0.3870  0.2343  2.1433  0.5626\n",
      "ARMA(2,2)         100  0.6281  0.3486  0.2293  1.5775  0.5580\n",
      "MA(1)             100  0.6089  0.3130  0.2357  1.4938  0.5438\n",
      "MA(2)             100  0.5921  0.2989  0.2416  1.4388  0.5237\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: AR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.5703  0.2861  0.2293  1.8880  0.5278\n",
      "mixture         120  0.6397  0.3888  0.2467  2.8281  0.5654\n",
      "normal          120  0.6585  0.3795  0.2559  2.1433  0.5698\n",
      "t-student       120  0.6127  0.3140  0.2412  1.5290  0.5437\n",
      "uniform         120  0.6514  0.3583  0.2592  1.8935  0.5835\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: normal\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.2875  0.0660  0.2293  0.8389  0.2707\n",
      "0.5               150  0.4665  0.1151  0.3588  1.1431  0.4260\n",
      "1.0               150  0.6330  0.1182  0.5072  1.1139  0.5937\n",
      "3.0               150  1.1193  0.2625  0.8807  2.8281  1.0358\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.8807\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     MCPS\n",
      "    5      ARMA(2,2)  exponential             0.2 0.229263\n",
      "    1      ARMA(2,2)  exponential             0.2 0.229965\n",
      "    1          AR(2)  exponential             0.2 0.231397\n",
      "    1      ARMA(1,1)  exponential             0.2 0.234289\n",
      "    5          AR(2)  exponential             0.2 0.235624\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     MCPS\n",
      "    2      ARMA(1,1)      uniform             3.0 1.804323\n",
      "    5      ARMA(1,1)  exponential             3.0 1.888016\n",
      "    4          AR(1)      uniform             3.0 1.893470\n",
      "    4      ARMA(1,1)       normal             3.0 2.143291\n",
      "    4          AR(1)      mixture             3.0 2.828056\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2293\n",
      "   Peor rendimiento: 2.8281\n",
      "   Rendimiento promedio: 0.6265\n",
      "   Desviaci√≥n est√°ndar: 0.3479\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=0.2794)\n",
      "   Paso menos estable: 4 (std=0.4136)\n",
      "   Tipo m√°s estable: MA(2) (std=0.2989)\n",
      "   Tipo menos estable: AR(1) (std=0.4060)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\MCPS\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[9/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: Sieve Bootstrap en escenario Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       120  0.5341  0.2665  0.2210  1.0080  0.4546\n",
      "2       120  0.5459  0.2743  0.2243  1.0540  0.4625\n",
      "3       120  0.5439  0.2744  0.2249  1.1914  0.4628\n",
      "4       120  0.5457  0.2819  0.2230  1.4898  0.4812\n",
      "5       120  0.5398  0.2696  0.2226  1.0742  0.4680\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 2\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "AR(1)             100  0.5439  0.2770  0.2260  1.1914  0.4665\n",
      "AR(2)             100  0.5408  0.2719  0.2210  1.0452  0.4851\n",
      "ARMA(1,1)         100  0.5382  0.2683  0.2237  1.0364  0.4670\n",
      "ARMA(2,2)         100  0.5434  0.2821  0.2226  1.4898  0.4622\n",
      "MA(1)             100  0.5485  0.2751  0.2230  1.0581  0.4602\n",
      "MA(2)             100  0.5365  0.2672  0.2220  1.0175  0.4572\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: MA(2)\n",
      "‚úì Peor rendimiento en tipo: MA(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     120  0.4968  0.2464  0.2210  0.9539  0.4533\n",
      "mixture         120  0.5453  0.2725  0.2461  1.0581  0.4945\n",
      "normal          120  0.5641  0.2877  0.2487  1.4898  0.5116\n",
      "t-student       120  0.5329  0.2681  0.2364  1.1914  0.4681\n",
      "uniform         120  0.5705  0.2842  0.2572  1.1480  0.5023\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               150  0.2480  0.0126  0.2210  0.2786  0.2496\n",
      "0.5               150  0.3932  0.0211  0.3511  0.4652  0.3954\n",
      "1.0               150  0.5602  0.0445  0.4955  0.9259  0.5587\n",
      "3.0               150  0.9662  0.0720  0.8555  1.4898  0.9699\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.9739\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Sieve Bootstrap\n",
      "    1          AR(2)  exponential             0.2         0.220954\n",
      "    1          MA(2)  exponential             0.2         0.222022\n",
      "    5      ARMA(2,2)  exponential             0.2         0.222596\n",
      "    4          MA(1)  exponential             0.2         0.222958\n",
      "    4      ARMA(1,1)  exponential             0.2         0.223715\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Sieve Bootstrap\n",
      "    4          MA(1)      mixture             3.0         1.058056\n",
      "    5      ARMA(2,2)       normal             3.0         1.074196\n",
      "    4          AR(1)      uniform             3.0         1.147970\n",
      "    3          AR(1)    t-student             3.0         1.191408\n",
      "    4      ARMA(2,2)       normal             3.0         1.489834\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2210\n",
      "   Peor rendimiento: 1.4898\n",
      "   Rendimiento promedio: 0.5419\n",
      "   Desviaci√≥n est√°ndar: 0.2725\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=0.2665)\n",
      "   Paso menos estable: 4 (std=0.2819)\n",
      "   Tipo m√°s estable: MA(2) (std=0.2672)\n",
      "   Tipo menos estable: ARMA(2,2) (std=0.2821)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\Estacionario_Lineal\\Sieve Bootstrap\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[10/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: AREPD en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6257  0.4151  0.2358  3.1905  0.5512\n",
      "2       140  0.6410  0.4466  0.2328  4.3719  0.5539\n",
      "3       140  0.6534  0.3755  0.2403  1.8113  0.5666\n",
      "4       140  0.6639  0.3924  0.2307  2.2752  0.5482\n",
      "5       140  0.6403  0.3422  0.2371  1.8090  0.5585\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.7006  0.4696  0.2481  3.1905  0.5688\n",
      "EXPAR(2,1)        100  0.6009  0.3292  0.2388  1.7813  0.5333\n",
      "SETAR(2,1)        100  0.6142  0.3111  0.2429  1.3838  0.5445\n",
      "SETAR(2,2)        100  0.6081  0.3029  0.2307  1.2976  0.5537\n",
      "SETAR(2,3)        100  0.6155  0.3340  0.2404  1.8090  0.5584\n",
      "TAR(2,1)          100  0.8014  0.5919  0.2516  4.3719  0.6116\n",
      "TAR(2,2)          100  0.5733  0.2901  0.2328  1.2295  0.4882\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.6484  0.4924  0.2307  4.3719  0.5254\n",
      "mixture         140  0.6345  0.3439  0.2516  1.7277  0.5699\n",
      "normal          140  0.6240  0.3294  0.2570  1.8289  0.5704\n",
      "t-student       140  0.6214  0.3455  0.2399  1.8306  0.5523\n",
      "uniform         140  0.6960  0.4385  0.2612  3.1905  0.6104\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2987  0.0797  0.2307  0.8863  0.2746\n",
      "0.5               175  0.4748  0.1287  0.3645  1.4932  0.4359\n",
      "1.0               175  0.6273  0.0998  0.5081  1.1271  0.5995\n",
      "3.0               175  1.1786  0.3970  0.8969  4.3719  1.0522\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.8292\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    AREPD\n",
      "    4     SETAR(2,2)  exponential             0.2 0.230658\n",
      "    2       TAR(2,2)  exponential             0.2 0.232838\n",
      "    1     SETAR(2,2)  exponential             0.2 0.235772\n",
      "    2     SETAR(2,2)  exponential             0.2 0.236450\n",
      "    5     SETAR(2,2)  exponential             0.2 0.237134\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    AREPD\n",
      "    4    BILINEAR(1)    t-student             3.0 1.830605\n",
      "    4       TAR(2,1)      uniform             3.0 2.275167\n",
      "    1       TAR(2,1)  exponential             3.0 3.019409\n",
      "    1    BILINEAR(1)      uniform             3.0 3.190504\n",
      "    2       TAR(2,1)  exponential             3.0 4.371926\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2307\n",
      "   Peor rendimiento: 4.3719\n",
      "   Rendimiento promedio: 0.6449\n",
      "   Desviaci√≥n est√°ndar: 0.3950\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3422)\n",
      "   Paso menos estable: 2 (std=0.4466)\n",
      "   Tipo m√°s estable: TAR(2,2) (std=0.2901)\n",
      "   Tipo menos estable: TAR(2,1) (std=0.5919)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\AREPD\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[11/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: AV-MCPS en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6097  0.6347  0.2268  7.1390  0.5086\n",
      "2       140  0.6355  0.3437  0.2284  1.9857  0.5526\n",
      "3       140  0.6372  0.3595  0.2267  2.4468  0.5630\n",
      "4       140  0.6371  0.3819  0.2257  2.8317  0.5495\n",
      "5       140  0.6180  0.3290  0.2267  1.9193  0.5664\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 3\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.7089  0.7709  0.2423  7.1390  0.5332\n",
      "EXPAR(2,1)        100  0.5937  0.3134  0.2257  1.7848  0.5148\n",
      "SETAR(2,1)        100  0.6011  0.2924  0.2267  1.2931  0.5411\n",
      "SETAR(2,2)        100  0.5763  0.2819  0.2294  1.1574  0.5161\n",
      "SETAR(2,3)        100  0.6152  0.3915  0.2270  2.8317  0.5414\n",
      "TAR(2,1)          100  0.7088  0.3860  0.2318  2.2352  0.5979\n",
      "TAR(2,2)          100  0.5887  0.3021  0.2267  1.3151  0.5331\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: SETAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: BILINEAR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.5959  0.3470  0.2257  2.2352  0.5221\n",
      "mixture         140  0.6144  0.3271  0.2471  1.6975  0.5581\n",
      "normal          140  0.6329  0.3372  0.2535  1.7848  0.5712\n",
      "t-student       140  0.6071  0.3392  0.2419  2.8317  0.5422\n",
      "uniform         140  0.6873  0.6659  0.2602  7.1390  0.5829\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2870  0.0789  0.2257  0.8807  0.2636\n",
      "0.5               175  0.4556  0.1068  0.3558  1.1331  0.4215\n",
      "1.0               175  0.6235  0.1053  0.4979  1.4435  0.5909\n",
      "3.0               175  1.1440  0.5287  0.8621  7.1390  1.0224\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7516\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  AV-MCPS\n",
      "    4     EXPAR(2,1)  exponential             0.2 0.225678\n",
      "    5       TAR(2,2)  exponential             0.2 0.226689\n",
      "    3     SETAR(2,1)  exponential             0.2 0.226726\n",
      "    1     SETAR(2,1)  exponential             0.2 0.226826\n",
      "    1     SETAR(2,3)  exponential             0.2 0.226996\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  AV-MCPS\n",
      "    2       TAR(2,1)  exponential             3.0 1.985666\n",
      "    1       TAR(2,1)  exponential             3.0 2.235202\n",
      "    3    BILINEAR(1)      uniform             3.0 2.446849\n",
      "    4     SETAR(2,3)    t-student             3.0 2.831683\n",
      "    1    BILINEAR(1)      uniform             3.0 7.138988\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2257\n",
      "   Peor rendimiento: 7.1390\n",
      "   Rendimiento promedio: 0.6275\n",
      "   Desviaci√≥n est√°ndar: 0.4242\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3290)\n",
      "   Paso menos estable: 1 (std=0.6347)\n",
      "   Tipo m√°s estable: SETAR(2,2) (std=0.2819)\n",
      "   Tipo menos estable: BILINEAR(1) (std=0.7709)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\AV-MCPS\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[12/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: Block Bootstrapping en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6169  0.6049  0.2224  6.2332  0.5284\n",
      "2       140  0.6111  0.4955  0.2236  5.1368  0.5285\n",
      "3       140  0.5690  0.3181  0.2240  1.9019  0.5005\n",
      "4       140  0.5902  0.3309  0.2245  2.1446  0.5226\n",
      "5       140  0.5695  0.2833  0.2241  1.2419  0.5014\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 3\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.6646  0.7824  0.2224  6.2332  0.4763\n",
      "EXPAR(2,1)        100  0.5623  0.2992  0.2271  1.5304  0.4910\n",
      "SETAR(2,1)        100  0.5633  0.2748  0.2244  1.1813  0.5141\n",
      "SETAR(2,2)        100  0.5548  0.2749  0.2282  1.1751  0.4943\n",
      "SETAR(2,3)        100  0.5496  0.2809  0.2236  1.1243  0.4676\n",
      "TAR(2,1)          100  0.7007  0.4869  0.2332  3.5078  0.5709\n",
      "TAR(2,2)          100  0.5441  0.2768  0.2240  1.2420  0.4663\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.5438  0.3627  0.2224  3.5078  0.4951\n",
      "mixture         140  0.5741  0.2958  0.2467  1.4108  0.5506\n",
      "normal          140  0.5954  0.3193  0.2514  1.9019  0.5615\n",
      "t-student       140  0.5711  0.3120  0.2374  1.9238  0.5348\n",
      "uniform         140  0.6722  0.6903  0.2587  6.2332  0.5784\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2658  0.0539  0.2224  0.7106  0.2557\n",
      "0.5               175  0.4185  0.0673  0.3513  0.8905  0.4032\n",
      "1.0               175  0.5802  0.0640  0.4944  0.9875  0.5713\n",
      "3.0               175  1.1009  0.5594  0.8573  6.2332  1.0036\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7382\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Block Bootstrapping\n",
      "    1    BILINEAR(1)  exponential             0.2             0.222436\n",
      "    2     SETAR(2,3)  exponential             0.2             0.223553\n",
      "    1     SETAR(2,3)  exponential             0.2             0.223837\n",
      "    3       TAR(2,2)  exponential             0.2             0.224042\n",
      "    5       TAR(2,2)  exponential             0.2             0.224054\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Block Bootstrapping\n",
      "    2       TAR(2,1)    t-student             3.0             1.923775\n",
      "    4       TAR(2,1)      uniform             3.0             2.144649\n",
      "    1       TAR(2,1)  exponential             3.0             3.507841\n",
      "    2    BILINEAR(1)      uniform             3.0             5.136773\n",
      "    1    BILINEAR(1)      uniform             3.0             6.233198\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2224\n",
      "   Peor rendimiento: 6.2332\n",
      "   Rendimiento promedio: 0.5913\n",
      "   Desviaci√≥n est√°ndar: 0.4241\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.2833)\n",
      "   Paso menos estable: 1 (std=0.6049)\n",
      "   Tipo m√°s estable: SETAR(2,1) (std=0.2748)\n",
      "   Tipo menos estable: BILINEAR(1) (std=0.7824)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\Block Bootstrapping\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[13/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: DeepAR en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6259  0.4943  0.2361  4.7574  0.5381\n",
      "2       140  0.6149  0.4044  0.2287  2.8573  0.5403\n",
      "3       140  0.5979  0.3349  0.2283  1.7370  0.5185\n",
      "4       140  0.5966  0.3317  0.2347  2.0081  0.5275\n",
      "5       140  0.5797  0.2879  0.2374  1.1896  0.5276\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.6526  0.5590  0.2338  4.7574  0.5513\n",
      "EXPAR(2,1)        100  0.5930  0.3272  0.2383  1.7814  0.4782\n",
      "SETAR(2,1)        100  0.5859  0.3002  0.2347  1.6072  0.5374\n",
      "SETAR(2,2)        100  0.5769  0.2943  0.2332  1.4418  0.5068\n",
      "SETAR(2,3)        100  0.5647  0.2789  0.2287  1.1230  0.5196\n",
      "TAR(2,1)          100  0.6848  0.4819  0.2434  2.8573  0.5687\n",
      "TAR(2,2)          100  0.5629  0.2884  0.2283  1.4762  0.4925\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.5829  0.4106  0.2283  2.8573  0.4851\n",
      "mixture         140  0.6009  0.3157  0.2530  1.7814  0.5689\n",
      "normal          140  0.6043  0.3121  0.2524  1.6072  0.5647\n",
      "t-student       140  0.5759  0.3152  0.2383  2.1816  0.5330\n",
      "uniform         140  0.6509  0.4957  0.2614  4.7574  0.5815\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2735  0.0531  0.2283  0.6257  0.2650\n",
      "0.5               175  0.4238  0.0443  0.3623  0.6455  0.4131\n",
      "1.0               175  0.5991  0.0685  0.5161  0.9520  0.5828\n",
      "3.0               175  1.1155  0.3941  0.8958  4.7574  1.0204\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.8381\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error   DeepAR\n",
      "    3       TAR(2,2)  exponential             0.2 0.228271\n",
      "    2     SETAR(2,3)  exponential             0.2 0.228717\n",
      "    3     SETAR(2,3)  exponential             0.2 0.232227\n",
      "    2     SETAR(2,2)  exponential             0.2 0.233189\n",
      "    3    BILINEAR(1)  exponential             0.2 0.233790\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error   DeepAR\n",
      "    2       TAR(2,1)    t-student             3.0 2.181567\n",
      "    2    BILINEAR(1)      uniform             3.0 2.430270\n",
      "    1       TAR(2,1)  exponential             3.0 2.773743\n",
      "    2       TAR(2,1)  exponential             3.0 2.857299\n",
      "    1    BILINEAR(1)      uniform             3.0 4.757393\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2283\n",
      "   Peor rendimiento: 4.7574\n",
      "   Rendimiento promedio: 0.6030\n",
      "   Desviaci√≥n est√°ndar: 0.3769\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.2879)\n",
      "   Paso menos estable: 1 (std=0.4943)\n",
      "   Tipo m√°s estable: SETAR(2,3) (std=0.2789)\n",
      "   Tipo menos estable: BILINEAR(1) (std=0.5590)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\DeepAR\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[14/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: EnCQR-LSTM en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6517  0.5692  0.2269  5.0975  0.5454\n",
      "2       140  0.6200  0.4164  0.2297  3.7699  0.5320\n",
      "3       140  0.6161  0.3749  0.2230  2.2517  0.5398\n",
      "4       140  0.6130  0.3602  0.2242  2.1942  0.5290\n",
      "5       140  0.5905  0.3034  0.2239  1.5296  0.5246\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.6977  0.5786  0.2402  5.0975  0.5445\n",
      "EXPAR(2,1)        100  0.5771  0.3174  0.2281  1.6379  0.5074\n",
      "SETAR(2,1)        100  0.5842  0.2951  0.2269  1.4397  0.5296\n",
      "SETAR(2,2)        100  0.5669  0.2867  0.2287  1.2543  0.5069\n",
      "SETAR(2,3)        100  0.5546  0.2823  0.2269  1.2612  0.4704\n",
      "TAR(2,1)          100  0.8026  0.6285  0.2503  4.2189  0.6037\n",
      "TAR(2,2)          100  0.5447  0.2727  0.2230  1.0544  0.4574\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.6065  0.5194  0.2230  4.2189  0.5028\n",
      "mixture         140  0.5958  0.3038  0.2460  1.4125  0.5535\n",
      "normal          140  0.6101  0.3351  0.2529  2.2517  0.5669\n",
      "t-student       140  0.6040  0.3261  0.2395  1.8046  0.5406\n",
      "uniform         140  0.6750  0.5260  0.2578  5.0975  0.5836\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: mixture\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2892  0.1008  0.2230  0.9785  0.2611\n",
      "0.5               175  0.4452  0.1173  0.3525  1.1459  0.4106\n",
      "1.0               175  0.5950  0.0833  0.4962  1.2447  0.5808\n",
      "3.0               175  1.1436  0.4902  0.8599  5.0975  1.0122\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7753\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  EnCQR-LSTM\n",
      "    3       TAR(2,2)  exponential             0.2    0.222981\n",
      "    5       TAR(2,2)  exponential             0.2    0.223906\n",
      "    4       TAR(2,2)  exponential             0.2    0.224188\n",
      "    1     SETAR(2,3)  exponential             0.2    0.226874\n",
      "    5     SETAR(2,1)  exponential             0.2    0.226940\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  EnCQR-LSTM\n",
      "    4    BILINEAR(1)      uniform             3.0    2.194173\n",
      "    3       TAR(2,1)       normal             3.0    2.251675\n",
      "    2       TAR(2,1)  exponential             3.0    3.769857\n",
      "    1       TAR(2,1)  exponential             3.0    4.218916\n",
      "    1    BILINEAR(1)      uniform             3.0    5.097550\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2230\n",
      "   Peor rendimiento: 5.0975\n",
      "   Rendimiento promedio: 0.6183\n",
      "   Desviaci√≥n est√°ndar: 0.4139\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3034)\n",
      "   Paso menos estable: 1 (std=0.5692)\n",
      "   Tipo m√°s estable: TAR(2,2) (std=0.2727)\n",
      "   Tipo menos estable: TAR(2,1) (std=0.6285)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\EnCQR-LSTM\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[15/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: LSPM en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  0.7231  0.9764  0.2248  10.6688  0.5560\n",
      "2       140  0.6507  0.5003  0.2247   4.2855  0.5450\n",
      "3       140  0.6937  0.6823  0.2241   5.7644  0.5622\n",
      "4       140  0.6611  0.5221  0.2242   4.8967  0.5511\n",
      "5       140  0.6245  0.3618  0.2243   2.3617  0.5396\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Tipo de Modelo                                                \n",
      "BILINEAR(1)       100  0.8869  1.2580  0.2435  10.6688  0.5655\n",
      "EXPAR(2,1)        100  0.6092  0.3501  0.2273   1.9314  0.5374\n",
      "SETAR(2,1)        100  0.5928  0.3008  0.2267   1.3035  0.5313\n",
      "SETAR(2,2)        100  0.5900  0.3134  0.2270   2.0476  0.5359\n",
      "SETAR(2,3)        100  0.5584  0.2825  0.2242   1.1245  0.4869\n",
      "TAR(2,1)          100  0.9085  0.8508  0.2559   5.0627  0.6347\n",
      "TAR(2,2)          100  0.5487  0.2691  0.2241   1.0958  0.4713\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  0.6570  0.6885  0.2241   5.0627  0.5119\n",
      "mixture         140  0.6170  0.3343  0.2467   2.0902  0.5544\n",
      "normal          140  0.6537  0.4019  0.2528   2.8264  0.5693\n",
      "t-student       140  0.6169  0.3388  0.2394   2.1627  0.5449\n",
      "uniform         140  0.8085  1.0914  0.2584  10.6688  0.5831\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  0.3065  0.1270  0.2241   1.0489  0.2631\n",
      "0.5               175  0.4706  0.1933  0.3539   2.3316  0.4215\n",
      "1.0               175  0.6195  0.1123  0.4977   1.2433  0.5881\n",
      "3.0               175  1.2859  1.0186  0.8641  10.6688  1.0182\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.5781\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     LSPM\n",
      "    3       TAR(2,2)  exponential             0.2 0.224086\n",
      "    4     SETAR(2,3)  exponential             0.2 0.224196\n",
      "    5       TAR(2,2)  exponential             0.2 0.224251\n",
      "    4       TAR(2,2)  exponential             0.2 0.224490\n",
      "    2     SETAR(2,3)  exponential             0.2 0.224684\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error      LSPM\n",
      "    4    BILINEAR(1)      uniform             3.0  4.896705\n",
      "    1       TAR(2,1)  exponential             3.0  4.988903\n",
      "    3       TAR(2,1)  exponential             3.0  5.062722\n",
      "    3    BILINEAR(1)      uniform             3.0  5.764354\n",
      "    1    BILINEAR(1)      uniform             3.0 10.668794\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2241\n",
      "   Peor rendimiento: 10.6688\n",
      "   Rendimiento promedio: 0.6706\n",
      "   Desviaci√≥n est√°ndar: 0.6429\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3618)\n",
      "   Paso menos estable: 1 (std=0.9764)\n",
      "   Tipo m√°s estable: TAR(2,2) (std=0.2691)\n",
      "   Tipo menos estable: BILINEAR(1) (std=1.2580)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\LSPM\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[16/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: LSPMW en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  0.7116  0.9652  0.2252  10.7483  0.5616\n",
      "2       140  0.6813  0.5606  0.2298   4.9874  0.5409\n",
      "3       140  0.7159  0.6337  0.2301   5.8864  0.5732\n",
      "4       140  0.7017  0.5321  0.2266   4.9474  0.5654\n",
      "5       140  0.6573  0.4085  0.2261   2.9360  0.5661\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 3\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Tipo de Modelo                                                \n",
      "BILINEAR(1)       100  0.9332  1.2791  0.2483  10.7483  0.5911\n",
      "EXPAR(2,1)        100  0.6329  0.3978  0.2282   2.3239  0.5455\n",
      "SETAR(2,1)        100  0.6314  0.3396  0.2252   1.6238  0.5360\n",
      "SETAR(2,2)        100  0.6014  0.3283  0.2266   2.3611  0.5561\n",
      "SETAR(2,3)        100  0.5711  0.2873  0.2399   1.1544  0.4786\n",
      "TAR(2,1)          100  0.9141  0.7857  0.2557   4.9874  0.6538\n",
      "TAR(2,2)          100  0.5707  0.2777  0.2288   1.2667  0.5145\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: BILINEAR(1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  0.6765  0.6479  0.2252   4.9874  0.5162\n",
      "mixture         140  0.6408  0.3588  0.2463   2.3879  0.5633\n",
      "normal          140  0.6823  0.4257  0.2525   2.7474  0.5800\n",
      "t-student       140  0.6431  0.3736  0.2382   2.3146  0.5571\n",
      "uniform         140  0.8252  1.0987  0.2593  10.7483  0.5908\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: mixture\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  0.3149  0.1360  0.2252   1.2446  0.2717\n",
      "0.5               175  0.4843  0.1653  0.3531   1.5945  0.4294\n",
      "1.0               175  0.6589  0.1887  0.5020   1.6238  0.5962\n",
      "3.0               175  1.3162  1.0082  0.8627  10.7483  1.0308\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.5862\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    LSPMW\n",
      "    1     SETAR(2,1)  exponential             0.2 0.225156\n",
      "    5     SETAR(2,1)  exponential             0.2 0.226088\n",
      "    4     SETAR(2,2)  exponential             0.2 0.226597\n",
      "    4     EXPAR(2,1)  exponential             0.2 0.228236\n",
      "    4       TAR(2,2)  exponential             0.2 0.228835\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     LSPMW\n",
      "    1       TAR(2,1)  exponential             3.0  4.443527\n",
      "    4    BILINEAR(1)      uniform             3.0  4.947444\n",
      "    2       TAR(2,1)  exponential             3.0  4.987437\n",
      "    3    BILINEAR(1)      uniform             3.0  5.886357\n",
      "    1    BILINEAR(1)      uniform             3.0 10.748265\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2252\n",
      "   Peor rendimiento: 10.7483\n",
      "   Rendimiento promedio: 0.6936\n",
      "   Desviaci√≥n est√°ndar: 0.6462\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.4085)\n",
      "   Paso menos estable: 1 (std=0.9652)\n",
      "   Tipo m√°s estable: TAR(2,2) (std=0.2777)\n",
      "   Tipo menos estable: BILINEAR(1) (std=1.2791)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\LSPMW\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[17/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: MCPS en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6186  0.7207  0.2278  8.3173  0.4916\n",
      "2       140  0.6275  0.3777  0.2290  2.8406  0.5259\n",
      "3       140  0.6416  0.3980  0.2336  3.1044  0.5548\n",
      "4       140  0.6074  0.3323  0.2275  1.7057  0.5216\n",
      "5       140  0.6068  0.3039  0.2378  1.4192  0.5401\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 3\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.7072  0.8706  0.2446  8.3173  0.5113\n",
      "EXPAR(2,1)        100  0.5833  0.2942  0.2275  1.2501  0.5134\n",
      "SETAR(2,1)        100  0.5953  0.2893  0.2320  1.4250  0.5498\n",
      "SETAR(2,2)        100  0.5819  0.2919  0.2278  1.4983  0.5111\n",
      "SETAR(2,3)        100  0.5849  0.3002  0.2371  1.3755  0.5322\n",
      "TAR(2,1)          100  0.7116  0.4668  0.2439  3.1044  0.5813\n",
      "TAR(2,2)          100  0.5785  0.3062  0.2316  1.2939  0.5188\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.5884  0.3918  0.2275  3.1044  0.5113\n",
      "mixture         140  0.5969  0.2935  0.2491  1.4348  0.5605\n",
      "normal          140  0.6303  0.3439  0.2554  2.0254  0.5703\n",
      "t-student       140  0.5923  0.2911  0.2407  1.3336  0.5424\n",
      "uniform         140  0.6939  0.7572  0.2604  8.3173  0.5478\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2899  0.0744  0.2275  0.9199  0.2662\n",
      "0.5               175  0.4353  0.0637  0.3568  0.7429  0.4173\n",
      "1.0               175  0.6079  0.0920  0.5072  1.1200  0.5866\n",
      "3.0               175  1.1484  0.6125  0.8828  8.3173  1.0249\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7175\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     MCPS\n",
      "    4     EXPAR(2,1)  exponential             0.2 0.227501\n",
      "    1     SETAR(2,2)  exponential             0.2 0.227775\n",
      "    2     EXPAR(2,1)  exponential             0.2 0.228999\n",
      "    1     EXPAR(2,1)  exponential             0.2 0.230992\n",
      "    1       TAR(2,2)  exponential             0.2 0.231556\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     MCPS\n",
      "    3       TAR(2,1)       normal             3.0 2.025409\n",
      "    1       TAR(2,1)  exponential             3.0 2.056311\n",
      "    2    BILINEAR(1)      uniform             3.0 2.840601\n",
      "    3       TAR(2,1)  exponential             3.0 3.104448\n",
      "    1    BILINEAR(1)      uniform             3.0 8.317317\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2275\n",
      "   Peor rendimiento: 8.3173\n",
      "   Rendimiento promedio: 0.6204\n",
      "   Desviaci√≥n est√°ndar: 0.4513\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.3039)\n",
      "   Paso menos estable: 1 (std=0.7207)\n",
      "   Tipo m√°s estable: SETAR(2,1) (std=0.2893)\n",
      "   Tipo menos estable: BILINEAR(1) (std=0.8706)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\MCPS\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[18/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: Sieve Bootstrap en escenario No_Lineal_Estacionario\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.6184  0.6072  0.2238  6.2003  0.5297\n",
      "2       140  0.6063  0.4167  0.2232  3.7698  0.5351\n",
      "3       140  0.5858  0.3346  0.2241  1.8763  0.4997\n",
      "4       140  0.5880  0.3195  0.2240  1.7340  0.5164\n",
      "5       140  0.5664  0.2830  0.2234  1.2275  0.4925\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "BILINEAR(1)       100  0.6569  0.7129  0.2247  6.2003  0.4782\n",
      "EXPAR(2,1)        100  0.5652  0.3031  0.2265  1.5895  0.4816\n",
      "SETAR(2,1)        100  0.5643  0.2800  0.2275  1.1976  0.5099\n",
      "SETAR(2,2)        100  0.5568  0.2765  0.2275  1.1097  0.4890\n",
      "SETAR(2,3)        100  0.5574  0.2910  0.2232  1.3681  0.4748\n",
      "TAR(2,1)          100  0.7015  0.4830  0.2340  3.6481  0.5770\n",
      "TAR(2,2)          100  0.5489  0.2855  0.2234  1.4981  0.4838\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: TAR(2,2)\n",
      "‚úì Peor rendimiento en tipo: TAR(2,1)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.5498  0.3746  0.2232  3.6481  0.4979\n",
      "mixture         140  0.5781  0.2966  0.2475  1.4635  0.5537\n",
      "normal          140  0.5950  0.3227  0.2511  1.8763  0.5612\n",
      "t-student       140  0.5797  0.3209  0.2370  2.0039  0.5374\n",
      "uniform         140  0.6623  0.6289  0.2588  6.2003  0.5788\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2670  0.0586  0.2232  0.8051  0.2560\n",
      "0.5               175  0.4209  0.0685  0.3520  0.8554  0.4077\n",
      "1.0               175  0.5826  0.0664  0.4950  0.9737  0.5698\n",
      "3.0               175  1.1014  0.5101  0.8588  6.2003  1.0096\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.7655\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Sieve Bootstrap\n",
      "    2     SETAR(2,3)  exponential             0.2         0.223210\n",
      "    5       TAR(2,2)  exponential             0.2         0.223444\n",
      "    1     SETAR(2,3)  exponential             0.2         0.223775\n",
      "    4       TAR(2,2)  exponential             0.2         0.223956\n",
      "    3       TAR(2,2)  exponential             0.2         0.224056\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Sieve Bootstrap\n",
      "    3       TAR(2,1)       normal             3.0         1.876312\n",
      "    2       TAR(2,1)    t-student             3.0         2.003942\n",
      "    1       TAR(2,1)  exponential             3.0         3.648126\n",
      "    2    BILINEAR(1)      uniform             3.0         3.769797\n",
      "    1    BILINEAR(1)      uniform             3.0         6.200287\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2232\n",
      "   Peor rendimiento: 6.2003\n",
      "   Rendimiento promedio: 0.5930\n",
      "   Desviaci√≥n est√°ndar: 0.4082\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 5 (std=0.2830)\n",
      "   Paso menos estable: 1 (std=0.6072)\n",
      "   Tipo m√°s estable: SETAR(2,2) (std=0.2765)\n",
      "   Tipo menos estable: BILINEAR(1) (std=0.7129)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Lineal_Estacionario\\Sieve Bootstrap\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[19/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: AREPD en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean      std     min       max  median\n",
      "Paso                                                  \n",
      "1       140  9.3704  16.4240  0.2497  120.6715  3.2342\n",
      "2       140  9.5107  16.3501  0.2527  118.2628  3.3887\n",
      "3       140  9.7029  16.8979  0.2479  122.8322  3.4951\n",
      "4       140  9.5789  16.9989  0.2671  121.2458  3.2810\n",
      "5       140  9.6719  17.1570  0.2622  121.8629  3.8840\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 3\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count     mean      std     min       max   median\n",
      "Tipo de Modelo                                                    \n",
      "ARIMA(0,1,0)      100   3.0924   3.2400  0.2479   11.7104   1.4844\n",
      "ARIMA(0,1,1)      100   3.5450   3.5477  0.2527   16.0802   2.4368\n",
      "ARIMA(0,1,2)      100   4.2623   6.0915  0.2869   29.8609   1.8971\n",
      "ARIMA(1,1,0)      100   7.7939   8.4182  0.4299   36.8470   4.7099\n",
      "ARIMA(1,1,1)      100  10.3856  11.4692  0.3206   37.9469   5.3940\n",
      "ARIMA(2,1,0)      100   5.2395   5.0606  0.2671   18.5117   3.4271\n",
      "ARIMA(2,1,2)      100  32.6500  31.8985  0.3303  122.8322  25.3846\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,0)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count     mean      std     min       max  median\n",
      "Distribuci√≥n                                                   \n",
      "exponential     140   8.8571  10.2723  0.4120   43.4345  4.8099\n",
      "mixture         140   7.9294  16.9407  0.2527   99.1204  2.2064\n",
      "normal          140  12.7947  24.0559  0.2671  122.8322  4.4637\n",
      "t-student       140   8.4553  13.3497  0.2479   59.7595  2.6843\n",
      "uniform         140   9.7983  15.5435  0.2869   74.2528  3.1934\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: mixture\n",
      "‚úì Peor rendimiento en distribuci√≥n: normal\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count     mean      std     min       max  median\n",
      "Varianza error                                                   \n",
      "0.2               175   3.7705   5.2963  0.2479   31.1760  2.2989\n",
      "0.5               175   7.0723  11.4456  0.4120   60.1119  2.3347\n",
      "1.0               175   9.2264  11.9897  0.5395   59.7595  4.8408\n",
      "3.0               175  18.1987  26.5572  0.9805  122.8322  6.7859\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.3172\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    AREPD\n",
      "    3   ARIMA(0,1,0)    t-student             0.2 0.247947\n",
      "    1   ARIMA(0,1,0)    t-student             0.2 0.249689\n",
      "    2   ARIMA(0,1,1)      mixture             0.2 0.252743\n",
      "    1   ARIMA(0,1,0)      mixture             0.2 0.254779\n",
      "    5   ARIMA(0,1,0)      mixture             0.2 0.262230\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error      AREPD\n",
      "    2   ARIMA(2,1,2)       normal             3.0 118.262761\n",
      "    1   ARIMA(2,1,2)       normal             3.0 120.671511\n",
      "    4   ARIMA(2,1,2)       normal             3.0 121.245824\n",
      "    5   ARIMA(2,1,2)       normal             3.0 121.862887\n",
      "    3   ARIMA(2,1,2)       normal             3.0 122.832161\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2479\n",
      "   Peor rendimiento: 122.8322\n",
      "   Rendimiento promedio: 9.5670\n",
      "   Desviaci√≥n est√°ndar: 16.7210\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=16.3501)\n",
      "   Paso menos estable: 5 (std=17.1570)\n",
      "   Tipo m√°s estable: ARIMA(0,1,0) (std=3.2400)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=31.8985)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\AREPD\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[20/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: AV-MCPS en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  1.2436  2.9898  0.2310  34.2554  0.6518\n",
      "2       140  2.1107  5.2999  0.2512  60.4301  0.9738\n",
      "3       140  2.3551  5.5791  0.2389  61.8293  1.0687\n",
      "4       140  2.6316  5.3871  0.2529  56.4375  1.0281\n",
      "5       140  2.4549  4.5570  0.2499  45.5540  1.1367\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean      std     min      max  median\n",
      "Tipo de Modelo                                                 \n",
      "ARIMA(0,1,0)      100  1.1583   1.3429  0.2310   7.4498  0.7649\n",
      "ARIMA(0,1,1)      100  1.0236   0.8170  0.2574   5.7552  0.8321\n",
      "ARIMA(0,1,2)      100  1.3286   1.4066  0.2389   7.4369  0.8244\n",
      "ARIMA(1,1,0)      100  1.8203   1.7075  0.2574   9.8856  1.0797\n",
      "ARIMA(1,1,1)      100  2.3616   3.2808  0.2507  18.2508  1.1670\n",
      "ARIMA(2,1,0)      100  1.1723   1.2201  0.2417   7.2470  0.8153\n",
      "ARIMA(2,1,2)      100  6.2498  11.2519  0.2792  61.8293  3.7259\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,1)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  2.0303  2.6083  0.2310  17.8741  1.1071\n",
      "mixture         140  1.6585  2.3835  0.2477  13.9114  0.7285\n",
      "normal          140  3.6743  9.8421  0.2585  61.8293  0.9984\n",
      "t-student       140  1.5400  1.5367  0.2417   7.2470  0.9590\n",
      "uniform         140  1.8929  2.0950  0.2626  11.2309  1.0649\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: normal\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  0.8410  1.0317  0.2310   6.2492  0.4384\n",
      "0.5               175  1.5564  1.8597  0.3730  13.8065  0.7839\n",
      "1.0               175  1.8871  2.1658  0.5178  11.2309  0.9437\n",
      "3.0               175  4.3524  8.8846  0.8950  61.8293  1.8397\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.2703\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  AV-MCPS\n",
      "    1   ARIMA(0,1,0)  exponential             0.2 0.230991\n",
      "    3   ARIMA(0,1,2)  exponential             0.2 0.238881\n",
      "    1   ARIMA(2,1,0)    t-student             0.2 0.241705\n",
      "    3   ARIMA(0,1,0)    t-student             0.2 0.242488\n",
      "    1   ARIMA(0,1,0)      mixture             0.2 0.247729\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error   AV-MCPS\n",
      "    1   ARIMA(2,1,2)       normal             3.0 34.255427\n",
      "    5   ARIMA(2,1,2)       normal             3.0 45.553990\n",
      "    4   ARIMA(2,1,2)       normal             3.0 56.437514\n",
      "    2   ARIMA(2,1,2)       normal             3.0 60.430076\n",
      "    3   ARIMA(2,1,2)       normal             3.0 61.829320\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2310\n",
      "   Peor rendimiento: 61.8293\n",
      "   Rendimiento promedio: 2.1592\n",
      "   Desviaci√≥n est√°ndar: 4.8674\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=2.9898)\n",
      "   Paso menos estable: 3 (std=5.5791)\n",
      "   Tipo m√°s estable: ARIMA(0,1,1) (std=0.8170)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=11.2519)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\AV-MCPS\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[21/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: Block Bootstrapping en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  0.5368  0.2690  0.2211  1.0194  0.4586\n",
      "2       140  0.5450  0.2722  0.2236  1.0488  0.4706\n",
      "3       140  0.5442  0.2728  0.2225  1.0633  0.4626\n",
      "4       140  0.5449  0.2752  0.2231  1.2894  0.4989\n",
      "5       140  0.5416  0.2716  0.2227  1.0600  0.4586\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 2\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "ARIMA(0,1,0)      100  0.5407  0.2700  0.2225  1.0665  0.4665\n",
      "ARIMA(0,1,1)      100  0.5407  0.2705  0.2225  1.0752  0.4693\n",
      "ARIMA(0,1,2)      100  0.5404  0.2677  0.2245  1.0488  0.5004\n",
      "ARIMA(1,1,0)      100  0.5413  0.2747  0.2211  1.0223  0.4644\n",
      "ARIMA(1,1,1)      100  0.5417  0.2757  0.2227  1.2894  0.4605\n",
      "ARIMA(2,1,0)      100  0.5459  0.2746  0.2231  1.0600  0.4599\n",
      "ARIMA(2,1,2)      100  0.5467  0.2746  0.2242  1.0633  0.4614\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,2)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  0.5040  0.2547  0.2211  1.0136  0.4982\n",
      "mixture         140  0.5475  0.2709  0.2462  1.0194  0.4906\n",
      "normal          140  0.5585  0.2787  0.2517  1.2894  0.4908\n",
      "t-student       140  0.5305  0.2658  0.2386  1.0335  0.4697\n",
      "uniform         140  0.5720  0.2847  0.2569  1.0752  0.5071\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.2486  0.0142  0.2211  0.3077  0.2495\n",
      "0.5               175  0.3947  0.0231  0.3506  0.5648  0.3954\n",
      "1.0               175  0.5595  0.0414  0.4975  0.7683  0.5598\n",
      "3.0               175  0.9672  0.0534  0.8575  1.2894  0.9736\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.9782\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Block Bootstrapping\n",
      "    1   ARIMA(1,1,0)  exponential             0.2             0.221126\n",
      "    3   ARIMA(0,1,0)  exponential             0.2             0.222547\n",
      "    1   ARIMA(0,1,1)  exponential             0.2             0.222549\n",
      "    5   ARIMA(1,1,1)  exponential             0.2             0.222678\n",
      "    5   ARIMA(0,1,0)  exponential             0.2             0.222721\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Block Bootstrapping\n",
      "    5   ARIMA(2,1,0)      uniform             3.0             1.059957\n",
      "    3   ARIMA(2,1,2)      uniform             3.0             1.063322\n",
      "    4   ARIMA(0,1,0)      uniform             3.0             1.066541\n",
      "    4   ARIMA(0,1,1)      uniform             3.0             1.075161\n",
      "    4   ARIMA(1,1,1)       normal             3.0             1.289371\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2211\n",
      "   Peor rendimiento: 1.2894\n",
      "   Rendimiento promedio: 0.5425\n",
      "   Desviaci√≥n est√°ndar: 0.2714\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 1 (std=0.2690)\n",
      "   Paso menos estable: 4 (std=0.2752)\n",
      "   Tipo m√°s estable: ARIMA(0,1,2) (std=0.2677)\n",
      "   Tipo menos estable: ARIMA(1,1,1) (std=0.2757)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\Block Bootstrapping\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[22/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: DeepAR en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  1.7451  3.1514  0.2751  28.0104  0.9945\n",
      "2       140  1.7737  2.6842  0.2764  23.9732  0.9463\n",
      "3       140  2.1550  4.3560  0.2756  39.4717  0.8312\n",
      "4       140  2.0187  3.6433  0.2455  27.5677  0.8599\n",
      "5       140  2.0835  4.2098  0.2421  35.8705  0.8419\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 3\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Tipo de Modelo                                                \n",
      "ARIMA(0,1,0)      100  0.9711  0.7664  0.2672   4.3256  0.7745\n",
      "ARIMA(0,1,1)      100  1.0853  1.2390  0.2986   8.1551  0.7030\n",
      "ARIMA(0,1,2)      100  1.1920  1.4376  0.3111  12.5392  0.7720\n",
      "ARIMA(1,1,0)      100  2.1525  2.5048  0.2421  14.3163  1.2121\n",
      "ARIMA(1,1,1)      100  1.2775  0.9580  0.2955   4.8204  0.9449\n",
      "ARIMA(2,1,0)      100  0.9677  0.6071  0.2835   3.0534  0.7394\n",
      "ARIMA(2,1,2)      100  6.0402  7.8734  0.2537  39.4717  3.5155\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(2,1,0)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  1.9173  3.4325  0.2421  35.8705  1.1011\n",
      "mixture         140  2.0160  3.7529  0.2537  27.5677  0.8118\n",
      "normal          140  2.0195  3.5914  0.2915  28.0104  0.9845\n",
      "t-student       140  1.4897  1.6054  0.2672  12.0055  0.8635\n",
      "uniform         140  2.3335  5.0529  0.2835  39.4717  0.8534\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: uniform\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  0.7724  0.9386  0.2421   7.7815  0.4239\n",
      "0.5               175  1.3293  1.5612  0.3974   8.8438  0.6775\n",
      "1.0               175  2.0495  4.0380  0.5299  39.4717  0.8674\n",
      "3.0               175  3.6696  5.4156  0.9790  35.8705  1.5905\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.2938\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error   DeepAR\n",
      "    5   ARIMA(1,1,0)  exponential             0.2 0.242110\n",
      "    4   ARIMA(1,1,0)  exponential             0.2 0.245501\n",
      "    5   ARIMA(2,1,2)      mixture             0.2 0.253729\n",
      "    5   ARIMA(0,1,0)    t-student             0.2 0.267229\n",
      "    4   ARIMA(0,1,0)  exponential             0.2 0.267617\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    DeepAR\n",
      "    3   ARIMA(2,1,2)      mixture             3.0 24.631292\n",
      "    4   ARIMA(2,1,2)      mixture             3.0 27.567728\n",
      "    1   ARIMA(2,1,2)       normal             3.0 28.010418\n",
      "    5   ARIMA(2,1,2)  exponential             3.0 35.870541\n",
      "    3   ARIMA(2,1,2)      uniform             1.0 39.471709\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2421\n",
      "   Peor rendimiento: 39.4717\n",
      "   Rendimiento promedio: 1.9552\n",
      "   Desviaci√≥n est√°ndar: 3.6568\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=2.6842)\n",
      "   Paso menos estable: 3 (std=4.3560)\n",
      "   Tipo m√°s estable: ARIMA(2,1,0) (std=0.6071)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=7.8734)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\DeepAR\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[23/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: EnCQR-LSTM en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  3.5172  4.4392  0.2817  32.8916  1.8749\n",
      "2       140  3.6383  4.3986  0.3229  28.0780  2.2263\n",
      "3       140  3.7928  4.8734  0.2967  29.4289  2.0114\n",
      "4       140  3.6732  4.8532  0.3298  31.1474  1.9141\n",
      "5       140  3.6893  4.7518  0.3482  35.6487  1.9536\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 3\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count     mean     std     min      max  median\n",
      "Tipo de Modelo                                                 \n",
      "ARIMA(0,1,0)      100   1.4142  0.9838  0.2817   5.3384  1.2192\n",
      "ARIMA(0,1,1)      100   1.9083  1.6636  0.4208   8.1222  1.2385\n",
      "ARIMA(0,1,2)      100   2.0654  1.9642  0.3736  12.6851  1.4294\n",
      "ARIMA(1,1,0)      100   3.9497  3.0046  0.4965  17.0637  3.6619\n",
      "ARIMA(1,1,1)      100   3.6834  3.1229  0.7437  17.4455  2.3763\n",
      "ARIMA(2,1,0)      100   2.4824  2.2429  0.3826  11.5946  1.6411\n",
      "ARIMA(2,1,2)      100  10.1319  8.1883  0.9318  35.6487  7.3553\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,0)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  3.8487  4.0727  0.3826  19.5923  2.2856\n",
      "mixture         140  2.8078  2.8831  0.3070  18.6398  1.8169\n",
      "normal          140  4.4355  5.9116  0.3229  32.8916  2.3947\n",
      "t-student       140  3.3012  3.4913  0.2817  16.0676  1.7966\n",
      "uniform         140  3.9178  5.9474  0.3736  35.6487  1.5789\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: mixture\n",
      "‚úì Peor rendimiento en distribuci√≥n: normal\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  1.8343  2.5827  0.2817  16.9445  0.8796\n",
      "0.5               175  2.8612  3.4081  0.5071  23.4513  1.7118\n",
      "1.0               175  3.7195  5.1062  0.6075  35.6487  2.0744\n",
      "3.0               175  6.2337  5.6602  1.2407  32.8916  4.2506\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.3449\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  EnCQR-LSTM\n",
      "    1   ARIMA(0,1,0)    t-student             0.2    0.281704\n",
      "    3   ARIMA(0,1,0)    t-student             0.2    0.296740\n",
      "    1   ARIMA(0,1,0)      mixture             0.2    0.306987\n",
      "    2   ARIMA(0,1,0)       normal             0.2    0.322919\n",
      "    4   ARIMA(0,1,0)      mixture             0.2    0.329801\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  EnCQR-LSTM\n",
      "    2   ARIMA(2,1,2)       normal             3.0   28.078030\n",
      "    3   ARIMA(2,1,2)      uniform             1.0   29.428930\n",
      "    4   ARIMA(2,1,2)      uniform             1.0   31.147448\n",
      "    1   ARIMA(2,1,2)       normal             3.0   32.891611\n",
      "    5   ARIMA(2,1,2)      uniform             1.0   35.648721\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2817\n",
      "   Peor rendimiento: 35.6487\n",
      "   Rendimiento promedio: 3.6622\n",
      "   Desviaci√≥n est√°ndar: 4.6552\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=4.3986)\n",
      "   Paso menos estable: 3 (std=4.8734)\n",
      "   Tipo m√°s estable: ARIMA(0,1,0) (std=0.9838)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=8.1883)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\EnCQR-LSTM\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[24/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: LSPM en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  1.1291  1.0188  0.2272  7.8588  0.8698\n",
      "2       140  1.0648  0.8336  0.2251  4.9208  0.8794\n",
      "3       140  1.1188  0.9425  0.2495  6.1956  0.8760\n",
      "4       140  1.1086  0.9424  0.2324  7.2885  0.8533\n",
      "5       140  1.0346  1.0416  0.2266  9.4901  0.7461\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 1\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "ARIMA(0,1,0)      100  0.7724  0.4566  0.2474  2.1677  0.6201\n",
      "ARIMA(0,1,1)      100  1.0236  0.7675  0.2645  4.0725  0.8122\n",
      "ARIMA(0,1,2)      100  0.8773  0.5683  0.2324  2.5430  0.7052\n",
      "ARIMA(1,1,0)      100  1.1901  1.0846  0.2266  7.8588  0.9039\n",
      "ARIMA(1,1,1)      100  0.9635  0.6466  0.2492  3.2366  0.7802\n",
      "ARIMA(2,1,0)      100  1.0870  0.8818  0.2251  4.8330  0.8563\n",
      "ARIMA(2,1,2)      100  1.7244  1.5433  0.2516  9.4901  1.1660\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,0)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  1.0637  1.1943  0.2251  9.4901  0.8261\n",
      "mixture         140  1.1728  1.0924  0.2462  7.2885  0.8399\n",
      "normal          140  1.0543  0.8370  0.2510  4.2785  0.7823\n",
      "t-student       140  1.0139  0.7909  0.2474  4.9208  0.8417\n",
      "uniform         140  1.1513  0.7977  0.2600  3.6807  0.9777\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: mixture\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.5297  0.3992  0.2251  2.8396  0.3605\n",
      "0.5               175  0.8518  0.5610  0.3587  3.4743  0.6641\n",
      "1.0               175  1.1060  0.7328  0.4990  5.6337  0.8412\n",
      "3.0               175  1.8773  1.2913  0.8614  9.4901  1.4483\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.5130\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     LSPM\n",
      "    2   ARIMA(2,1,0)  exponential             0.2 0.225076\n",
      "    5   ARIMA(1,1,0)  exponential             0.2 0.226597\n",
      "    1   ARIMA(2,1,0)  exponential             0.2 0.227182\n",
      "    4   ARIMA(0,1,2)  exponential             0.2 0.232385\n",
      "    2   ARIMA(1,1,0)  exponential             0.2 0.235553\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     LSPM\n",
      "    4   ARIMA(2,1,2)  exponential             1.0 5.633734\n",
      "    3   ARIMA(2,1,2)      mixture             3.0 6.195609\n",
      "    4   ARIMA(2,1,2)      mixture             3.0 7.288522\n",
      "    1   ARIMA(1,1,0)  exponential             3.0 7.858848\n",
      "    5   ARIMA(2,1,2)  exponential             3.0 9.490063\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2251\n",
      "   Peor rendimiento: 9.4901\n",
      "   Rendimiento promedio: 1.0912\n",
      "   Desviaci√≥n est√°ndar: 0.9565\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=0.8336)\n",
      "   Paso menos estable: 5 (std=1.0416)\n",
      "   Tipo m√°s estable: ARIMA(0,1,0) (std=0.4566)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=1.5433)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\LSPM\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[25/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: LSPMW en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min     max  median\n",
      "Paso                                               \n",
      "1       140  1.0562  0.8989  0.2272  6.1799  0.8281\n",
      "2       140  1.0694  0.8174  0.2239  4.8818  0.9020\n",
      "3       140  1.1349  0.9408  0.2387  5.9531  0.9110\n",
      "4       140  1.1424  0.9341  0.2300  6.9528  0.9221\n",
      "5       140  1.0473  1.0456  0.2481  9.8141  0.7385\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 5\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Tipo de Modelo                                               \n",
      "ARIMA(0,1,0)      100  0.8071  0.5084  0.2481  2.6988  0.6116\n",
      "ARIMA(0,1,1)      100  1.0553  0.7896  0.2756  4.0204  0.8672\n",
      "ARIMA(0,1,2)      100  0.8879  0.5871  0.2300  3.3269  0.6679\n",
      "ARIMA(1,1,0)      100  1.1669  0.9652  0.2387  6.1799  0.9342\n",
      "ARIMA(1,1,1)      100  0.9467  0.6270  0.2754  2.9547  0.7761\n",
      "ARIMA(2,1,0)      100  1.1070  0.8990  0.2239  5.4403  0.9296\n",
      "ARIMA(2,1,2)      100  1.6592  1.4999  0.2513  9.8141  1.1240\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,0)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min     max  median\n",
      "Distribuci√≥n                                               \n",
      "exponential     140  1.0766  1.1517  0.2239  9.8141  0.8425\n",
      "mixture         140  1.1604  1.0400  0.2501  6.9528  0.8987\n",
      "normal          140  1.0490  0.7790  0.2716  4.1644  0.8261\n",
      "t-student       140  1.0053  0.8170  0.2481  5.4403  0.7413\n",
      "uniform         140  1.1587  0.7964  0.2615  3.5930  0.9620\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: mixture\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min     max  median\n",
      "Varianza error                                               \n",
      "0.2               175  0.5181  0.3483  0.2239  2.4964  0.3815\n",
      "0.5               175  0.8372  0.5418  0.3622  3.5019  0.6133\n",
      "1.0               175  1.1108  0.6870  0.5077  5.5091  0.8937\n",
      "3.0               175  1.8940  1.2380  0.9037  9.8141  1.4535\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.5410\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    LSPMW\n",
      "    2   ARIMA(2,1,0)  exponential             0.2 0.223950\n",
      "    1   ARIMA(2,1,0)  exponential             0.2 0.227203\n",
      "    4   ARIMA(0,1,2)  exponential             0.2 0.230003\n",
      "    3   ARIMA(1,1,0)  exponential             0.2 0.238658\n",
      "    5   ARIMA(0,1,0)    t-student             0.2 0.248068\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error    LSPMW\n",
      "    4   ARIMA(2,1,2)  exponential             1.0 5.509101\n",
      "    3   ARIMA(2,1,2)      mixture             3.0 5.953120\n",
      "    1   ARIMA(1,1,0)  exponential             3.0 6.179926\n",
      "    4   ARIMA(2,1,2)      mixture             3.0 6.952830\n",
      "    5   ARIMA(2,1,2)  exponential             3.0 9.814071\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2239\n",
      "   Peor rendimiento: 9.8141\n",
      "   Rendimiento promedio: 1.0900\n",
      "   Desviaci√≥n est√°ndar: 0.9285\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=0.8174)\n",
      "   Paso menos estable: 5 (std=1.0456)\n",
      "   Tipo m√°s estable: ARIMA(0,1,0) (std=0.5084)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=1.4999)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\LSPMW\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[26/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: MCPS en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  2.1581  4.9626  0.2317  48.9828  0.8159\n",
      "2       140  2.6095  4.9068  0.2475  46.4192  1.0464\n",
      "3       140  2.6375  5.1996  0.2408  49.6527  1.0504\n",
      "4       140  2.5866  5.2548  0.2461  47.2389  1.0574\n",
      "5       140  2.7488  5.4855  0.2458  46.9513  1.0745\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 1\n",
      "‚úì Peor rendimiento en paso: 5\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean      std     min      max  median\n",
      "Tipo de Modelo                                                 \n",
      "ARIMA(0,1,0)      100  1.0917   0.9382  0.2317   4.6180  0.7597\n",
      "ARIMA(0,1,1)      100  1.0229   0.8303  0.2567   4.3869  0.7329\n",
      "ARIMA(0,1,2)      100  1.6477   2.3048  0.2702  12.5439  0.9436\n",
      "ARIMA(1,1,0)      100  1.7641   1.6416  0.2672   8.2736  1.0419\n",
      "ARIMA(1,1,1)      100  2.5896   2.9398  0.2588  13.5737  1.4779\n",
      "ARIMA(2,1,0)      100  1.3327   1.3728  0.2427   6.8190  0.8096\n",
      "ARIMA(2,1,2)      100  8.3879  11.2104  0.2629  49.6527  4.5431\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,1)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  2.4174  2.8913  0.2317  12.5439  1.1257\n",
      "mixture         140  2.1800  4.6807  0.2495  31.0564  0.7063\n",
      "normal          140  4.0286  9.2631  0.2583  49.6527  1.0388\n",
      "t-student       140  1.6503  1.7758  0.2408   7.5423  0.9562\n",
      "uniform         140  2.4642  3.3530  0.2619  17.6373  1.1844\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: t-student\n",
      "‚úì Peor rendimiento en distribuci√≥n: normal\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  1.0056  1.5483  0.2317  10.4352  0.5148\n",
      "0.5               175  2.0246  3.2568  0.3743  23.3571  0.7602\n",
      "1.0               175  2.1352  2.9315  0.5172  17.6373  0.9406\n",
      "3.0               175  5.0269  8.7263  0.8924  49.6527  1.9793\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.2869\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error     MCPS\n",
      "    1   ARIMA(0,1,0)  exponential             0.2 0.231722\n",
      "    3   ARIMA(0,1,0)    t-student             0.2 0.240822\n",
      "    1   ARIMA(2,1,0)    t-student             0.2 0.242661\n",
      "    5   ARIMA(0,1,0)    t-student             0.2 0.245780\n",
      "    4   ARIMA(0,1,0)    t-student             0.2 0.246070\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error      MCPS\n",
      "    2   ARIMA(2,1,2)       normal             3.0 46.419245\n",
      "    5   ARIMA(2,1,2)       normal             3.0 46.951294\n",
      "    4   ARIMA(2,1,2)       normal             3.0 47.238947\n",
      "    1   ARIMA(2,1,2)       normal             3.0 48.982755\n",
      "    3   ARIMA(2,1,2)       normal             3.0 49.652705\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2317\n",
      "   Peor rendimiento: 49.6527\n",
      "   Rendimiento promedio: 2.5481\n",
      "   Desviaci√≥n est√°ndar: 5.1553\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=4.9068)\n",
      "   Paso menos estable: 5 (std=5.4855)\n",
      "   Tipo m√°s estable: ARIMA(0,1,1) (std=0.8303)\n",
      "   Tipo menos estable: ARIMA(2,1,2) (std=11.2104)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\MCPS\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "[27/27] Procesando...\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS: Sieve Bootstrap en escenario No_Estacionario_Lineal\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  AN√ÅLISIS POR PASO\n",
      "--------------------------------------------------\n",
      "      count    mean     std     min      max  median\n",
      "Paso                                                \n",
      "1       140  0.6754  0.5273  0.2231   3.8165  0.5536\n",
      "2       140  0.6640  0.3967  0.2292   1.9568  0.5612\n",
      "3       140  0.7137  0.5345  0.2223   3.6237  0.5620\n",
      "4       140  0.9748  2.9197  0.2276  34.3411  0.5539\n",
      "5       140  0.7168  0.5849  0.2230   4.1779  0.5496\n",
      "\n",
      "‚úì Mejor rendimiento en paso: 2\n",
      "‚úì Peor rendimiento en paso: 4\n",
      "\n",
      "2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Tipo de Modelo                                                \n",
      "ARIMA(0,1,0)      100  0.5555  0.2728  0.2223   1.0683  0.4827\n",
      "ARIMA(0,1,1)      100  0.6011  0.2970  0.2231   1.4389  0.5429\n",
      "ARIMA(0,1,2)      100  0.5672  0.2794  0.2327   1.2195  0.4952\n",
      "ARIMA(1,1,0)      100  0.6846  0.4047  0.2263   2.5410  0.5802\n",
      "ARIMA(1,1,1)      100  0.6801  0.6041  0.2292   5.6730  0.5495\n",
      "ARIMA(2,1,0)      100  0.9442  3.3927  0.2271  34.3411  0.5238\n",
      "ARIMA(2,1,2)      100  1.2099  0.9590  0.2512   4.6818  0.9226\n",
      "\n",
      "‚úì Mejor rendimiento en tipo: ARIMA(0,1,0)\n",
      "‚úì Peor rendimiento en tipo: ARIMA(2,1,2)\n",
      "\n",
      "3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\n",
      "--------------------------------------------------\n",
      "              count    mean     std     min      max  median\n",
      "Distribuci√≥n                                                \n",
      "exponential     140  0.7117  0.5418  0.2223   3.6891  0.5083\n",
      "mixture         140  0.7410  0.7653  0.2457   5.6730  0.5573\n",
      "normal          140  0.7256  0.5408  0.2512   3.8165  0.5825\n",
      "t-student       140  0.8476  2.8682  0.2402  34.3411  0.5403\n",
      "uniform         140  0.7188  0.4930  0.2575   2.4550  0.5823\n",
      "\n",
      "‚úì Mejor rendimiento en distribuci√≥n: exponential\n",
      "‚úì Peor rendimiento en distribuci√≥n: t-student\n",
      "\n",
      "4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\n",
      "--------------------------------------------------\n",
      "                count    mean     std     min      max  median\n",
      "Varianza error                                                \n",
      "0.2               175  0.3846  0.5093  0.2223   5.6730  0.2635\n",
      "0.5               175  0.5513  0.3812  0.3531   2.5869  0.4135\n",
      "1.0               175  0.6548  0.2589  0.4933   2.4215  0.5831\n",
      "3.0               175  1.4050  2.5741  0.8605  34.3411  1.0087\n",
      "\n",
      "‚úì Mejor rendimiento en varianza: 0.2\n",
      "‚úì Peor rendimiento en varianza: 3.0\n",
      "‚úì Correlaci√≥n con varianza: 0.2816\n",
      "\n",
      "5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\n",
      "--------------------------------------------------\n",
      "\n",
      "üèÜ TOP 5 MEJORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Sieve Bootstrap\n",
      "    3   ARIMA(0,1,0)  exponential             0.2         0.222322\n",
      "    5   ARIMA(0,1,0)  exponential             0.2         0.223022\n",
      "    1   ARIMA(0,1,1)  exponential             0.2         0.223099\n",
      "    1   ARIMA(0,1,0)  exponential             0.2         0.226022\n",
      "    3   ARIMA(1,1,0)  exponential             0.2         0.226280\n",
      "\n",
      "‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\n",
      " Paso Tipo de Modelo Distribuci√≥n  Varianza error  Sieve Bootstrap\n",
      "    1   ARIMA(2,1,2)       normal             3.0         3.816540\n",
      "    5   ARIMA(2,1,2)      mixture             3.0         4.177879\n",
      "    4   ARIMA(2,1,2)      mixture             3.0         4.681807\n",
      "    4   ARIMA(1,1,1)      mixture             0.2         5.673008\n",
      "    4   ARIMA(2,1,0)    t-student             3.0        34.341079\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   Mejor rendimiento: 0.2223\n",
      "   Peor rendimiento: 34.3411\n",
      "   Rendimiento promedio: 0.7489\n",
      "   Desviaci√≥n est√°ndar: 1.3856\n",
      "\n",
      "6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\n",
      "--------------------------------------------------\n",
      "üìà ESTABILIDAD DEL MODELO:\n",
      "   Paso m√°s estable: 2 (std=0.3967)\n",
      "   Paso menos estable: 4 (std=2.9197)\n",
      "   Tipo m√°s estable: ARIMA(0,1,0) (std=0.2728)\n",
      "   Tipo menos estable: ARIMA(2,1,0) (std=3.3927)\n",
      "\n",
      "‚úÖ Resumen guardado en: Resultados\\No_Estacionario_Lineal\\Sieve Bootstrap\\resumen_analisis.txt\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERANDO RESUMEN COMPARATIVO\n",
      "================================================================================\n",
      "\n",
      "‚úì Comparativo generado para Estacionario_Lineal\n",
      "‚úì Comparativo generado para No_Lineal_Estacionario\n",
      "‚úì Comparativo generado para No_Estacionario_Lineal\n",
      "\n",
      "================================================================================\n",
      "üèÜ MEJORES MODELOS POR ESCENARIO\n",
      "================================================================================\n",
      "\n",
      "Estacionario_Lineal:\n",
      "  Mejor modelo: Block Bootstrapping\n",
      "  Rendimiento promedio: 0.5396\n",
      "  Desviaci√≥n est√°ndar: 0.2681\n",
      "  Rango: [0.2215, 1.0419]\n",
      "\n",
      "No_Lineal_Estacionario:\n",
      "  Mejor modelo: Block Bootstrapping\n",
      "  Rendimiento promedio: 0.5913\n",
      "  Desviaci√≥n est√°ndar: 0.4241\n",
      "  Rango: [0.2224, 6.2332]\n",
      "\n",
      "No_Estacionario_Lineal:\n",
      "  Mejor modelo: Block Bootstrapping\n",
      "  Rendimiento promedio: 0.5425\n",
      "  Desviaci√≥n est√°ndar: 0.2714\n",
      "  Rango: [0.2211, 1.2894]\n",
      "\n",
      "================================================================================\n",
      "üîù RANKING GLOBAL DE MODELOS (promediado en todos los escenarios)\n",
      "================================================================================\n",
      "1. Block Bootstrapping  - Media: 0.5578, Std promedio: 0.3212\n",
      "2. Sieve Bootstrap      - Media: 0.6279, Std promedio: 0.6888\n",
      "3. LSPM                 - Media: 0.8308, Std promedio: 0.6832\n",
      "4. LSPMW                - Media: 0.8490, Std promedio: 0.6848\n",
      "5. DeepAR               - Media: 1.0506, Std promedio: 1.4458\n",
      "6. AV-MCPS              - Media: 1.1361, Std promedio: 1.8864\n",
      "7. MCPS                 - Media: 1.2650, Std promedio: 1.9848\n",
      "8. EnCQR-LSTM           - Media: 1.6514, Std promedio: 1.8255\n",
      "9. AREPD                - Media: 3.6398, Std promedio: 5.8578\n",
      "\n",
      "\n",
      "‚úÖ Res√∫menes comparativos guardados en: Resultados\\Comparativo\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚úÖ AN√ÅLISIS COMPLETO FINALIZADO\n",
      "   Total de an√°lisis realizados: 27\n",
      "   Resultados guardados en: ./Resultados/\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà                         AN√ÅLISIS COMPLETADO EXITOSAMENTE                      ‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üìÅ Los resultados se encuentran en:\n",
      "   ‚îî‚îÄ‚îÄ ./Resultados/\n",
      "       ‚îú‚îÄ‚îÄ Estacionario_Lineal/\n",
      "       ‚îÇ   ‚îú‚îÄ‚îÄ LSPM/\n",
      "       ‚îÇ   ‚îú‚îÄ‚îÄ LSPMW/\n",
      "       ‚îÇ   ‚îî‚îÄ‚îÄ ... (todos los modelos)\n",
      "       ‚îú‚îÄ‚îÄ No_Lineal_Estacionario/\n",
      "       ‚îú‚îÄ‚îÄ No_Estacionario_Lineal/\n",
      "       ‚îî‚îÄ‚îÄ Comparativo/\n",
      "           ‚îú‚îÄ‚îÄ comparativo_*.png\n",
      "           ‚îú‚îÄ‚îÄ ranking_*.xlsx\n",
      "           ‚îî‚îÄ‚îÄ resumen_global_completo.xlsx\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ruta de datos\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "\n",
    "# Modelos a analizar\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "\n",
    "# Escenarios\n",
    "ESCENARIOS = ['Estacionario_Lineal', 'No_Lineal_Estacionario', 'No_Estacionario_Lineal']\n",
    "\n",
    "# Caracter√≠sticas de simulaci√≥n\n",
    "CARACTERISTICAS = ['Paso', 'Tipo de Modelo', 'Distribuci√≥n', 'Varianza error']\n",
    "\n",
    "\n",
    "class AnalizadorModelos:\n",
    "    \"\"\"Clase para analizar el desempe√±o de modelos de predicci√≥n\"\"\"\n",
    "    \n",
    "    def __init__(self, ruta_datos):\n",
    "        \"\"\"Inicializa el analizador cargando los datos\"\"\"\n",
    "        self.df = pd.read_excel(ruta_datos)\n",
    "        self.resultados_analisis = {}\n",
    "        print(f\"‚úì Datos cargados: {self.df.shape[0]} filas, {self.df.shape[1]} columnas\")\n",
    "        print(f\"\\nEscenarios encontrados: {self.df['Escenario'].unique()}\")\n",
    "        print(f\"Modelos a analizar: {len(MODELOS)}\")\n",
    "        \n",
    "    def analizar_modelo_escenario(self, escenario, modelo):\n",
    "        \"\"\"\n",
    "        Realiza an√°lisis completo de un modelo en un escenario espec√≠fico\n",
    "        \n",
    "        Args:\n",
    "            escenario: Nombre del escenario\n",
    "            modelo: Nombre del modelo a analizar\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"AN√ÅLISIS: {modelo} en escenario {escenario}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Filtrar datos\n",
    "        df_filtrado = self.df[self.df['Escenario'] == escenario].copy()\n",
    "        \n",
    "        if df_filtrado.empty:\n",
    "            print(f\"‚ö† No hay datos para el escenario {escenario}\")\n",
    "            return\n",
    "        \n",
    "        # Crear directorio para guardar resultados\n",
    "        dir_salida = Path(f\"./Resultados/{escenario}/{modelo}\")\n",
    "        dir_salida.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        resultados = {\n",
    "            'escenario': escenario,\n",
    "            'modelo': modelo,\n",
    "            'n_observaciones': len(df_filtrado)\n",
    "        }\n",
    "        \n",
    "        # 1. An√°lisis por Paso\n",
    "        self._analizar_por_paso(df_filtrado, modelo, dir_salida, resultados)\n",
    "        \n",
    "        # 2. An√°lisis por Tipo de Modelo\n",
    "        self._analizar_por_tipo_modelo(df_filtrado, modelo, dir_salida, resultados)\n",
    "        \n",
    "        # 3. An√°lisis por Distribuci√≥n\n",
    "        self._analizar_por_distribucion(df_filtrado, modelo, dir_salida, resultados)\n",
    "        \n",
    "        # 4. An√°lisis por Varianza\n",
    "        self._analizar_por_varianza(df_filtrado, modelo, dir_salida, resultados)\n",
    "        \n",
    "        # 5. Mejores y peores configuraciones\n",
    "        self._analizar_configuraciones_extremas(df_filtrado, modelo, dir_salida, resultados)\n",
    "        \n",
    "        # 6. An√°lisis adicionales\n",
    "        self._analisis_interacciones(df_filtrado, modelo, dir_salida, resultados)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        self._guardar_resumen(resultados, dir_salida)\n",
    "        \n",
    "        return resultados\n",
    "    \n",
    "    def _analizar_por_paso(self, df, modelo, dir_salida, resultados):\n",
    "        \"\"\"Analiza c√≥mo cambia el rendimiento con respecto a cada valor de paso\"\"\"\n",
    "        print(\"1Ô∏è‚É£  AN√ÅLISIS POR PASO\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        stats_paso = df.groupby('Paso')[modelo].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "        ]).round(4)\n",
    "        \n",
    "        print(stats_paso)\n",
    "        \n",
    "        # Gr√°fica\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Boxplot\n",
    "        df.boxplot(column=modelo, by='Paso', ax=axes[0])\n",
    "        axes[0].set_title(f'{modelo} - Distribuci√≥n por Paso')\n",
    "        axes[0].set_xlabel('Paso')\n",
    "        axes[0].set_ylabel('Rendimiento')\n",
    "        axes[0].get_figure().suptitle('')\n",
    "        \n",
    "        # L√≠nea de tendencia\n",
    "        paso_mean = df.groupby('Paso')[modelo].mean()\n",
    "        axes[1].plot(paso_mean.index, paso_mean.values, marker='o', linewidth=2, markersize=8)\n",
    "        axes[1].fill_between(paso_mean.index, \n",
    "                             df.groupby('Paso')[modelo].quantile(0.25),\n",
    "                             df.groupby('Paso')[modelo].quantile(0.75),\n",
    "                             alpha=0.3)\n",
    "        axes[1].set_title(f'{modelo} - Tendencia por Paso')\n",
    "        axes[1].set_xlabel('Paso')\n",
    "        axes[1].set_ylabel('Rendimiento Promedio')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'analisis_paso.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        resultados['paso'] = {\n",
    "            'estadisticas': stats_paso.to_dict(),\n",
    "            'mejor_paso': paso_mean.idxmin(),\n",
    "            'peor_paso': paso_mean.idxmax(),\n",
    "            'variacion': paso_mean.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úì Mejor rendimiento en paso: {resultados['paso']['mejor_paso']}\")\n",
    "        print(f\"‚úì Peor rendimiento en paso: {resultados['paso']['peor_paso']}\\n\")\n",
    "    \n",
    "    def _analizar_por_tipo_modelo(self, df, modelo, dir_salida, resultados):\n",
    "        \"\"\"Analiza c√≥mo cambia el rendimiento con respecto a cada tipo de modelo\"\"\"\n",
    "        print(\"2Ô∏è‚É£  AN√ÅLISIS POR TIPO DE MODELO\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        stats_tipo = df.groupby('Tipo de Modelo')[modelo].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "        ]).round(4)\n",
    "        \n",
    "        print(stats_tipo)\n",
    "        \n",
    "        # Gr√°fica\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Boxplot\n",
    "        df.boxplot(column=modelo, by='Tipo de Modelo', ax=axes[0])\n",
    "        axes[0].set_title(f'{modelo} - Distribuci√≥n por Tipo de Modelo')\n",
    "        axes[0].set_xlabel('Tipo de Modelo')\n",
    "        axes[0].set_ylabel('Rendimiento')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].get_figure().suptitle('')\n",
    "        \n",
    "        # Barplot con error\n",
    "        tipo_stats = df.groupby('Tipo de Modelo')[modelo].agg(['mean', 'std'])\n",
    "        axes[1].bar(range(len(tipo_stats)), tipo_stats['mean'], \n",
    "                   yerr=tipo_stats['std'], capsize=5, alpha=0.7)\n",
    "        axes[1].set_xticks(range(len(tipo_stats)))\n",
    "        axes[1].set_xticklabels(tipo_stats.index, rotation=45, ha='right')\n",
    "        axes[1].set_title(f'{modelo} - Promedio por Tipo de Modelo')\n",
    "        axes[1].set_ylabel('Rendimiento Promedio')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'analisis_tipo_modelo.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        tipo_mean = df.groupby('Tipo de Modelo')[modelo].mean()\n",
    "        resultados['tipo_modelo'] = {\n",
    "            'estadisticas': stats_tipo.to_dict(),\n",
    "            'mejor_tipo': tipo_mean.idxmin(),\n",
    "            'peor_tipo': tipo_mean.idxmax(),\n",
    "            'variacion': tipo_mean.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úì Mejor rendimiento en tipo: {resultados['tipo_modelo']['mejor_tipo']}\")\n",
    "        print(f\"‚úì Peor rendimiento en tipo: {resultados['tipo_modelo']['peor_tipo']}\\n\")\n",
    "    \n",
    "    def _analizar_por_distribucion(self, df, modelo, dir_salida, resultados):\n",
    "        \"\"\"Analiza c√≥mo cambia el rendimiento con respecto a cada distribuci√≥n\"\"\"\n",
    "        print(\"3Ô∏è‚É£  AN√ÅLISIS POR DISTRIBUCI√ìN\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        stats_dist = df.groupby('Distribuci√≥n')[modelo].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "        ]).round(4)\n",
    "        \n",
    "        print(stats_dist)\n",
    "        \n",
    "        # Gr√°fica\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Violin plot\n",
    "        df_plot = df[['Distribuci√≥n', modelo]].copy()\n",
    "        sns.violinplot(data=df_plot, x='Distribuci√≥n', y=modelo, ax=axes[0])\n",
    "        axes[0].set_title(f'{modelo} - Distribuci√≥n por Tipo de Distribuci√≥n')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Stripplot superpuesto\n",
    "        sns.stripplot(data=df_plot, x='Distribuci√≥n', y=modelo, \n",
    "                     ax=axes[0], color='black', alpha=0.3, size=2)\n",
    "        \n",
    "        # Comparaci√≥n de medianas\n",
    "        dist_median = df.groupby('Distribuci√≥n')[modelo].median().sort_values()\n",
    "        axes[1].barh(range(len(dist_median)), dist_median.values, alpha=0.7)\n",
    "        axes[1].set_yticks(range(len(dist_median)))\n",
    "        axes[1].set_yticklabels(dist_median.index)\n",
    "        axes[1].set_title(f'{modelo} - Mediana por Distribuci√≥n')\n",
    "        axes[1].set_xlabel('Rendimiento Mediano')\n",
    "        axes[1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'analisis_distribucion.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        dist_mean = df.groupby('Distribuci√≥n')[modelo].mean()\n",
    "        resultados['distribucion'] = {\n",
    "            'estadisticas': stats_dist.to_dict(),\n",
    "            'mejor_distribucion': dist_mean.idxmin(),\n",
    "            'peor_distribucion': dist_mean.idxmax(),\n",
    "            'variacion': dist_mean.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úì Mejor rendimiento en distribuci√≥n: {resultados['distribucion']['mejor_distribucion']}\")\n",
    "        print(f\"‚úì Peor rendimiento en distribuci√≥n: {resultados['distribucion']['peor_distribucion']}\\n\")\n",
    "    \n",
    "    def _analizar_por_varianza(self, df, modelo, dir_salida, resultados):\n",
    "        \"\"\"Analiza c√≥mo cambia el rendimiento con respecto a cada varianza\"\"\"\n",
    "        print(\"4Ô∏è‚É£  AN√ÅLISIS POR VARIANZA DE ERROR\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        stats_var = df.groupby('Varianza error')[modelo].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "        ]).round(4)\n",
    "        \n",
    "        print(stats_var)\n",
    "        \n",
    "        # Gr√°fica\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        for var in df['Varianza error'].unique():\n",
    "            df_var = df[df['Varianza error'] == var]\n",
    "            axes[0].scatter(df_var['Varianza error'], df_var[modelo], \n",
    "                          alpha=0.5, label=f'Var={var}', s=50)\n",
    "        \n",
    "        var_mean = df.groupby('Varianza error')[modelo].mean()\n",
    "        axes[0].plot(var_mean.index, var_mean.values, 'r-', linewidth=2, \n",
    "                    marker='D', markersize=10, label='Media')\n",
    "        axes[0].set_xlabel('Varianza de Error')\n",
    "        axes[0].set_ylabel('Rendimiento')\n",
    "        axes[0].set_title(f'{modelo} - Rendimiento vs Varianza')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Boxplot\n",
    "        df.boxplot(column=modelo, by='Varianza error', ax=axes[1])\n",
    "        axes[1].set_title(f'{modelo} - Distribuci√≥n por Varianza')\n",
    "        axes[1].set_xlabel('Varianza de Error')\n",
    "        axes[1].set_ylabel('Rendimiento')\n",
    "        axes[1].get_figure().suptitle('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'analisis_varianza.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        resultados['varianza'] = {\n",
    "            'estadisticas': stats_var.to_dict(),\n",
    "            'mejor_varianza': var_mean.idxmin(),\n",
    "            'peor_varianza': var_mean.idxmax(),\n",
    "            'variacion': var_mean.std(),\n",
    "            'correlacion': df[['Varianza error', modelo]].corr().iloc[0, 1]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úì Mejor rendimiento en varianza: {resultados['varianza']['mejor_varianza']}\")\n",
    "        print(f\"‚úì Peor rendimiento en varianza: {resultados['varianza']['peor_varianza']}\")\n",
    "        print(f\"‚úì Correlaci√≥n con varianza: {resultados['varianza']['correlacion']:.4f}\\n\")\n",
    "    \n",
    "    def _analizar_configuraciones_extremas(self, df, modelo, dir_salida, resultados):\n",
    "        \"\"\"Identifica las mejores y peores configuraciones\"\"\"\n",
    "        print(\"5Ô∏è‚É£  MEJORES Y PEORES CONFIGURACIONES\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Ordenar por rendimiento\n",
    "        df_sorted = df.sort_values(by=modelo)\n",
    "        \n",
    "        # Top 5 mejores\n",
    "        top5_mejores = df_sorted.head(5)[['Paso', 'Tipo de Modelo', 'Distribuci√≥n', \n",
    "                                          'Varianza error', modelo]]\n",
    "        print(\"\\nüèÜ TOP 5 MEJORES CONFIGURACIONES:\")\n",
    "        print(top5_mejores.to_string(index=False))\n",
    "        \n",
    "        # Top 5 peores\n",
    "        top5_peores = df_sorted.tail(5)[['Paso', 'Tipo de Modelo', 'Distribuci√≥n', \n",
    "                                         'Varianza error', modelo]]\n",
    "        print(\"\\n‚ö†Ô∏è  TOP 5 PEORES CONFIGURACIONES:\")\n",
    "        print(top5_peores.to_string(index=False))\n",
    "        \n",
    "        # Estad√≠sticas generales\n",
    "        print(f\"\\nüìä ESTAD√çSTICAS GENERALES:\")\n",
    "        print(f\"   Mejor rendimiento: {df[modelo].min():.4f}\")\n",
    "        print(f\"   Peor rendimiento: {df[modelo].max():.4f}\")\n",
    "        print(f\"   Rendimiento promedio: {df[modelo].mean():.4f}\")\n",
    "        print(f\"   Desviaci√≥n est√°ndar: {df[modelo].std():.4f}\")\n",
    "        \n",
    "        # Gr√°fica de distribuci√≥n\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Histograma\n",
    "        axes[0, 0].hist(df[modelo], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 0].axvline(df[modelo].mean(), color='r', linestyle='--', \n",
    "                          linewidth=2, label=f'Media: {df[modelo].mean():.4f}')\n",
    "        axes[0, 0].axvline(df[modelo].median(), color='g', linestyle='--', \n",
    "                          linewidth=2, label=f'Mediana: {df[modelo].median():.4f}')\n",
    "        axes[0, 0].set_xlabel('Rendimiento')\n",
    "        axes[0, 0].set_ylabel('Frecuencia')\n",
    "        axes[0, 0].set_title(f'{modelo} - Distribuci√≥n General')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Heatmap de configuraciones\n",
    "        pivot_data = df.groupby(['Tipo de Modelo', 'Distribuci√≥n'])[modelo].mean().unstack()\n",
    "        if not pivot_data.empty:\n",
    "            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn_r', \n",
    "                       ax=axes[0, 1], cbar_kws={'label': 'Rendimiento'})\n",
    "            axes[0, 1].set_title(f'{modelo} - Heatmap Tipo vs Distribuci√≥n')\n",
    "        \n",
    "        # Paso vs Varianza heatmap\n",
    "        pivot_data2 = df.groupby(['Paso', 'Varianza error'])[modelo].mean().unstack()\n",
    "        if not pivot_data2.empty:\n",
    "            sns.heatmap(pivot_data2, annot=True, fmt='.3f', cmap='RdYlGn_r', \n",
    "                       ax=axes[1, 0], cbar_kws={'label': 'Rendimiento'})\n",
    "            axes[1, 0].set_title(f'{modelo} - Heatmap Paso vs Varianza')\n",
    "        \n",
    "        # Q-Q plot para normalidad\n",
    "        from scipy import stats\n",
    "        stats.probplot(df[modelo], dist=\"norm\", plot=axes[1, 1])\n",
    "        axes[1, 1].set_title(f'{modelo} - Q-Q Plot')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'configuraciones_extremas.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        resultados['configuraciones'] = {\n",
    "            'top5_mejores': top5_mejores.to_dict('records'),\n",
    "            'top5_peores': top5_peores.to_dict('records'),\n",
    "            'mejor_global': df[modelo].min(),\n",
    "            'peor_global': df[modelo].max(),\n",
    "            'promedio': df[modelo].mean(),\n",
    "            'std': df[modelo].std()\n",
    "        }\n",
    "        print()\n",
    "    \n",
    "    def _analisis_interacciones(self, df, modelo, dir_salida, resultados):\n",
    "        \"\"\"An√°lisis adicionales: interacciones entre variables\"\"\"\n",
    "        print(\"6Ô∏è‚É£  AN√ÅLISIS DE INTERACCIONES Y PATRONES\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Interacci√≥n Paso x Distribuci√≥n\n",
    "        for dist in df['Distribuci√≥n'].unique():\n",
    "            df_dist = df[df['Distribuci√≥n'] == dist]\n",
    "            paso_mean = df_dist.groupby('Paso')[modelo].mean()\n",
    "            axes[0, 0].plot(paso_mean.index, paso_mean.values, \n",
    "                          marker='o', label=dist, linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Paso')\n",
    "        axes[0, 0].set_ylabel('Rendimiento Promedio')\n",
    "        axes[0, 0].set_title('Interacci√≥n: Paso x Distribuci√≥n')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Interacci√≥n Varianza x Tipo de Modelo\n",
    "        for tipo in df['Tipo de Modelo'].unique():\n",
    "            df_tipo = df[df['Tipo de Modelo'] == tipo]\n",
    "            var_mean = df_tipo.groupby('Varianza error')[modelo].mean()\n",
    "            axes[0, 1].plot(var_mean.index, var_mean.values, \n",
    "                          marker='s', label=tipo, linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Varianza de Error')\n",
    "        axes[0, 1].set_ylabel('Rendimiento Promedio')\n",
    "        axes[0, 1].set_title('Interacci√≥n: Varianza x Tipo de Modelo')\n",
    "        axes[0, 1].legend(fontsize=8)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # An√°lisis por cuartiles\n",
    "        cuartiles = pd.qcut(df[modelo], q=4, labels=['Q1 (Mejor)', 'Q2', 'Q3', 'Q4 (Peor)'])\n",
    "        df_cuartiles = df.copy()\n",
    "        df_cuartiles['Cuartil'] = cuartiles\n",
    "        \n",
    "        cuartil_dist = df_cuartiles.groupby(['Cuartil', 'Distribuci√≥n']).size().unstack(fill_value=0)\n",
    "        cuartil_dist.plot(kind='bar', ax=axes[1, 0], stacked=True)\n",
    "        axes[1, 0].set_xlabel('Cuartil de Rendimiento')\n",
    "        axes[1, 0].set_ylabel('Frecuencia')\n",
    "        axes[1, 0].set_title('Distribuci√≥n de Cuartiles por Tipo de Distribuci√≥n')\n",
    "        axes[1, 0].legend(title='Distribuci√≥n', fontsize=8)\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Matriz de correlaci√≥n\n",
    "        caracteristicas_num = df[['Paso', 'Varianza error', modelo]].copy()\n",
    "        # Convertir Tipo de Modelo a num√©rico si es posible\n",
    "        if df['Tipo de Modelo'].dtype == 'object':\n",
    "            caracteristicas_num['Tipo_Modelo_Num'] = pd.Categorical(df['Tipo de Modelo']).codes\n",
    "        corr_matrix = caracteristicas_num.corr()\n",
    "        \n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "                   center=0, ax=axes[1, 1], square=True)\n",
    "        axes[1, 1].set_title('Matriz de Correlaci√≥n')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'analisis_interacciones.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de estabilidad\n",
    "        estabilidad_paso = df.groupby('Paso')[modelo].std()\n",
    "        estabilidad_tipo = df.groupby('Tipo de Modelo')[modelo].std()\n",
    "        \n",
    "        print(f\"üìà ESTABILIDAD DEL MODELO:\")\n",
    "        print(f\"   Paso m√°s estable: {estabilidad_paso.idxmin()} (std={estabilidad_paso.min():.4f})\")\n",
    "        print(f\"   Paso menos estable: {estabilidad_paso.idxmax()} (std={estabilidad_paso.max():.4f})\")\n",
    "        print(f\"   Tipo m√°s estable: {estabilidad_tipo.idxmin()} (std={estabilidad_tipo.min():.4f})\")\n",
    "        print(f\"   Tipo menos estable: {estabilidad_tipo.idxmax()} (std={estabilidad_tipo.max():.4f})\")\n",
    "        \n",
    "        resultados['interacciones'] = {\n",
    "            'estabilidad_paso': estabilidad_paso.to_dict(),\n",
    "            'estabilidad_tipo': estabilidad_tipo.to_dict(),\n",
    "            'correlaciones': corr_matrix.to_dict()\n",
    "        }\n",
    "        print()\n",
    "    \n",
    "    def _guardar_resumen(self, resultados, dir_salida):\n",
    "        \"\"\"Guarda un resumen en formato texto\"\"\"\n",
    "        with open(dir_salida / 'resumen_analisis.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "            f.write(f\"RESUMEN DE AN√ÅLISIS\\n\")\n",
    "            f.write(f\"Modelo: {resultados['modelo']}\\n\")\n",
    "            f.write(f\"Escenario: {resultados['escenario']}\\n\")\n",
    "            f.write(f\"Observaciones: {resultados['n_observaciones']}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"RECOMENDACIONES:\\n\")\n",
    "            f.write(f\"‚úì Mejor paso: {resultados['paso']['mejor_paso']}\\n\")\n",
    "            f.write(f\"‚úì Mejor tipo de modelo: {resultados['tipo_modelo']['mejor_tipo']}\\n\")\n",
    "            f.write(f\"‚úì Mejor distribuci√≥n: {resultados['distribucion']['mejor_distribucion']}\\n\")\n",
    "            f.write(f\"‚úì Mejor varianza: {resultados['varianza']['mejor_varianza']}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"EVITAR:\\n\")\n",
    "            f.write(f\"‚ö† Peor paso: {resultados['paso']['peor_paso']}\\n\")\n",
    "            f.write(f\"‚ö† Peor tipo de modelo: {resultados['tipo_modelo']['peor_tipo']}\\n\")\n",
    "            f.write(f\"‚ö† Peor distribuci√≥n: {resultados['distribucion']['peor_distribucion']}\\n\")\n",
    "            f.write(f\"‚ö† Peor varianza: {resultados['varianza']['peor_varianza']}\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Resumen guardado en: {dir_salida / 'resumen_analisis.txt'}\\n\")\n",
    "    \n",
    "    def ejecutar_analisis_completo(self):\n",
    "        \"\"\"Ejecuta el an√°lisis completo para todos los modelos y escenarios\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INICIANDO AN√ÅLISIS COMPLETO DE MODELOS\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        total_analisis = len(ESCENARIOS) * len(MODELOS)\n",
    "        contador = 0\n",
    "        \n",
    "        for escenario in ESCENARIOS:\n",
    "            for modelo in MODELOS:\n",
    "                contador += 1\n",
    "                print(f\"\\n[{contador}/{total_analisis}] Procesando...\")\n",
    "                \n",
    "                try:\n",
    "                    resultado = self.analizar_modelo_escenario(escenario, modelo)\n",
    "                    key = f\"{escenario}_{modelo}\"\n",
    "                    self.resultados_analisis[key] = resultado\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error en {modelo} - {escenario}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Generar resumen comparativo\n",
    "        self._generar_resumen_comparativo()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ AN√ÅLISIS COMPLETO FINALIZADO\")\n",
    "        print(f\"   Total de an√°lisis realizados: {contador}\")\n",
    "        print(f\"   Resultados guardados en: ./Resultados/\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    def _generar_resumen_comparativo(self):\n",
    "        \"\"\"Genera un resumen comparativo entre todos los modelos y escenarios\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"GENERANDO RESUMEN COMPARATIVO\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        dir_salida = Path(\"./Resultados/Comparativo\")\n",
    "        dir_salida.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Crear comparaci√≥n por escenario\n",
    "        for escenario in ESCENARIOS:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            fig.suptitle(f'Comparaci√≥n de Modelos - {escenario}', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            df_esc = self.df[self.df['Escenario'] == escenario]\n",
    "            \n",
    "            # Rendimiento promedio por modelo\n",
    "            modelo_means = df_esc[MODELOS].mean().sort_values()\n",
    "            axes[0, 0].barh(range(len(modelo_means)), modelo_means.values, alpha=0.7)\n",
    "            axes[0, 0].set_yticks(range(len(modelo_means)))\n",
    "            axes[0, 0].set_yticklabels(modelo_means.index, fontsize=9)\n",
    "            axes[0, 0].set_xlabel('Rendimiento Promedio')\n",
    "            axes[0, 0].set_title('Rendimiento Promedio por Modelo')\n",
    "            axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Estabilidad (desviaci√≥n est√°ndar)\n",
    "            modelo_stds = df_esc[MODELOS].std().sort_values()\n",
    "            axes[0, 1].barh(range(len(modelo_stds)), modelo_stds.values, alpha=0.7, color='orange')\n",
    "            axes[0, 1].set_yticks(range(len(modelo_stds)))\n",
    "            axes[0, 1].set_yticklabels(modelo_stds.index, fontsize=9)\n",
    "            axes[0, 1].set_xlabel('Desviaci√≥n Est√°ndar')\n",
    "            axes[0, 1].set_title('Estabilidad por Modelo (menor es mejor)')\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Boxplot comparativo\n",
    "            df_esc[MODELOS].boxplot(ax=axes[1, 0], rot=90)\n",
    "            axes[1, 0].set_ylabel('Rendimiento')\n",
    "            axes[1, 0].set_title('Distribuci√≥n de Rendimiento por Modelo')\n",
    "            axes[1, 0].tick_params(axis='x', labelsize=8)\n",
    "            \n",
    "            # Ranking de modelos\n",
    "            ranking_data = []\n",
    "            for modelo in MODELOS:\n",
    "                ranking_data.append({\n",
    "                    'Modelo': modelo,\n",
    "                    'Media': df_esc[modelo].mean(),\n",
    "                    'Mediana': df_esc[modelo].median(),\n",
    "                    'M√≠nimo': df_esc[modelo].min(),\n",
    "                    'M√°ximo': df_esc[modelo].max(),\n",
    "                    'Std': df_esc[modelo].std()\n",
    "                })\n",
    "            \n",
    "            df_ranking = pd.DataFrame(ranking_data).sort_values('Media')\n",
    "            \n",
    "            # Tabla de ranking\n",
    "            axes[1, 1].axis('tight')\n",
    "            axes[1, 1].axis('off')\n",
    "            tabla = axes[1, 1].table(\n",
    "                cellText=df_ranking.round(4).values,\n",
    "                colLabels=df_ranking.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center'\n",
    "            )\n",
    "            tabla.auto_set_font_size(False)\n",
    "            tabla.set_fontsize(8)\n",
    "            tabla.scale(1, 2)\n",
    "            axes[1, 1].set_title('Ranking de Modelos (ordenado por media)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(dir_salida / f'comparativo_{escenario}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Guardar tabla de ranking\n",
    "            df_ranking.to_excel(dir_salida / f'ranking_{escenario}.xlsx', index=False)\n",
    "            print(f\"‚úì Comparativo generado para {escenario}\")\n",
    "        \n",
    "        # Comparaci√≥n global entre escenarios\n",
    "        self._comparacion_global_escenarios(dir_salida)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Res√∫menes comparativos guardados en: {dir_salida}\\n\")\n",
    "    \n",
    "    def _comparacion_global_escenarios(self, dir_salida):\n",
    "        \"\"\"Genera una comparaci√≥n global entre todos los escenarios\"\"\"\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "        fig.suptitle('Comparaci√≥n Global: Modelos x Escenarios', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx, modelo in enumerate(MODELOS):\n",
    "            row = idx // 3\n",
    "            col = idx % 3\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            datos_plot = []\n",
    "            labels = []\n",
    "            \n",
    "            for escenario in ESCENARIOS:\n",
    "                df_esc = self.df[self.df['Escenario'] == escenario]\n",
    "                datos_plot.append(df_esc[modelo].values)\n",
    "                labels.append(escenario.replace('_', '\\n'))\n",
    "            \n",
    "            bp = ax.boxplot(datos_plot, labels=labels, patch_artist=True)\n",
    "            \n",
    "            # Colorear cajas\n",
    "            colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "            \n",
    "            ax.set_title(modelo, fontweight='bold', fontsize=10)\n",
    "            ax.set_ylabel('Rendimiento', fontsize=9)\n",
    "            ax.tick_params(axis='x', labelsize=7, rotation=0)\n",
    "            ax.tick_params(axis='y', labelsize=8)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'comparacion_global_escenarios.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Crear heatmap global\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        heatmap_data = []\n",
    "        for escenario in ESCENARIOS:\n",
    "            df_esc = self.df[self.df['Escenario'] == escenario]\n",
    "            medias = df_esc[MODELOS].mean().values\n",
    "            heatmap_data.append(medias)\n",
    "        \n",
    "        heatmap_df = pd.DataFrame(heatmap_data, columns=MODELOS, index=ESCENARIOS)\n",
    "        \n",
    "        sns.heatmap(heatmap_df, annot=True, fmt='.3f', cmap='RdYlGn_r', \n",
    "                   ax=ax, cbar_kws={'label': 'Rendimiento Promedio'})\n",
    "        ax.set_title('Heatmap Global: Rendimiento Promedio por Modelo y Escenario', \n",
    "                    fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Escenarios', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_salida / 'heatmap_global.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Generar tabla resumen global\n",
    "        resumen_global = []\n",
    "        for escenario in ESCENARIOS:\n",
    "            df_esc = self.df[self.df['Escenario'] == escenario]\n",
    "            for modelo in MODELOS:\n",
    "                resumen_global.append({\n",
    "                    'Escenario': escenario,\n",
    "                    'Modelo': modelo,\n",
    "                    'Media': df_esc[modelo].mean(),\n",
    "                    'Mediana': df_esc[modelo].median(),\n",
    "                    'Std': df_esc[modelo].std(),\n",
    "                    'Min': df_esc[modelo].min(),\n",
    "                    'Max': df_esc[modelo].max(),\n",
    "                    'CV': (df_esc[modelo].std() / df_esc[modelo].mean() * 100) if df_esc[modelo].mean() != 0 else np.nan\n",
    "                })\n",
    "        \n",
    "        df_resumen_global = pd.DataFrame(resumen_global)\n",
    "        df_resumen_global.to_excel(dir_salida / 'resumen_global_completo.xlsx', index=False)\n",
    "        \n",
    "        # Identificar el mejor modelo por escenario\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üèÜ MEJORES MODELOS POR ESCENARIO\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for escenario in ESCENARIOS:\n",
    "            df_esc_resumen = df_resumen_global[df_resumen_global['Escenario'] == escenario]\n",
    "            mejor = df_esc_resumen.loc[df_esc_resumen['Media'].idxmin()]\n",
    "            print(f\"\\n{escenario}:\")\n",
    "            print(f\"  Mejor modelo: {mejor['Modelo']}\")\n",
    "            print(f\"  Rendimiento promedio: {mejor['Media']:.4f}\")\n",
    "            print(f\"  Desviaci√≥n est√°ndar: {mejor['Std']:.4f}\")\n",
    "            print(f\"  Rango: [{mejor['Min']:.4f}, {mejor['Max']:.4f}]\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîù RANKING GLOBAL DE MODELOS (promediado en todos los escenarios)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        ranking_global = df_resumen_global.groupby('Modelo')['Media'].mean().sort_values()\n",
    "        for rank, (modelo, media) in enumerate(ranking_global.items(), 1):\n",
    "            std_global = df_resumen_global[df_resumen_global['Modelo'] == modelo]['Std'].mean()\n",
    "            print(f\"{rank}. {modelo:20s} - Media: {media:.4f}, Std promedio: {std_global:.4f}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCI√ìN PRINCIPAL DE EJECUCI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal que ejecuta todo el an√°lisis\"\"\"\n",
    "    print(\"\\n\" + \"‚ñà\"*80)\n",
    "    print(\"‚ñà\" + \" \"*78 + \"‚ñà\")\n",
    "    print(\"‚ñà\" + \" \"*20 + \"AN√ÅLISIS DE MODELOS DE PREDICCI√ìN\" + \" \"*26 + \"‚ñà\")\n",
    "    print(\"‚ñà\" + \" \"*78 + \"‚ñà\")\n",
    "    print(\"‚ñà\"*80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear instancia del analizador\n",
    "        analizador = AnalizadorModelos(RUTA_DATOS)\n",
    "        \n",
    "        # Ejecutar an√°lisis completo\n",
    "        analizador.ejecutar_analisis_completo()\n",
    "        \n",
    "        print(\"\\n\" + \"‚ñà\"*80)\n",
    "        print(\"‚ñà\" + \" \"*78 + \"‚ñà\")\n",
    "        print(\"‚ñà\" + \" \"*25 + \"AN√ÅLISIS COMPLETADO EXITOSAMENTE\" + \" \"*22 + \"‚ñà\")\n",
    "        print(\"‚ñà\" + \" \"*78 + \"‚ñà\")\n",
    "        print(\"‚ñà\"*80 + \"\\n\")\n",
    "        \n",
    "        print(\"üìÅ Los resultados se encuentran en:\")\n",
    "        print(\"   ‚îî‚îÄ‚îÄ ./Resultados/\")\n",
    "        print(\"       ‚îú‚îÄ‚îÄ Estacionario_Lineal/\")\n",
    "        print(\"       ‚îÇ   ‚îú‚îÄ‚îÄ LSPM/\")\n",
    "        print(\"       ‚îÇ   ‚îú‚îÄ‚îÄ LSPMW/\")\n",
    "        print(\"       ‚îÇ   ‚îî‚îÄ‚îÄ ... (todos los modelos)\")\n",
    "        print(\"       ‚îú‚îÄ‚îÄ No_Lineal_Estacionario/\")\n",
    "        print(\"       ‚îú‚îÄ‚îÄ No_Estacionario_Lineal/\")\n",
    "        print(\"       ‚îî‚îÄ‚îÄ Comparativo/\")\n",
    "        print(\"           ‚îú‚îÄ‚îÄ comparativo_*.png\")\n",
    "        print(\"           ‚îú‚îÄ‚îÄ ranking_*.xlsx\")\n",
    "        print(\"           ‚îî‚îÄ‚îÄ resumen_global_completo.xlsx\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå ERROR: No se encontr√≥ el archivo {RUTA_DATOS}\")\n",
    "        print(\"   Por favor, verifica que el archivo existe y la ruta es correcta.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR INESPERADO: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
