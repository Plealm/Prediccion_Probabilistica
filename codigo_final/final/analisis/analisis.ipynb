{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b99d120",
   "metadata": {},
   "source": [
    "# Analisis Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b278203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS COMPREHENSIVO DE MODELOS DE PREDICCI√ìN PROBABIL√çSTICA\n",
      "================================================================================\n",
      "\n",
      "Cargando datos...\n",
      "‚úì Estacionario: 1320 filas\n",
      "  Columnas: ['Paso', 'Valores de AR', 'Valores MA', 'Distribuci√≥n', 'Varianza error', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario']\n",
      "‚úì No Estacionario: 840 filas\n",
      "  Columnas: ['Paso', 'Tipo de Modelo', 'Valores de AR', 'Valores MA', 'Distribuci√≥n', 'Varianza error', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario']\n",
      "‚úì No Lineal: 840 filas\n",
      "  Columnas: ['Paso', 'Tipo de Modelo', 'Distribuci√≥n', 'Varianza error', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario']\n",
      "‚úì Tipos de datos convertidos\n",
      "‚úì Filas despu√©s de limpieza: 2600\n",
      "\n",
      "‚úì Datos combinados: 2600 observaciones totales\n",
      "‚úì Columnas finales: ['Paso', 'Valores de AR', 'Valores MA', 'Distribuci√≥n', 'Varianza', 'AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', 'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap', 'Mejor Modelo', 'Escenario', 'Tipo de Modelo']\n",
      "\n",
      "================================================================================\n",
      "INICIANDO AN√ÅLISIS COMPREHENSIVO DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "1. Analizando caracter√≠sticas del proceso generador...\n",
      "  - Analizando efecto de estacionaridad...\n",
      "  - Analizando efecto de no linealidad...\n",
      "  - Analizando efecto del tipo de modelo...\n",
      "\n",
      "2. Analizando efecto de distribuciones...\n",
      "  - Analizando efectos de distribuciones...\n",
      "  - Analizando efectos de varianza...\n",
      "\n",
      "3. Analizando horizonte de predicci√≥n...\n",
      "  - Analizando deterioro por horizonte...\n",
      "  - Analizando consistencia de ranking...\n",
      "\n",
      "4. Analizando interacciones complejas...\n",
      "  - Analizando interacciones Escenario √ó Distribuci√≥n...\n",
      "  - Analizando interacci√≥n triple...\n",
      "\n",
      "5. Analizando robustez y estabilidad...\n",
      "  - Calculando m√©tricas de robustez...\n",
      "  - Identificando peores casos...\n",
      "\n",
      "6. Realizando tests de Diebold-Mariano...\n",
      "  - Realizando tests de Diebold-Mariano...\n",
      "  - Realizando tests pareados de Diebold-Mariano...\n",
      "  - Creando matriz de p-valores...\n",
      "  - Analizando dominancia estad√≠stica...\n",
      "  - Analizando DM por escenario...\n",
      "\n",
      "7. Generando perfiles por modelo...\n",
      "  - Generando perfiles individuales...\n",
      "    > Analizando AREPD...\n",
      "    > Analizando AV-MCPS...\n",
      "    > Analizando Block Bootstrapping...\n",
      "    > Analizando DeepAR...\n",
      "    > Analizando EnCQR-LSTM...\n",
      "    > Analizando LSPM...\n",
      "    > Analizando LSPMW...\n",
      "    > Analizando MCPS...\n",
      "    > Analizando Sieve Bootstrap...\n",
      "\n",
      "8. Generando recomendaciones...\n",
      "  - Generando recomendaciones estrat√©gicas...\n",
      "================================================================================\n",
      "RECOMENDACIONES Y CONCLUSIONES\n",
      "================================================================================\n",
      "\n",
      "1. MODELO CAMPE√ìN GENERAL\n",
      "----------------------------------------\n",
      "Mejor rendimiento promedio: Block Bootstrapping\n",
      "ECRPS: 0.557547\n",
      "Desviaci√≥n Est√°ndar: 0.293355\n",
      "\n",
      "Peor rendimiento promedio: AREPD\n",
      "ECRPS: 3.083010\n",
      "\n",
      "2. RECOMENDACIONES POR ESCENARIO\n",
      "----------------------------------------\n",
      "\n",
      "Estacionario_Lineal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.539612\n",
      "\n",
      "No_Estacionario_Lineal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.542499\n",
      "\n",
      "No_Lineal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.603341\n",
      "\n",
      "3. RECOMENDACIONES POR DISTRIBUCI√ìN DE ERRORES\n",
      "----------------------------------------\n",
      "\n",
      "Distribuci√≥n normal:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.567834\n",
      "\n",
      "Distribuci√≥n uniform:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.585653\n",
      "\n",
      "Distribuci√≥n exponential:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.514920\n",
      "\n",
      "Distribuci√≥n t-student:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.547263\n",
      "\n",
      "Distribuci√≥n mixture:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.572066\n",
      "\n",
      "4. MODELOS M√ÅS ROBUSTOS (MENOR VARIABILIDAD)\n",
      "----------------------------------------\n",
      "1. Block Bootstrapping: CV = 0.5262\n",
      "2. LSPMW: CV = 0.7688\n",
      "3. LSPM: CV = 0.7896\n",
      "\n",
      "5. RECOMENDACIONES POR HORIZONTE DE PREDICCI√ìN\n",
      "----------------------------------------\n",
      "\n",
      "Paso 1.0:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.548020\n",
      "\n",
      "Paso 3.0:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.560285\n",
      "\n",
      "Paso 5.0:\n",
      "  Modelo Recomendado: Block Bootstrapping\n",
      "  ECRPS Promedio: 0.554843\n",
      "\n",
      "6. ESTRATEGIA DE ENSAMBLE SUGERIDA\n",
      "----------------------------------------\n",
      "Combinar los siguientes modelos:\n",
      "1. Block Bootstrapping (ECRPS: 0.557547)\n",
      "2. Sieve Bootstrap (ECRPS: 0.614997)\n",
      "3. LSPM (ECRPS: 0.811114)\n",
      "\n",
      "Justificaci√≥n:\n",
      "  - Estos modelos muestran el mejor rendimiento promedio\n",
      "  - Un ensamble puede capturar fortalezas complementarias\n",
      "  - Reduce el riesgo de seleccionar un modelo sub√≥ptimo\n",
      "\n",
      "7. MODELOS CON DOMINANCIA ESTAD√çSTICA\n",
      "----------------------------------------\n",
      "Modelos estad√≠sticamente superiores (test Diebold-Mariano):\n",
      "1. Block Bootstrapping: 7 victorias significativas\n",
      "2. LSPM: 6 victorias significativas\n",
      "3. LSPMW: 5 victorias significativas\n",
      "4. Sieve Bootstrap: 5 victorias significativas\n",
      "5. DeepAR: 3 victorias significativas\n",
      "\n",
      "8. REGLAS DE DECISI√ìN SUGERIDAS\n",
      "----------------------------------------\n",
      "\n",
      "SI el proceso es ESTACIONARIO y LINEAL:\n",
      "  ‚Üí Primera opci√≥n: Block Bootstrapping\n",
      "  ‚Üí Segunda opci√≥n: Sieve Bootstrap\n",
      "\n",
      "SI el proceso es NO ESTACIONARIO y LINEAL:\n",
      "  ‚Üí Primera opci√≥n: Block Bootstrapping\n",
      "  ‚Üí Segunda opci√≥n: Sieve Bootstrap\n",
      "\n",
      "SI el proceso es NO LINEAL:\n",
      "  ‚Üí Primera opci√≥n: Block Bootstrapping\n",
      "  ‚Üí Segunda opci√≥n: DeepAR\n",
      "\n",
      "SI la distribuci√≥n de errores:\n",
      "  ‚Ä¢ Es normal ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es uniform ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es exponential ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es t-student ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es mixture ‚Üí Usar Block Bootstrapping\n",
      "\n",
      "SI el nivel de varianza:\n",
      "  ‚Ä¢ Es bajo (0.2) ‚Üí Usar Block Bootstrapping\n",
      "  ‚Ä¢ Es alto (3.0) ‚Üí Usar Block Bootstrapping\n",
      "\n",
      "9. CONCLUSIONES PRINCIPALES\n",
      "----------------------------------------\n",
      "\n",
      "‚Ä¢ El modelo Block Bootstrapping muestra el mejor rendimiento general\n",
      "  con ECRPS promedio de 0.557547\n",
      "\n",
      "‚Ä¢ El modelo m√°s robusto (menor CV) es Block Bootstrapping\n",
      "\n",
      "‚Ä¢ Block Bootstrapping es consistentemente superior en procesos\n",
      "  estacionarios y no estacionarios\n",
      "\n",
      "‚Ä¢ Para procesos no lineales: Block Bootstrapping es la mejor opci√≥n\n",
      "\n",
      "‚Ä¢ Se recomienda implementar un ENSAMBLE de los top 3 modelos\n",
      "  para maximizar robustez y rendimiento\n",
      "\n",
      "10. CONSIDERACIONES PR√ÅCTICAS\n",
      "----------------------------------------\n",
      "\n",
      "Factores a considerar en la selecci√≥n:\n",
      "  1. Costo computacional vs ganancia en precisi√≥n\n",
      "  2. Robustez ante cambios en la distribuci√≥n de errores\n",
      "  3. Consistencia a trav√©s de horizontes de predicci√≥n\n",
      "  4. Facilidad de interpretaci√≥n y explicabilidad\n",
      "  5. Disponibilidad de recursos para implementaci√≥n\n",
      "\n",
      "Trade-offs identificados:\n",
      "  ‚Ä¢ Algunos modelos son especialistas en escenarios espec√≠ficos\n",
      "  ‚Ä¢ Otros modelos son generalistas con buen rendimiento global\n",
      "\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS COMPLETO. Resultados guardados en: resultados_analisis_completo/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "‚úì An√°lisis completado exitosamente\n",
      "‚úì Todos los resultados guardados en: resultados_analisis_completo/\n",
      "================================================================================\n",
      "\n",
      "Archivos generados:\n",
      "  üìä An√°lisis de caracter√≠sticas del DGP\n",
      "  üìà Efectos de distribuci√≥n y varianza\n",
      "  üéØ An√°lisis de horizonte de predicci√≥n\n",
      "  üîÑ Interacciones complejas\n",
      "  üí™ M√©tricas de robustez\n",
      "  üìâ Tests de Diebold-Mariano\n",
      "  üë§ Perfiles individuales por modelo\n",
      "  üí° Recomendaciones estrat√©gicas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DieboldMarianoTest:\n",
    "    \"\"\"\n",
    "    Implementaci√≥n del test de Diebold-Mariano para comparar pron√≥sticos\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def dm_test(errors1, errors2, h=1, crit=\"MSE\", power=2):\n",
    "        \"\"\"\n",
    "        Realiza el test de Diebold-Mariano\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        errors1 : array-like\n",
    "            Errores del primer modelo\n",
    "        errors2 : array-like\n",
    "            Errores del segundo modelo\n",
    "        h : int\n",
    "            Horizonte de predicci√≥n (para ajustar autocorrelaci√≥n)\n",
    "        crit : str\n",
    "            Criterio de p√©rdida: \"MSE\", \"MAE\", \"MAPE\"\n",
    "        power : int\n",
    "            Potencia para la funci√≥n de p√©rdida\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dm_stat : float\n",
    "            Estad√≠stico DM\n",
    "        p_value : float\n",
    "            P-valor (two-tailed)\n",
    "        \"\"\"\n",
    "        errors1 = np.array(errors1)\n",
    "        errors2 = np.array(errors2)\n",
    "        \n",
    "        # Calcular diferencias de p√©rdida\n",
    "        if crit == \"MSE\":\n",
    "            loss_diff = errors1**2 - errors2**2\n",
    "        elif crit == \"MAE\":\n",
    "            loss_diff = np.abs(errors1) - np.abs(errors2)\n",
    "        elif crit == \"MAPE\":\n",
    "            loss_diff = np.abs(errors1) - np.abs(errors2)\n",
    "        else:\n",
    "            loss_diff = errors1**power - errors2**power\n",
    "        \n",
    "        # Media de las diferencias\n",
    "        mean_diff = np.mean(loss_diff)\n",
    "        \n",
    "        # Varianza de las diferencias (ajustada por autocorrelaci√≥n)\n",
    "        n = len(loss_diff)\n",
    "        \n",
    "        # Calcular varianza con correcci√≥n de Newey-West\n",
    "        gamma0 = np.var(loss_diff, ddof=1)\n",
    "        \n",
    "        if h > 1:\n",
    "            gamma_sum = 0\n",
    "            for k in range(1, h):\n",
    "                gamma_k = np.cov(loss_diff[:-k], loss_diff[k:])[0, 1]\n",
    "                gamma_sum += (1 - k/h) * gamma_k\n",
    "            variance = (gamma0 + 2 * gamma_sum) / n\n",
    "        else:\n",
    "            variance = gamma0 / n\n",
    "        \n",
    "        # Estad√≠stico DM\n",
    "        dm_stat = mean_diff / np.sqrt(variance) if variance > 0 else 0\n",
    "        \n",
    "        # P-valor (two-tailed)\n",
    "        p_value = 2 * (1 - stats.norm.cdf(np.abs(dm_stat)))\n",
    "        \n",
    "        return dm_stat, p_value\n",
    "\n",
    "\n",
    "class ModelPerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    Clase para an√°lisis exhaustivo de rendimiento de modelos de predicci√≥n\n",
    "    en diferentes escenarios de simulaci√≥n.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el analizador cargando los datos de los tres escenarios.\n",
    "        \"\"\"\n",
    "        self.models = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "                      'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "        \n",
    "        # Cargar datos con las rutas especificadas\n",
    "        print(\"Cargando datos...\")\n",
    "        \n",
    "        try:\n",
    "            self.df_estacionario = pd.read_excel(\"./Datos/estacionario.xlsx\")\n",
    "            self.df_estacionario['Escenario'] = 'Estacionario_Lineal'\n",
    "            print(f\"‚úì Estacionario: {len(self.df_estacionario)} filas\")\n",
    "            print(f\"  Columnas: {self.df_estacionario.columns.tolist()}\")\n",
    "            \n",
    "            self.df_no_estacionario = pd.read_excel(\"./Datos/no_estacionario.xlsx\")\n",
    "            self.df_no_estacionario['Escenario'] = 'No_Estacionario_Lineal'\n",
    "            print(f\"‚úì No Estacionario: {len(self.df_no_estacionario)} filas\")\n",
    "            print(f\"  Columnas: {self.df_no_estacionario.columns.tolist()}\")\n",
    "            \n",
    "            self.df_no_lineal = pd.read_excel(\"./Datos/no_lineal.xlsx\")\n",
    "            self.df_no_lineal['Escenario'] = 'No_Lineal'\n",
    "            print(f\"‚úì No Lineal: {len(self.df_no_lineal)} filas\")\n",
    "            print(f\"  Columnas: {self.df_no_lineal.columns.tolist()}\")\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"ERROR: No se encontr√≥ el archivo - {e}\")\n",
    "            print(\"Verifica que los archivos est√©n en la carpeta './Datos/'\")\n",
    "            raise\n",
    "        \n",
    "        # Estandarizar nombres de columnas\n",
    "        self._standardize_columns()\n",
    "        \n",
    "        # Combinar todos los datos\n",
    "        self.df_all = pd.concat([self.df_estacionario, self.df_no_estacionario, \n",
    "                                 self.df_no_lineal], ignore_index=True)\n",
    "        \n",
    "        # Convertir tipos de datos cr√≠ticos\n",
    "        self._convert_data_types()\n",
    "        \n",
    "        print(f\"\\n‚úì Datos combinados: {len(self.df_all)} observaciones totales\")\n",
    "        print(f\"‚úì Columnas finales: {self.df_all.columns.tolist()}\")\n",
    "        \n",
    "    def _standardize_columns(self):\n",
    "        \"\"\"Estandariza nombres de columnas entre datasets\"\"\"\n",
    "        # Para estacionario\n",
    "        if 'Varianza error' in self.df_estacionario.columns:\n",
    "            self.df_estacionario.rename(columns={'Varianza error': 'Varianza'}, inplace=True)\n",
    "        \n",
    "        # Agregar columna 'Tipo de Modelo' si no existe en estacionario\n",
    "        if 'Tipo de Modelo' not in self.df_estacionario.columns:\n",
    "            # Crear tipo de modelo basado en valores AR y MA\n",
    "            def create_model_type(row):\n",
    "                ar_vals = row.get('Valores de AR', '')\n",
    "                ma_vals = row.get('Valores MA', '')\n",
    "                \n",
    "                ar_str = str(ar_vals) if pd.notna(ar_vals) else ''\n",
    "                ma_str = str(ma_vals) if pd.notna(ma_vals) else ''\n",
    "                \n",
    "                # Contar √≥rdenes\n",
    "                ar_order = len([x for x in ar_str.split(',') if x.strip() and x.strip() != '[]']) if ar_str else 0\n",
    "                ma_order = len([x for x in ma_str.split(',') if x.strip() and x.strip() != '[]']) if ma_str else 0\n",
    "                \n",
    "                if ar_order > 0 and ma_order > 0:\n",
    "                    return f'ARMA({ar_order},{ma_order})'\n",
    "                elif ar_order > 0:\n",
    "                    return f'AR({ar_order})'\n",
    "                elif ma_order > 0:\n",
    "                    return f'MA({ma_order})'\n",
    "                else:\n",
    "                    return 'Unknown'\n",
    "            \n",
    "            self.df_estacionario['Tipo de Modelo'] = self.df_estacionario.apply(create_model_type, axis=1)\n",
    "        \n",
    "        # Para no estacionario\n",
    "        if 'Varianza error' in self.df_no_estacionario.columns:\n",
    "            self.df_no_estacionario.rename(columns={'Varianza error': 'Varianza'}, inplace=True)\n",
    "        \n",
    "        # Para no lineal\n",
    "        if 'Varianza error' in self.df_no_lineal.columns:\n",
    "            self.df_no_lineal.rename(columns={'Varianza error': 'Varianza'}, inplace=True)\n",
    "    \n",
    "    def _convert_data_types(self):\n",
    "        \"\"\"Convierte tipos de datos para evitar errores de comparaci√≥n\"\"\"\n",
    "        # Convertir 'Paso' a num√©rico\n",
    "        self.df_all['Paso'] = pd.to_numeric(self.df_all['Paso'], errors='coerce')\n",
    "        \n",
    "        # Convertir 'Varianza' a num√©rico\n",
    "        self.df_all['Varianza'] = pd.to_numeric(self.df_all['Varianza'], errors='coerce')\n",
    "        \n",
    "        # Convertir columnas de modelos a num√©rico\n",
    "        for model in self.models:\n",
    "            self.df_all[model] = pd.to_numeric(self.df_all[model], errors='coerce')\n",
    "        \n",
    "        # Eliminar filas con valores NaN cr√≠ticos\n",
    "        critical_cols = ['Paso', 'Varianza'] + self.models\n",
    "        self.df_all.dropna(subset=critical_cols, inplace=True)\n",
    "        \n",
    "        print(f\"‚úì Tipos de datos convertidos\")\n",
    "        print(f\"‚úì Filas despu√©s de limpieza: {len(self.df_all)}\")\n",
    "        \n",
    "    def generate_full_report(self, output_dir='resultados_analisis'):\n",
    "        \"\"\"\n",
    "        Genera reporte completo respondiendo a todas las preguntas clave.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"INICIANDO AN√ÅLISIS COMPREHENSIVO DE MODELOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Crear archivo de reporte\n",
    "        report_file = f\"{output_dir}/reporte_completo.txt\"\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"REPORTE COMPLETO DE AN√ÅLISIS DE MODELOS DE PREDICCI√ìN\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # 1. AN√ÅLISIS POR CARACTER√çSTICAS DEL DGP\n",
    "        print(\"\\n1. Analizando caracter√≠sticas del proceso generador...\")\n",
    "        self.analyze_dgp_characteristics(output_dir)\n",
    "        \n",
    "        # 2. AN√ÅLISIS POR DISTRIBUCI√ìN DE ERRORES\n",
    "        print(\"\\n2. Analizando efecto de distribuciones...\")\n",
    "        self.analyze_distribution_effects(output_dir)\n",
    "        \n",
    "        # 3. AN√ÅLISIS POR HORIZONTE DE PREDICCI√ìN\n",
    "        print(\"\\n3. Analizando horizonte de predicci√≥n...\")\n",
    "        self.analyze_horizon_effects(output_dir)\n",
    "        \n",
    "        # 4. AN√ÅLISIS DE INTERACCIONES COMPLEJAS\n",
    "        print(\"\\n4. Analizando interacciones complejas...\")\n",
    "        self.analyze_interactions(output_dir)\n",
    "        \n",
    "        # 5. AN√ÅLISIS DE ROBUSTEZ Y ESTABILIDAD\n",
    "        print(\"\\n5. Analizando robustez y estabilidad...\")\n",
    "        self.analyze_robustness(output_dir)\n",
    "        \n",
    "        # 6. AN√ÅLISIS DE SIGNIFICANCIA ESTAD√çSTICA (DIEBOLD-MARIANO)\n",
    "        print(\"\\n6. Realizando tests de Diebold-Mariano...\")\n",
    "        self.analyze_statistical_significance_dm(output_dir)\n",
    "        \n",
    "        # 7. AN√ÅLISIS POR MODELO INDIVIDUAL\n",
    "        print(\"\\n7. Generando perfiles por modelo...\")\n",
    "        self.analyze_individual_models(output_dir)\n",
    "        \n",
    "        # 8. RECOMENDACIONES Y CONCLUSIONES\n",
    "        print(\"\\n8. Generando recomendaciones...\")\n",
    "        self.generate_recommendations(output_dir)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"AN√ÅLISIS COMPLETO. Resultados guardados en: {output_dir}/\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "    def analyze_dgp_characteristics(self, output_dir):\n",
    "        \"\"\"\n",
    "        1. AN√ÅLISIS DE CARACTER√çSTICAS DEL PROCESO GENERADOR\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # 1.1 Efecto de estacionaridad\n",
    "        print(\"  - Analizando efecto de estacionaridad...\")\n",
    "        for model in self.models:\n",
    "            est_mean = self.df_estacionario[model].mean()\n",
    "            no_est_mean = self.df_no_estacionario[model].mean()\n",
    "            diff = no_est_mean - est_mean\n",
    "            pct_change = (diff / est_mean) * 100 if est_mean != 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'Modelo': model,\n",
    "                'ECRPS_Estacionario': est_mean,\n",
    "                'ECRPS_No_Estacionario': no_est_mean,\n",
    "                'Diferencia': diff,\n",
    "                'Cambio_%': pct_change\n",
    "            })\n",
    "        \n",
    "        df_estacionaridad = pd.DataFrame(results)\n",
    "        df_estacionaridad = df_estacionaridad.sort_values('Cambio_%')\n",
    "        df_estacionaridad.to_csv(f'{output_dir}/1_efecto_estacionaridad.csv', index=False)\n",
    "        \n",
    "        # Visualizaci√≥n\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Gr√°fico de barras comparativas\n",
    "        x = np.arange(len(self.models))\n",
    "        width = 0.35\n",
    "        axes[0].bar(x - width/2, df_estacionaridad['ECRPS_Estacionario'], \n",
    "                   width, label='Estacionario', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, df_estacionaridad['ECRPS_No_Estacionario'], \n",
    "                   width, label='No Estacionario', alpha=0.8)\n",
    "        axes[0].set_xlabel('Modelo')\n",
    "        axes[0].set_ylabel('ECRPS Promedio')\n",
    "        axes[0].set_title('Rendimiento: Estacionario vs No Estacionario')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(df_estacionaridad['Modelo'], rotation=45, ha='right')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gr√°fico de cambio porcentual\n",
    "        colors = ['green' if x < 0 else 'red' for x in df_estacionaridad['Cambio_%']]\n",
    "        axes[1].barh(df_estacionaridad['Modelo'], df_estacionaridad['Cambio_%'], color=colors, alpha=0.7)\n",
    "        axes[1].set_xlabel('Cambio Porcentual (%)')\n",
    "        axes[1].set_title('Impacto de No Estacionaridad\\n(Negativo = Mejor en No Estacionario)')\n",
    "        axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/1_estacionaridad.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 1.2 Efecto de no linealidad\n",
    "        print(\"  - Analizando efecto de no linealidad...\")\n",
    "        results_nl = []\n",
    "        for model in self.models:\n",
    "            lin_mean = self.df_estacionario[model].mean()\n",
    "            nl_mean = self.df_no_lineal[model].mean()\n",
    "            diff = nl_mean - lin_mean\n",
    "            pct_change = (diff / lin_mean) * 100 if lin_mean != 0 else 0\n",
    "            \n",
    "            results_nl.append({\n",
    "                'Modelo': model,\n",
    "                'ECRPS_Lineal': lin_mean,\n",
    "                'ECRPS_No_Lineal': nl_mean,\n",
    "                'Diferencia': diff,\n",
    "                'Cambio_%': pct_change\n",
    "            })\n",
    "        \n",
    "        df_linealidad = pd.DataFrame(results_nl)\n",
    "        df_linealidad = df_linealidad.sort_values('Cambio_%')\n",
    "        df_linealidad.to_csv(f'{output_dir}/1_efecto_no_linealidad.csv', index=False)\n",
    "        \n",
    "        # Visualizaci√≥n no linealidad\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        x = np.arange(len(self.models))\n",
    "        axes[0].bar(x - width/2, df_linealidad['ECRPS_Lineal'], \n",
    "                   width, label='Lineal', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, df_linealidad['ECRPS_No_Lineal'], \n",
    "                   width, label='No Lineal', alpha=0.8)\n",
    "        axes[0].set_xlabel('Modelo')\n",
    "        axes[0].set_ylabel('ECRPS Promedio')\n",
    "        axes[0].set_title('Rendimiento: Lineal vs No Lineal')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(df_linealidad['Modelo'], rotation=45, ha='right')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        colors = ['green' if x < 0 else 'red' for x in df_linealidad['Cambio_%']]\n",
    "        axes[1].barh(df_linealidad['Modelo'], df_linealidad['Cambio_%'], color=colors, alpha=0.7)\n",
    "        axes[1].set_xlabel('Cambio Porcentual (%)')\n",
    "        axes[1].set_title('Impacto de No Linealidad\\n(Negativo = Mejor en No Lineal)')\n",
    "        axes[1].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/1_no_linealidad.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 1.3 An√°lisis por tipo de modelo\n",
    "        print(\"  - Analizando efecto del tipo de modelo...\")\n",
    "        self.analyze_model_type_effect(output_dir)\n",
    "        \n",
    "    def analyze_model_type_effect(self, output_dir):\n",
    "        \"\"\"Analiza el efecto del tipo de modelo en el rendimiento\"\"\"\n",
    "        \n",
    "        # An√°lisis para datos estacionarios\n",
    "        if 'Tipo de Modelo' in self.df_estacionario.columns:\n",
    "            results_type = []\n",
    "            for model in self.models:\n",
    "                for model_type in self.df_estacionario['Tipo de Modelo'].unique():\n",
    "                    subset = self.df_estacionario[self.df_estacionario['Tipo de Modelo'] == model_type]\n",
    "                    if len(subset) > 0:\n",
    "                        results_type.append({\n",
    "                            'Modelo_Predictor': model,\n",
    "                            'Tipo_Proceso': model_type,\n",
    "                            'ECRPS_Mean': subset[model].mean(),\n",
    "                            'ECRPS_Std': subset[model].std(),\n",
    "                            'N_Obs': len(subset)\n",
    "                        })\n",
    "            \n",
    "            df_type = pd.DataFrame(results_type)\n",
    "            df_type.to_csv(f'{output_dir}/1_efecto_tipo_modelo.csv', index=False)\n",
    "            \n",
    "            # Crear heatmap para tipos m√°s comunes\n",
    "            common_types = df_type['Tipo_Proceso'].value_counts().head(10).index\n",
    "            df_type_filtered = df_type[df_type['Tipo_Proceso'].isin(common_types)]\n",
    "            \n",
    "            if len(df_type_filtered) > 0:\n",
    "                pivot = df_type_filtered.pivot_table(\n",
    "                    index='Modelo_Predictor', \n",
    "                    columns='Tipo_Proceso', \n",
    "                    values='ECRPS_Mean'\n",
    "                )\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(14, 8))\n",
    "                sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax, \n",
    "                           cbar_kws={'label': 'ECRPS'})\n",
    "                ax.set_title('Rendimiento por Modelo Predictor y Tipo de Proceso', fontsize=14)\n",
    "                ax.set_xlabel('Tipo de Proceso')\n",
    "                ax.set_ylabel('Modelo Predictor')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/1_heatmap_tipo_modelo.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "    def analyze_distribution_effects(self, output_dir):\n",
    "        \"\"\"\n",
    "        2. AN√ÅLISIS DE EFECTOS DE DISTRIBUCI√ìN\n",
    "        \"\"\"\n",
    "        print(\"  - Analizando efectos de distribuciones...\")\n",
    "        \n",
    "        results_dist = []\n",
    "        for model in self.models:\n",
    "            for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "                if pd.notna(dist):\n",
    "                    subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "                    if len(subset) > 0:\n",
    "                        results_dist.append({\n",
    "                            'Modelo': model,\n",
    "                            'Distribuci√≥n': dist,\n",
    "                            'ECRPS_Mean': subset[model].mean(),\n",
    "                            'ECRPS_Std': subset[model].std(),\n",
    "                            'ECRPS_Min': subset[model].min(),\n",
    "                            'ECRPS_Max': subset[model].max()\n",
    "                        })\n",
    "        \n",
    "        df_dist = pd.DataFrame(results_dist)\n",
    "        df_dist.to_csv(f'{output_dir}/2_efecto_distribucion.csv', index=False)\n",
    "        \n",
    "        # Heatmap\n",
    "        if len(df_dist) > 0:\n",
    "            pivot = df_dist.pivot(index='Modelo', columns='Distribuci√≥n', values='ECRPS_Mean')\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax, cbar_kws={'label': 'ECRPS'})\n",
    "            ax.set_title('Rendimiento por Modelo y Distribuci√≥n', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/2_heatmap_distribucion.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # An√°lisis por varianza\n",
    "        print(\"  - Analizando efectos de varianza...\")\n",
    "        results_var = []\n",
    "        varianzas_unicas = sorted([v for v in self.df_all['Varianza'].unique() if pd.notna(v)])\n",
    "        \n",
    "        for model in self.models:\n",
    "            for var in varianzas_unicas:\n",
    "                subset = self.df_all[self.df_all['Varianza'] == var]\n",
    "                if len(subset) > 0:\n",
    "                    results_var.append({\n",
    "                        'Modelo': model,\n",
    "                        'Varianza': var,\n",
    "                        'ECRPS_Mean': subset[model].mean(),\n",
    "                        'ECRPS_Std': subset[model].std()\n",
    "                    })\n",
    "        \n",
    "        df_var = pd.DataFrame(results_var)\n",
    "        df_var.to_csv(f'{output_dir}/2_efecto_varianza.csv', index=False)\n",
    "        \n",
    "        # Gr√°fico de l√≠neas por varianza\n",
    "        if len(df_var) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            for model in self.models:\n",
    "                data = df_var[df_var['Modelo'] == model].sort_values('Varianza')\n",
    "                if len(data) > 0:\n",
    "                    ax.plot(data['Varianza'], data['ECRPS_Mean'], marker='o', label=model, linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Varianza', fontsize=12)\n",
    "            ax.set_ylabel('ECRPS Promedio', fontsize=12)\n",
    "            ax.set_title('Rendimiento seg√∫n Nivel de Varianza', fontsize=14)\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/2_efecto_varianza.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "    def analyze_horizon_effects(self, output_dir):\n",
    "        \"\"\"\n",
    "        3. AN√ÅLISIS DE HORIZONTE DE PREDICCI√ìN\n",
    "        \"\"\"\n",
    "        print(\"  - Analizando deterioro por horizonte...\")\n",
    "        \n",
    "        results_horizon = []\n",
    "        pasos_unicos = sorted([p for p in self.df_all['Paso'].unique() if pd.notna(p)])\n",
    "        \n",
    "        for model in self.models:\n",
    "            for paso in pasos_unicos:\n",
    "                subset = self.df_all[self.df_all['Paso'] == paso]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    std_val = subset[model].std()\n",
    "                    cv_val = std_val / mean_val if mean_val != 0 and pd.notna(mean_val) else 0\n",
    "                    \n",
    "                    results_horizon.append({\n",
    "                        'Modelo': model,\n",
    "                        'Paso': int(paso),\n",
    "                        'ECRPS_Mean': mean_val,\n",
    "                        'ECRPS_Std': std_val,\n",
    "                        'ECRPS_CV': cv_val\n",
    "                    })\n",
    "        \n",
    "        df_horizon = pd.DataFrame(results_horizon)\n",
    "        df_horizon.to_csv(f'{output_dir}/3_efecto_horizonte.csv', index=False)\n",
    "        \n",
    "        # Gr√°fico de deterioro\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # ECRPS promedio por paso\n",
    "        for model in self.models:\n",
    "            data = df_horizon[df_horizon['Modelo'] == model].sort_values('Paso')\n",
    "            if len(data) > 0:\n",
    "                axes[0].plot(data['Paso'], data['ECRPS_Mean'], marker='o', label=model, linewidth=2)\n",
    "        \n",
    "        axes[0].set_xlabel('Paso de Predicci√≥n', fontsize=12)\n",
    "        axes[0].set_ylabel('ECRPS Promedio', fontsize=12)\n",
    "        axes[0].set_title('Deterioro del Rendimiento por Horizonte', fontsize=14)\n",
    "        axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Tasa de deterioro\n",
    "        deterioro = []\n",
    "        for model in self.models:\n",
    "            data = df_horizon[df_horizon['Modelo'] == model].sort_values('Paso')\n",
    "            if len(data) >= 2:\n",
    "                paso_values = data['Paso'].tolist()\n",
    "                ecrps_paso1 = data.iloc[0]['ECRPS_Mean']\n",
    "                ecrps_paso_final = data.iloc[-1]['ECRPS_Mean']\n",
    "                \n",
    "                if pd.notna(ecrps_paso1) and pd.notna(ecrps_paso_final) and ecrps_paso1 != 0:\n",
    "                    tasa = ((ecrps_paso_final - ecrps_paso1) / ecrps_paso1) * 100\n",
    "                    deterioro.append({'Modelo': model, 'Deterioro_%': tasa})\n",
    "        \n",
    "        if deterioro:\n",
    "            df_deterioro = pd.DataFrame(deterioro).sort_values('Deterioro_%')\n",
    "            colors = ['green' if x < df_deterioro['Deterioro_%'].median() else 'red' \n",
    "                     for x in df_deterioro['Deterioro_%']]\n",
    "            axes[1].barh(df_deterioro['Modelo'], df_deterioro['Deterioro_%'], color=colors, alpha=0.7)\n",
    "            axes[1].set_xlabel(f'Deterioro Paso {pasos_unicos[0]}‚Üí{pasos_unicos[-1]} (%)', fontsize=12)\n",
    "            axes[1].set_title('Tasa de Deterioro por Modelo', fontsize=14)\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/3_horizonte_prediccion.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de consistencia de ranking\n",
    "        print(\"  - Analizando consistencia de ranking...\")\n",
    "        ranking_consistency = []\n",
    "        for paso in pasos_unicos:\n",
    "            subset = self.df_all[self.df_all['Paso'] == paso]\n",
    "            if len(subset) > 0:\n",
    "                ranks = subset[self.models].mean().rank()\n",
    "                rank_dict = ranks.to_dict()\n",
    "                rank_dict['Paso'] = int(paso)\n",
    "                ranking_consistency.append(rank_dict)\n",
    "        \n",
    "        df_ranks = pd.DataFrame(ranking_consistency)\n",
    "        df_ranks.to_csv(f'{output_dir}/3_ranking_por_paso.csv', index=False)\n",
    "        \n",
    "    def analyze_interactions(self, output_dir):\n",
    "        \"\"\"\n",
    "        4. AN√ÅLISIS DE INTERACCIONES COMPLEJAS\n",
    "        \"\"\"\n",
    "        print(\"  - Analizando interacciones Escenario √ó Distribuci√≥n...\")\n",
    "        \n",
    "        results_int = []\n",
    "        for model in self.models:\n",
    "            for escenario in self.df_all['Escenario'].unique():\n",
    "                for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "                    subset = self.df_all[(self.df_all['Escenario'] == escenario) & \n",
    "                                        (self.df_all['Distribuci√≥n'] == dist)]\n",
    "                    if len(subset) > 0:\n",
    "                        results_int.append({\n",
    "                            'Modelo': model,\n",
    "                            'Escenario': escenario,\n",
    "                            'Distribuci√≥n': dist,\n",
    "                            'ECRPS_Mean': subset[model].mean()\n",
    "                        })\n",
    "        \n",
    "        df_int = pd.DataFrame(results_int)\n",
    "        df_int.to_csv(f'{output_dir}/4_interacciones.csv', index=False)\n",
    "        \n",
    "        # Heatmap de interacciones para cada modelo\n",
    "        for model in self.models[:3]:  # Solo primeros 3 por espacio\n",
    "            model_data = df_int[df_int['Modelo'] == model]\n",
    "            if len(model_data) > 0:\n",
    "                pivot = model_data.pivot(\n",
    "                    index='Escenario', columns='Distribuci√≥n', values='ECRPS_Mean')\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=ax)\n",
    "                ax.set_title(f'Interacci√≥n Escenario √ó Distribuci√≥n: {model}', fontsize=12)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/4_interaccion_{model.replace(\" \", \"_\")}.png', \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        # Interacci√≥n triple: Escenario √ó Varianza √ó Paso\n",
    "        print(\"  - Analizando interacci√≥n triple...\")\n",
    "        results_triple = []\n",
    "        \n",
    "        varianzas_unicas = sorted([v for v in self.df_all['Varianza'].unique() if pd.notna(v)])\n",
    "        pasos_unicos = sorted([p for p in self.df_all['Paso'].unique() if pd.notna(p)])\n",
    "        \n",
    "        for model in self.models:\n",
    "            for escenario in self.df_all['Escenario'].unique():\n",
    "                for var in varianzas_unicas:\n",
    "                    for paso in pasos_unicos:\n",
    "                        subset = self.df_all[\n",
    "                            (self.df_all['Escenario'] == escenario) & \n",
    "                            (self.df_all['Varianza'] == var) &\n",
    "                            (self.df_all['Paso'] == paso)\n",
    "                        ]\n",
    "                        if len(subset) > 0:\n",
    "                            results_triple.append({\n",
    "                                'Modelo': model,\n",
    "                                'Escenario': escenario,\n",
    "                                'Varianza': var,\n",
    "                                'Paso': int(paso),\n",
    "                                'ECRPS_Mean': subset[model].mean()\n",
    "                            })\n",
    "        \n",
    "        df_triple = pd.DataFrame(results_triple)\n",
    "        df_triple.to_csv(f'{output_dir}/4_interaccion_triple.csv', index=False)\n",
    "        \n",
    "    def analyze_robustness(self, output_dir):\n",
    "        \"\"\"\n",
    "        5. AN√ÅLISIS DE ROBUSTEZ Y ESTABILIDAD\n",
    "        \"\"\"\n",
    "        print(\"  - Calculando m√©tricas de robustez...\")\n",
    "        \n",
    "        results_robust = []\n",
    "        for model in self.models:\n",
    "            ecrps_values = self.df_all[model]\n",
    "            \n",
    "            results_robust.append({\n",
    "                'Modelo': model,\n",
    "                'ECRPS_Mean': ecrps_values.mean(),\n",
    "                'ECRPS_Std': ecrps_values.std(),\n",
    "                'ECRPS_CV': ecrps_values.std() / ecrps_values.mean() if ecrps_values.mean() != 0 else 0,\n",
    "                'ECRPS_Min': ecrps_values.min(),\n",
    "                'ECRPS_Q25': ecrps_values.quantile(0.25),\n",
    "                'ECRPS_Median': ecrps_values.median(),\n",
    "                'ECRPS_Q75': ecrps_values.quantile(0.75),\n",
    "                'ECRPS_Max': ecrps_values.max(),\n",
    "                'ECRPS_IQR': ecrps_values.quantile(0.75) - ecrps_values.quantile(0.25)\n",
    "            })\n",
    "        \n",
    "        df_robust = pd.DataFrame(results_robust)\n",
    "        df_robust = df_robust.sort_values('ECRPS_CV')\n",
    "        df_robust.to_csv(f'{output_dir}/5_robustez.csv', index=False)\n",
    "        \n",
    "        # Gr√°fico de robustez\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Coeficiente de variaci√≥n\n",
    "        axes[0, 0].barh(df_robust['Modelo'], df_robust['ECRPS_CV'], alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Coeficiente de Variaci√≥n')\n",
    "        axes[0, 0].set_title('Estabilidad (Menor CV = M√°s Estable)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rango intercuart√≠lico\n",
    "        axes[0, 1].barh(df_robust['Modelo'], df_robust['ECRPS_IQR'], alpha=0.7, color='coral')\n",
    "        axes[0, 1].set_xlabel('Rango Intercuart√≠lico')\n",
    "        axes[0, 1].set_title('Variabilidad (Menor IQR = M√°s Consistente)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Boxplot comparativo\n",
    "        data_box = [self.df_all[model] for model in self.models]\n",
    "        bp = axes[1, 0].boxplot(data_box, labels=self.models, patch_artist=True)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "        axes[1, 0].set_ylabel('ECRPS')\n",
    "        axes[1, 0].set_title('Distribuci√≥n de ECRPS por Modelo')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter: Media vs Variabilidad\n",
    "        axes[1, 1].scatter(df_robust['ECRPS_Mean'], df_robust['ECRPS_Std'], \n",
    "                          s=100, alpha=0.6, c=range(len(df_robust)), cmap='viridis')\n",
    "        for idx, row in df_robust.iterrows():\n",
    "            axes[1, 1].annotate(row['Modelo'], \n",
    "                               (row['ECRPS_Mean'], row['ECRPS_Std']),\n",
    "                               fontsize=8, alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('ECRPS Promedio')\n",
    "        axes[1, 1].set_ylabel('Desviaci√≥n Est√°ndar')\n",
    "        axes[1, 1].set_title('Trade-off Rendimiento vs Estabilidad')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/5_robustez.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de peores casos\n",
    "        print(\"  - Identificando peores casos...\")\n",
    "        worst_cases = []\n",
    "        for model in self.models:\n",
    "            df_temp = self.df_all.copy()\n",
    "            df_temp['ECRPS'] = df_temp[model]\n",
    "            worst = df_temp.nlargest(10, 'ECRPS')[\n",
    "                ['Escenario', 'Tipo de Modelo', 'Distribuci√≥n', 'Varianza', 'Paso', 'ECRPS']\n",
    "            ]\n",
    "            worst['Modelo_Predictor'] = model\n",
    "            worst_cases.append(worst)\n",
    "        \n",
    "        df_worst = pd.concat(worst_cases, ignore_index=True)\n",
    "        df_worst.to_csv(f'{output_dir}/5_peores_casos.csv', index=False)\n",
    "        \n",
    "    def analyze_statistical_significance_dm(self, output_dir):\n",
    "        \"\"\"\n",
    "        6. AN√ÅLISIS DE SIGNIFICANCIA ESTAD√çSTICA CON DIEBOLD-MARIANO\n",
    "        \"\"\"\n",
    "        print(\"  - Realizando tests de Diebold-Mariano...\")\n",
    "        \n",
    "        # Test de Friedman por escenario (para comparaci√≥n general)\n",
    "        results_friedman = []\n",
    "        for escenario in self.df_all['Escenario'].unique():\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            data_matrix = subset[self.models].values\n",
    "            \n",
    "            try:\n",
    "                statistic, p_value = friedmanchisquare(*[data_matrix[:, i] for i in range(len(self.models))])\n",
    "                \n",
    "                results_friedman.append({\n",
    "                    'Escenario': escenario,\n",
    "                    'Friedman_Statistic': statistic,\n",
    "                    'P_Value': p_value,\n",
    "                    'Significativo': 'S√≠' if p_value < 0.05 else 'No'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    Advertencia: Error en test de Friedman para {escenario}: {e}\")\n",
    "        \n",
    "        if results_friedman:\n",
    "            df_friedman = pd.DataFrame(results_friedman)\n",
    "            df_friedman.to_csv(f'{output_dir}/6_test_friedman.csv', index=False)\n",
    "        \n",
    "        # Tests de Diebold-Mariano pareados\n",
    "        print(\"  - Realizando tests pareados de Diebold-Mariano...\")\n",
    "        pairs = list(combinations(self.models, 2))\n",
    "        dm_results = []\n",
    "        \n",
    "        for model1, model2 in pairs:\n",
    "            # Calcular errores (usamos ECRPS directamente como m√©trica de p√©rdida)\n",
    "            errors1 = self.df_all[model1].values\n",
    "            errors2 = self.df_all[model2].values\n",
    "            \n",
    "            # Test de Diebold-Mariano\n",
    "            dm_stat, p_value = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "            \n",
    "            mean_diff = self.df_all[model1].mean() - self.df_all[model2].mean()\n",
    "            \n",
    "            # Determinar ganador\n",
    "            if p_value < 0.05:\n",
    "                if mean_diff < 0:\n",
    "                    ganador = model1\n",
    "                else:\n",
    "                    ganador = model2\n",
    "            else:\n",
    "                ganador = 'Empate'\n",
    "            \n",
    "            dm_results.append({\n",
    "                'Modelo_1': model1,\n",
    "                'Modelo_2': model2,\n",
    "                'Diferencia_Media': mean_diff,\n",
    "                'DM_Statistic': dm_stat,\n",
    "                'P_Value': p_value,\n",
    "                'Significativo_0.05': 'S√≠' if p_value < 0.05 else 'No',\n",
    "                'Significativo_0.01': 'S√≠' if p_value < 0.01 else 'No',\n",
    "                'Ganador': ganador\n",
    "            })\n",
    "        \n",
    "        df_dm = pd.DataFrame(dm_results)\n",
    "        df_dm = df_dm.sort_values('P_Value')\n",
    "        df_dm.to_csv(f'{output_dir}/6_tests_diebold_mariano.csv', index=False)\n",
    "        \n",
    "        # Matriz de p-valores (Diebold-Mariano)\n",
    "        print(\"  - Creando matriz de p-valores...\")\n",
    "        p_matrix = np.ones((len(self.models), len(self.models)))\n",
    "        for i, model1 in enumerate(self.models):\n",
    "            for j, model2 in enumerate(self.models):\n",
    "                if i != j:\n",
    "                    errors1 = self.df_all[model1].values\n",
    "                    errors2 = self.df_all[model2].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    p_matrix[i, j] = p_val\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        sns.heatmap(p_matrix, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                   xticklabels=self.models, yticklabels=self.models, \n",
    "                   ax=ax, vmin=0, vmax=0.1, cbar_kws={'label': 'P-valor'})\n",
    "        ax.set_title('Matriz de P-valores (Test de Diebold-Mariano)\\nVerde = Diferencia Significativa', \n",
    "                    fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/6_matriz_pvalores_dm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Dominancia estad√≠stica con Diebold-Mariano\n",
    "        print(\"  - Analizando dominancia estad√≠stica...\")\n",
    "        dominance = []\n",
    "        for model in self.models:\n",
    "            wins = 0\n",
    "            losses = 0\n",
    "            ties = 0\n",
    "            for other_model in self.models:\n",
    "                if model != other_model:\n",
    "                    errors1 = self.df_all[model].values\n",
    "                    errors2 = self.df_all[other_model].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = self.df_all[model].mean() - self.df_all[other_model].mean()\n",
    "                    \n",
    "                    if p_val < 0.05:\n",
    "                        if mean_diff < 0:  # modelo es mejor (menor ECRPS)\n",
    "                            wins += 1\n",
    "                        else:\n",
    "                            losses += 1\n",
    "                    else:\n",
    "                        ties += 1\n",
    "            \n",
    "            dominance.append({\n",
    "                'Modelo': model,\n",
    "                'Victorias_Significativas': wins,\n",
    "                'Derrotas_Significativas': losses,\n",
    "                'Empates': ties,\n",
    "                'Score_Neto': wins - losses\n",
    "            })\n",
    "        \n",
    "        df_dominance = pd.DataFrame(dominance)\n",
    "        df_dominance = df_dominance.sort_values('Score_Neto', ascending=False)\n",
    "        df_dominance.to_csv(f'{output_dir}/6_dominancia_estadistica_dm.csv', index=False)\n",
    "        \n",
    "        # Visualizaci√≥n de dominancia\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        x = np.arange(len(df_dominance))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax.bar(x - width, df_dominance['Victorias_Significativas'], \n",
    "               width, label='Victorias', color='green', alpha=0.7)\n",
    "        ax.bar(x, df_dominance['Empates'], \n",
    "               width, label='Empates', color='gray', alpha=0.7)\n",
    "        ax.bar(x + width, df_dominance['Derrotas_Significativas'], \n",
    "               width, label='Derrotas', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Modelo')\n",
    "        ax.set_ylabel('N√∫mero de Comparaciones')\n",
    "        ax.set_title('Dominancia Estad√≠stica (Test Diebold-Mariano)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df_dominance['Modelo'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/6_dominancia_dm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis de Diebold-Mariano por escenario\n",
    "        print(\"  - Analizando DM por escenario...\")\n",
    "        dm_by_scenario = []\n",
    "        for escenario in self.df_all['Escenario'].unique():\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            \n",
    "            for model1, model2 in combinations(self.models, 2):\n",
    "                errors1 = subset[model1].values\n",
    "                errors2 = subset[model2].values\n",
    "                \n",
    "                if len(errors1) > 0 and len(errors2) > 0:\n",
    "                    dm_stat, p_value = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = subset[model1].mean() - subset[model2].mean()\n",
    "                    \n",
    "                    dm_by_scenario.append({\n",
    "                        'Escenario': escenario,\n",
    "                        'Modelo_1': model1,\n",
    "                        'Modelo_2': model2,\n",
    "                        'DM_Statistic': dm_stat,\n",
    "                        'P_Value': p_value,\n",
    "                        'Diferencia_Media': mean_diff,\n",
    "                        'Significativo': 'S√≠' if p_value < 0.05 else 'No'\n",
    "                    })\n",
    "        \n",
    "        df_dm_scenario = pd.DataFrame(dm_by_scenario)\n",
    "        df_dm_scenario.to_csv(f'{output_dir}/6_dm_por_escenario.csv', index=False)\n",
    "    \n",
    "    def analyze_individual_models(self, output_dir):\n",
    "        \"\"\"\n",
    "        7. PERFILES INDIVIDUALES POR MODELO\n",
    "        \"\"\"\n",
    "        print(\"  - Generando perfiles individuales...\")\n",
    "        \n",
    "        for model in self.models:\n",
    "            print(f\"    > Analizando {model}...\")\n",
    "            \n",
    "            # Crear subdirectorio para el modelo\n",
    "            model_dir = f\"{output_dir}/perfiles_modelos/{model.replace(' ', '_')}\"\n",
    "            import os\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "            # Reporte del modelo\n",
    "            report = []\n",
    "            report.append(f\"=\"*80)\n",
    "            report.append(f\"PERFIL DETALLADO: {model}\")\n",
    "            report.append(f\"=\"*80)\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Estad√≠sticas generales\n",
    "            report.append(\"1. ESTAD√çSTICAS GENERALES\")\n",
    "            report.append(\"-\" * 40)\n",
    "            report.append(f\"ECRPS Promedio Global: {self.df_all[model].mean():.6f}\")\n",
    "            report.append(f\"Desviaci√≥n Est√°ndar: {self.df_all[model].std():.6f}\")\n",
    "            cv = self.df_all[model].std()/self.df_all[model].mean() if self.df_all[model].mean() != 0 else 0\n",
    "            report.append(f\"Coeficiente de Variaci√≥n: {cv:.4f}\")\n",
    "            report.append(f\"M√≠nimo: {self.df_all[model].min():.6f}\")\n",
    "            report.append(f\"Mediana: {self.df_all[model].median():.6f}\")\n",
    "            report.append(f\"M√°ximo: {self.df_all[model].max():.6f}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Ranking general\n",
    "            mean_scores = self.df_all[self.models].mean()\n",
    "            ranking = mean_scores.rank().astype(int)\n",
    "            report.append(f\"Ranking General: {ranking[model]}¬∞ de {len(self.models)}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Mejor escenario\n",
    "            report.append(\"2. MEJORES ESCENARIOS\")\n",
    "            report.append(\"-\" * 40)\n",
    "            best_idx = self.df_all[model].idxmin()\n",
    "            best_row = self.df_all.loc[best_idx]\n",
    "            report.append(f\"Mejor ECRPS: {best_row[model]:.6f}\")\n",
    "            report.append(f\"  - Escenario: {best_row['Escenario']}\")\n",
    "            if 'Tipo de Modelo' in best_row:\n",
    "                report.append(f\"  - Tipo Modelo: {best_row['Tipo de Modelo']}\")\n",
    "            report.append(f\"  - Distribuci√≥n: {best_row['Distribuci√≥n']}\")\n",
    "            report.append(f\"  - Varianza: {best_row['Varianza']}\")\n",
    "            report.append(f\"  - Paso: {best_row['Paso']}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Peor escenario\n",
    "            report.append(\"3. PEORES ESCENARIOS\")\n",
    "            report.append(\"-\" * 40)\n",
    "            worst_idx = self.df_all[model].idxmax()\n",
    "            worst_row = self.df_all.loc[worst_idx]\n",
    "            report.append(f\"Peor ECRPS: {worst_row[model]:.6f}\")\n",
    "            report.append(f\"  - Escenario: {worst_row['Escenario']}\")\n",
    "            if 'Tipo de Modelo' in worst_row:\n",
    "                report.append(f\"  - Tipo Modelo: {worst_row['Tipo de Modelo']}\")\n",
    "            report.append(f\"  - Distribuci√≥n: {worst_row['Distribuci√≥n']}\")\n",
    "            report.append(f\"  - Varianza: {worst_row['Varianza']}\")\n",
    "            report.append(f\"  - Paso: {worst_row['Paso']}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Rendimiento por escenario\n",
    "            report.append(\"4. RENDIMIENTO POR ESCENARIO\")\n",
    "            report.append(\"-\" * 40)\n",
    "            for escenario in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']:\n",
    "                subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    rank = subset[self.models].mean().rank()[model]\n",
    "                    report.append(f\"{escenario}:\")\n",
    "                    report.append(f\"  ECRPS: {mean_val:.6f} (Ranking: {int(rank)}¬∞)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Fortalezas y debilidades\n",
    "            report.append(\"5. FORTALEZAS Y DEBILIDADES\")\n",
    "            report.append(\"-\" * 40)\n",
    "            \n",
    "            # Por distribuci√≥n\n",
    "            report.append(\"Por Distribuci√≥n:\")\n",
    "            dist_performance = []\n",
    "            for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "                subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    rank = subset[self.models].mean().rank()[model]\n",
    "                    dist_performance.append((dist, mean_val, rank))\n",
    "            \n",
    "            if dist_performance:\n",
    "                dist_performance.sort(key=lambda x: x[2])\n",
    "                report.append(f\"  Mejor: {dist_performance[0][0]} (Ranking {int(dist_performance[0][2])}¬∞)\")\n",
    "                report.append(f\"  Peor: {dist_performance[-1][0]} (Ranking {int(dist_performance[-1][2])}¬∞)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Por varianza\n",
    "            report.append(\"Por Varianza:\")\n",
    "            var_performance = []\n",
    "            for var in sorted(self.df_all['Varianza'].unique()):\n",
    "                subset = self.df_all[self.df_all['Varianza'] == var]\n",
    "                if len(subset) > 0:\n",
    "                    mean_val = subset[model].mean()\n",
    "                    rank = subset[self.models].mean().rank()[model]\n",
    "                    var_performance.append((var, mean_val, rank))\n",
    "            \n",
    "            if var_performance:\n",
    "                var_performance.sort(key=lambda x: x[2])\n",
    "                report.append(f\"  Mejor: Varianza {var_performance[0][0]} (Ranking {int(var_performance[0][2])}¬∞)\")\n",
    "                report.append(f\"  Peor: Varianza {var_performance[-1][0]} (Ranking {int(var_performance[-1][2])}¬∞)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Comparaciones con Diebold-Mariano\n",
    "            report.append(\"6. COMPARACIONES ESTAD√çSTICAS (DIEBOLD-MARIANO)\")\n",
    "            report.append(\"-\" * 40)\n",
    "            \n",
    "            wins = 0\n",
    "            losses = 0\n",
    "            for other_model in self.models:\n",
    "                if model != other_model:\n",
    "                    errors1 = self.df_all[model].values\n",
    "                    errors2 = self.df_all[other_model].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = self.df_all[model].mean() - self.df_all[other_model].mean()\n",
    "                    \n",
    "                    if p_val < 0.05:\n",
    "                        if mean_diff < 0:\n",
    "                            wins += 1\n",
    "                        else:\n",
    "                            losses += 1\n",
    "            \n",
    "            report.append(f\"Victorias significativas: {wins}\")\n",
    "            report.append(f\"Derrotas significativas: {losses}\")\n",
    "            report.append(f\"Score neto: {wins - losses}\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Guardar reporte\n",
    "            with open(f\"{model_dir}/perfil_{model.replace(' ', '_')}.txt\", 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(report))\n",
    "            \n",
    "            # Visualizaciones del modelo\n",
    "            self._create_model_visualizations(model, model_dir)\n",
    "    \n",
    "    def _create_model_visualizations(self, model, model_dir):\n",
    "        \"\"\"Crea visualizaciones espec√≠ficas para un modelo\"\"\"\n",
    "        \n",
    "        # 1. Distribuci√≥n de ECRPS\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Histograma\n",
    "        axes[0, 0].hist(self.df_all[model], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "        axes[0, 0].axvline(self.df_all[model].mean(), color='red', linestyle='--', \n",
    "                          linewidth=2, label=f'Media: {self.df_all[model].mean():.4f}')\n",
    "        axes[0, 0].axvline(self.df_all[model].median(), color='green', linestyle='--', \n",
    "                          linewidth=2, label=f'Mediana: {self.df_all[model].median():.4f}')\n",
    "        axes[0, 0].set_xlabel('ECRPS')\n",
    "        axes[0, 0].set_ylabel('Frecuencia')\n",
    "        axes[0, 0].set_title(f'Distribuci√≥n de ECRPS - {model}')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Boxplot por escenario\n",
    "        data_by_scenario = [self.df_all[self.df_all['Escenario'] == esc][model] \n",
    "                           for esc in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']]\n",
    "        bp = axes[0, 1].boxplot(data_by_scenario, labels=['Est. Lin.', 'No Est. Lin.', 'No Lin.'], \n",
    "                               patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
    "            patch.set_facecolor(color)\n",
    "        axes[0, 1].set_ylabel('ECRPS')\n",
    "        axes[0, 1].set_title(f'ECRPS por Escenario - {model}')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rendimiento por paso\n",
    "        paso_data = []\n",
    "        for p in sorted(self.df_all['Paso'].unique()):\n",
    "            subset = self.df_all[self.df_all['Paso'] == p]\n",
    "            if len(subset) > 0:\n",
    "                paso_data.append((p, subset[model].mean()))\n",
    "        \n",
    "        if paso_data:\n",
    "            pasos, means = zip(*paso_data)\n",
    "            axes[1, 0].plot(pasos, means, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "            axes[1, 0].set_xlabel('Paso de Predicci√≥n')\n",
    "            axes[1, 0].set_ylabel('ECRPS Promedio')\n",
    "            axes[1, 0].set_title(f'Rendimiento por Horizonte - {model}')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Heatmap: Distribuci√≥n √ó Varianza\n",
    "        pivot_data = []\n",
    "        dist_labels = []\n",
    "        var_labels = sorted(self.df_all['Varianza'].unique())\n",
    "        \n",
    "        for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "            row = []\n",
    "            for var in var_labels:\n",
    "                subset = self.df_all[(self.df_all['Distribuci√≥n'] == dist) & \n",
    "                                    (self.df_all['Varianza'] == var)]\n",
    "                if len(subset) > 0:\n",
    "                    row.append(subset[model].mean())\n",
    "                else:\n",
    "                    row.append(np.nan)\n",
    "            if not all(np.isnan(row)):\n",
    "                pivot_data.append(row)\n",
    "                dist_labels.append(dist)\n",
    "        \n",
    "        if pivot_data:\n",
    "            pivot_df = pd.DataFrame(pivot_data, index=dist_labels, columns=var_labels)\n",
    "            \n",
    "            sns.heatmap(pivot_df, annot=True, fmt='.4f', cmap='RdYlGn_r', ax=axes[1, 1],\n",
    "                       cbar_kws={'label': 'ECRPS'})\n",
    "            axes[1, 1].set_title(f'ECRPS: Distribuci√≥n √ó Varianza - {model}')\n",
    "            axes[1, 1].set_xlabel('Varianza')\n",
    "            axes[1, 1].set_ylabel('Distribuci√≥n')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/visualizaciones_{model.replace(\" \", \"_\")}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Comparaci√≥n con otros modelos\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        means = self.df_all[self.models].mean().sort_values()\n",
    "        colors = ['red' if m == model else 'steelblue' for m in means.index]\n",
    "        bars = ax.barh(means.index, means.values, color=colors, alpha=0.7)\n",
    "        \n",
    "        # Destacar el modelo actual\n",
    "        for i, bar in enumerate(bars):\n",
    "            if means.index[i] == model:\n",
    "                bar.set_edgecolor('black')\n",
    "                bar.set_linewidth(3)\n",
    "        \n",
    "        ax.set_xlabel('ECRPS Promedio')\n",
    "        ax.set_title(f'Comparaci√≥n Global - {model} (Destacado en Rojo)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/comparacion_{model.replace(\" \", \"_\")}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_recommendations(self, output_dir):\n",
    "        \"\"\"\n",
    "        8. GENERACI√ìN DE RECOMENDACIONES\n",
    "        \"\"\"\n",
    "        print(\"  - Generando recomendaciones estrat√©gicas...\")\n",
    "        \n",
    "        recommendations = []\n",
    "        recommendations.append(\"=\"*80)\n",
    "        recommendations.append(\"RECOMENDACIONES Y CONCLUSIONES\")\n",
    "        recommendations.append(\"=\"*80)\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 1. Modelo campe√≥n general\n",
    "        overall_best = self.df_all[self.models].mean().idxmin()\n",
    "        overall_worst = self.df_all[self.models].mean().idxmax()\n",
    "        \n",
    "        recommendations.append(\"1. MODELO CAMPE√ìN GENERAL\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(f\"Mejor rendimiento promedio: {overall_best}\")\n",
    "        recommendations.append(f\"ECRPS: {self.df_all[overall_best].mean():.6f}\")\n",
    "        recommendations.append(f\"Desviaci√≥n Est√°ndar: {self.df_all[overall_best].std():.6f}\")\n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(f\"Peor rendimiento promedio: {overall_worst}\")\n",
    "        recommendations.append(f\"ECRPS: {self.df_all[overall_worst].mean():.6f}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 2. Modelos por escenario\n",
    "        recommendations.append(\"2. RECOMENDACIONES POR ESCENARIO\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        for escenario in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']:\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            if len(subset) > 0:\n",
    "                best_model = subset[self.models].mean().idxmin()\n",
    "                best_score = subset[best_model].mean()\n",
    "                \n",
    "                recommendations.append(f\"\\n{escenario}:\")\n",
    "                recommendations.append(f\"  Modelo Recomendado: {best_model}\")\n",
    "                recommendations.append(f\"  ECRPS Promedio: {best_score:.6f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 3. Modelos por distribuci√≥n\n",
    "        recommendations.append(\"3. RECOMENDACIONES POR DISTRIBUCI√ìN DE ERRORES\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "            subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "            if len(subset) > 0:\n",
    "                best_model = subset[self.models].mean().idxmin()\n",
    "                best_score = subset[best_model].mean()\n",
    "                \n",
    "                recommendations.append(f\"\\nDistribuci√≥n {dist}:\")\n",
    "                recommendations.append(f\"  Modelo Recomendado: {best_model}\")\n",
    "                recommendations.append(f\"  ECRPS Promedio: {best_score:.6f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 4. Modelos m√°s robustos\n",
    "        recommendations.append(\"4. MODELOS M√ÅS ROBUSTOS (MENOR VARIABILIDAD)\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        cv_scores = {model: self.df_all[model].std() / self.df_all[model].mean() \n",
    "                    for model in self.models if self.df_all[model].mean() != 0}\n",
    "        cv_sorted = sorted(cv_scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "        for i, (model, cv) in enumerate(cv_sorted[:3], 1):\n",
    "            recommendations.append(f\"{i}. {model}: CV = {cv:.4f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 5. Modelos por horizonte\n",
    "        recommendations.append(\"5. RECOMENDACIONES POR HORIZONTE DE PREDICCI√ìN\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        pasos_unicos = sorted(self.df_all['Paso'].unique())\n",
    "        for paso in [pasos_unicos[0], pasos_unicos[len(pasos_unicos)//2], pasos_unicos[-1]]:\n",
    "            subset = self.df_all[self.df_all['Paso'] == paso]\n",
    "            if len(subset) > 0:\n",
    "                best_model = subset[self.models].mean().idxmin()\n",
    "                best_score = subset[best_model].mean()\n",
    "                \n",
    "                recommendations.append(f\"\\nPaso {paso}:\")\n",
    "                recommendations.append(f\"  Modelo Recomendado: {best_model}\")\n",
    "                recommendations.append(f\"  ECRPS Promedio: {best_score:.6f}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 6. Estrategia de ensamble\n",
    "        recommendations.append(\"6. ESTRATEGIA DE ENSAMBLE SUGERIDA\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        # Top 3 modelos complementarios\n",
    "        top3 = self.df_all[self.models].mean().nsmallest(3)\n",
    "        recommendations.append(\"Combinar los siguientes modelos:\")\n",
    "        for i, (model, score) in enumerate(top3.items(), 1):\n",
    "            recommendations.append(f\"{i}. {model} (ECRPS: {score:.6f})\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(\"Justificaci√≥n:\")\n",
    "        recommendations.append(\"  - Estos modelos muestran el mejor rendimiento promedio\")\n",
    "        recommendations.append(\"  - Un ensamble puede capturar fortalezas complementarias\")\n",
    "        recommendations.append(\"  - Reduce el riesgo de seleccionar un modelo sub√≥ptimo\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 7. Modelos con dominancia estad√≠stica\n",
    "        recommendations.append(\"7. MODELOS CON DOMINANCIA ESTAD√çSTICA\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        \n",
    "        dominance_scores = []\n",
    "        for model in self.models:\n",
    "            wins = 0\n",
    "            for other_model in self.models:\n",
    "                if model != other_model:\n",
    "                    errors1 = self.df_all[model].values\n",
    "                    errors2 = self.df_all[other_model].values\n",
    "                    _, p_val = DieboldMarianoTest.dm_test(errors1, errors2, h=1, crit=\"MSE\")\n",
    "                    mean_diff = self.df_all[model].mean() - self.df_all[other_model].mean()\n",
    "                    \n",
    "                    if p_val < 0.05 and mean_diff < 0:\n",
    "                        wins += 1\n",
    "            \n",
    "            dominance_scores.append((model, wins))\n",
    "        \n",
    "        dominance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        recommendations.append(\"Modelos estad√≠sticamente superiores (test Diebold-Mariano):\")\n",
    "        for i, (model, wins) in enumerate(dominance_scores[:5], 1):\n",
    "            recommendations.append(f\"{i}. {model}: {wins} victorias significativas\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 8. Reglas de decisi√≥n\n",
    "        recommendations.append(\"8. REGLAS DE DECISI√ìN SUGERIDAS\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Reglas por escenario\n",
    "        for escenario in ['Estacionario_Lineal', 'No_Estacionario_Lineal', 'No_Lineal']:\n",
    "            subset = self.df_all[self.df_all['Escenario'] == escenario]\n",
    "            if len(subset) > 0:\n",
    "                top2 = subset[self.models].mean().nsmallest(2)\n",
    "                \n",
    "                if escenario == 'Estacionario_Lineal':\n",
    "                    recommendations.append(\"SI el proceso es ESTACIONARIO y LINEAL:\")\n",
    "                elif escenario == 'No_Estacionario_Lineal':\n",
    "                    recommendations.append(\"SI el proceso es NO ESTACIONARIO y LINEAL:\")\n",
    "                else:\n",
    "                    recommendations.append(\"SI el proceso es NO LINEAL:\")\n",
    "                \n",
    "                recommendations.append(f\"  ‚Üí Primera opci√≥n: {top2.index[0]}\")\n",
    "                recommendations.append(f\"  ‚Üí Segunda opci√≥n: {top2.index[1]}\")\n",
    "                recommendations.append(\"\")\n",
    "        \n",
    "        # Reglas por distribuci√≥n\n",
    "        recommendations.append(\"SI la distribuci√≥n de errores:\")\n",
    "        for dist in self.df_all['Distribuci√≥n'].unique():\n",
    "            subset = self.df_all[self.df_all['Distribuci√≥n'] == dist]\n",
    "            if len(subset) > 0:\n",
    "                best = subset[self.models].mean().idxmin()\n",
    "                recommendations.append(f\"  ‚Ä¢ Es {dist} ‚Üí Usar {best}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Reglas por varianza\n",
    "        recommendations.append(\"SI el nivel de varianza:\")\n",
    "        variances = sorted(self.df_all['Varianza'].unique())\n",
    "        if len(variances) >= 2:\n",
    "            low_var = variances[0]\n",
    "            high_var = variances[-1]\n",
    "            \n",
    "            subset_low = self.df_all[self.df_all['Varianza'] == low_var]\n",
    "            subset_high = self.df_all[self.df_all['Varianza'] == high_var]\n",
    "            \n",
    "            best_low = subset_low[self.models].mean().idxmin()\n",
    "            best_high = subset_high[self.models].mean().idxmin()\n",
    "            \n",
    "            recommendations.append(f\"  ‚Ä¢ Es bajo ({low_var}) ‚Üí Usar {best_low}\")\n",
    "            recommendations.append(f\"  ‚Ä¢ Es alto ({high_var}) ‚Üí Usar {best_high}\")\n",
    "        \n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # 9. Conclusiones finales\n",
    "        recommendations.append(\"9. CONCLUSIONES PRINCIPALES\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(f\"‚Ä¢ El modelo {overall_best} muestra el mejor rendimiento general\")\n",
    "        recommendations.append(f\"  con ECRPS promedio de {self.df_all[overall_best].mean():.6f}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # An√°lisis de robustez\n",
    "        most_robust = min(cv_scores.items(), key=lambda x: x[1])[0]\n",
    "        recommendations.append(f\"‚Ä¢ El modelo m√°s robusto (menor CV) es {most_robust}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Comparaci√≥n estacionario vs no estacionario\n",
    "        est_best = self.df_estacionario[self.models].mean().idxmin()\n",
    "        no_est_best = self.df_no_estacionario[self.models].mean().idxmin()\n",
    "        \n",
    "        if est_best == no_est_best:\n",
    "            recommendations.append(f\"‚Ä¢ {est_best} es consistentemente superior en procesos\")\n",
    "            recommendations.append(\"  estacionarios y no estacionarios\")\n",
    "        else:\n",
    "            recommendations.append(f\"‚Ä¢ Para procesos estacionarios: preferir {est_best}\")\n",
    "            recommendations.append(f\"‚Ä¢ Para procesos no estacionarios: preferir {no_est_best}\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # An√°lisis de no linealidad\n",
    "        nl_best = self.df_no_lineal[self.models].mean().idxmin()\n",
    "        recommendations.append(f\"‚Ä¢ Para procesos no lineales: {nl_best} es la mejor opci√≥n\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Recomendaci√≥n de ensamble\n",
    "        recommendations.append(\"‚Ä¢ Se recomienda implementar un ENSAMBLE de los top 3 modelos\")\n",
    "        recommendations.append(\"  para maximizar robustez y rendimiento\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Consideraciones pr√°cticas\n",
    "        recommendations.append(\"10. CONSIDERACIONES PR√ÅCTICAS\")\n",
    "        recommendations.append(\"-\" * 40)\n",
    "        recommendations.append(\"\")\n",
    "        recommendations.append(\"Factores a considerar en la selecci√≥n:\")\n",
    "        recommendations.append(\"  1. Costo computacional vs ganancia en precisi√≥n\")\n",
    "        recommendations.append(\"  2. Robustez ante cambios en la distribuci√≥n de errores\")\n",
    "        recommendations.append(\"  3. Consistencia a trav√©s de horizontes de predicci√≥n\")\n",
    "        recommendations.append(\"  4. Facilidad de interpretaci√≥n y explicabilidad\")\n",
    "        recommendations.append(\"  5. Disponibilidad de recursos para implementaci√≥n\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Trade-offs identificados\n",
    "        recommendations.append(\"Trade-offs identificados:\")\n",
    "        \n",
    "        # Mejor vs m√°s robusto\n",
    "        if overall_best != most_robust:\n",
    "            recommendations.append(f\"  ‚Ä¢ Rendimiento vs Robustez: {overall_best} (mejor) vs {most_robust} (m√°s robusto)\")\n",
    "        \n",
    "        # Modelos especializados\n",
    "        recommendations.append(\"  ‚Ä¢ Algunos modelos son especialistas en escenarios espec√≠ficos\")\n",
    "        recommendations.append(\"  ‚Ä¢ Otros modelos son generalistas con buen rendimiento global\")\n",
    "        recommendations.append(\"\")\n",
    "        \n",
    "        # Guardar recomendaciones\n",
    "        with open(f'{output_dir}/8_recomendaciones.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(recommendations))\n",
    "        \n",
    "        print('\\n'.join(recommendations))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# C√ìDIGO DE EJECUCI√ìN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para ejecutar el an√°lisis completo\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AN√ÅLISIS COMPREHENSIVO DE MODELOS DE PREDICCI√ìN PROBABIL√çSTICA\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Crear analizador\n",
    "    try:\n",
    "        analyzer = ModelPerformanceAnalyzer()\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nERROR: No se encontraron los archivos de datos\")\n",
    "        print(\"Verifica que existan los siguientes archivos:\")\n",
    "        print(\"  - ./Datos/estacionario.xlsx\")\n",
    "        print(\"  - ./Datos/no_estacionario.xlsx\")\n",
    "        print(\"  - ./Datos/no_lineal.xlsx\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR al cargar datos: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Ejecutar an√°lisis completo\n",
    "    output_directory = 'resultados_analisis_completo'\n",
    "    \n",
    "    try:\n",
    "        analyzer.generate_full_report(output_dir=output_directory)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úì An√°lisis completado exitosamente\")\n",
    "        print(f\"‚úì Todos los resultados guardados en: {output_directory}/\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        print(\"Archivos generados:\")\n",
    "        print(\"  üìä An√°lisis de caracter√≠sticas del DGP\")\n",
    "        print(\"  üìà Efectos de distribuci√≥n y varianza\")\n",
    "        print(\"  üéØ An√°lisis de horizonte de predicci√≥n\")\n",
    "        print(\"  üîÑ Interacciones complejas\")\n",
    "        print(\"  üí™ M√©tricas de robustez\")\n",
    "        print(\"  üìâ Tests de Diebold-Mariano\")\n",
    "        print(\"  üë§ Perfiles individuales por modelo\")\n",
    "        print(\"  üí° Recomendaciones estrat√©gicas\")\n",
    "        print(\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR durante el an√°lisis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1784b",
   "metadata": {},
   "source": [
    "# Pre analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f559ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores √∫nicos encontrados en 'Tipo de Modelo':\n",
      "['AR(1)' 'AR(2)' 'MA(1)' 'MA(2)' 'ARMA(1,1)' 'ARMA(2,2)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paso</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Distribuci√≥n</th>\n",
       "      <th>Varianza error</th>\n",
       "      <th>AREPD</th>\n",
       "      <th>AV-MCPS</th>\n",
       "      <th>Block Bootstrapping</th>\n",
       "      <th>DeepAR</th>\n",
       "      <th>EnCQR-LSTM</th>\n",
       "      <th>LSPM</th>\n",
       "      <th>LSPMW</th>\n",
       "      <th>MCPS</th>\n",
       "      <th>Sieve Bootstrap</th>\n",
       "      <th>Mejor Modelo</th>\n",
       "      <th>Escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.294667</td>\n",
       "      <td>0.355344</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.263419</td>\n",
       "      <td>0.306622</td>\n",
       "      <td>0.440706</td>\n",
       "      <td>0.431452</td>\n",
       "      <td>0.285427</td>\n",
       "      <td>0.248691</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.604540</td>\n",
       "      <td>0.307449</td>\n",
       "      <td>0.254264</td>\n",
       "      <td>0.273001</td>\n",
       "      <td>0.565522</td>\n",
       "      <td>0.470424</td>\n",
       "      <td>0.474111</td>\n",
       "      <td>0.285430</td>\n",
       "      <td>0.254193</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.273622</td>\n",
       "      <td>0.276230</td>\n",
       "      <td>0.258388</td>\n",
       "      <td>0.315765</td>\n",
       "      <td>0.269452</td>\n",
       "      <td>0.520070</td>\n",
       "      <td>0.517876</td>\n",
       "      <td>0.337990</td>\n",
       "      <td>0.258039</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.261423</td>\n",
       "      <td>0.279697</td>\n",
       "      <td>0.254453</td>\n",
       "      <td>0.289443</td>\n",
       "      <td>0.269285</td>\n",
       "      <td>0.287989</td>\n",
       "      <td>0.288111</td>\n",
       "      <td>0.282999</td>\n",
       "      <td>0.254655</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>AR(1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.626252</td>\n",
       "      <td>0.273680</td>\n",
       "      <td>0.254842</td>\n",
       "      <td>0.272827</td>\n",
       "      <td>0.639437</td>\n",
       "      <td>0.763960</td>\n",
       "      <td>0.753066</td>\n",
       "      <td>0.308347</td>\n",
       "      <td>0.254952</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.082513</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>0.953857</td>\n",
       "      <td>1.116455</td>\n",
       "      <td>1.053269</td>\n",
       "      <td>2.030504</td>\n",
       "      <td>2.165650</td>\n",
       "      <td>0.990087</td>\n",
       "      <td>0.954156</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>2</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.903173</td>\n",
       "      <td>0.971148</td>\n",
       "      <td>0.954440</td>\n",
       "      <td>1.005615</td>\n",
       "      <td>1.518301</td>\n",
       "      <td>1.431610</td>\n",
       "      <td>1.522051</td>\n",
       "      <td>1.141614</td>\n",
       "      <td>0.954065</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>3</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.310542</td>\n",
       "      <td>1.021845</td>\n",
       "      <td>0.976235</td>\n",
       "      <td>1.002865</td>\n",
       "      <td>1.615073</td>\n",
       "      <td>1.026140</td>\n",
       "      <td>1.036051</td>\n",
       "      <td>1.484601</td>\n",
       "      <td>0.962417</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>4</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.324103</td>\n",
       "      <td>0.968827</td>\n",
       "      <td>0.961514</td>\n",
       "      <td>0.977739</td>\n",
       "      <td>1.072897</td>\n",
       "      <td>1.453428</td>\n",
       "      <td>1.530595</td>\n",
       "      <td>1.125230</td>\n",
       "      <td>0.960919</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>5</td>\n",
       "      <td>ARMA(2,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.522689</td>\n",
       "      <td>1.011090</td>\n",
       "      <td>0.960863</td>\n",
       "      <td>1.001003</td>\n",
       "      <td>1.123328</td>\n",
       "      <td>1.061991</td>\n",
       "      <td>1.038016</td>\n",
       "      <td>1.179879</td>\n",
       "      <td>0.962942</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Paso Tipo de Modelo Distribuci√≥n  Varianza error     AREPD   AV-MCPS  \\\n",
       "0       1          AR(1)       normal             0.2  0.294667  0.355344   \n",
       "2       2          AR(1)       normal             0.2  0.604540  0.307449   \n",
       "4       3          AR(1)       normal             0.2  0.273622  0.276230   \n",
       "6       4          AR(1)       normal             0.2  0.261423  0.279697   \n",
       "8       5          AR(1)       normal             0.2  0.626252  0.273680   \n",
       "...   ...            ...          ...             ...       ...       ...   \n",
       "1309    1      ARMA(2,2)      mixture             3.0  1.082513  0.999066   \n",
       "1311    2      ARMA(2,2)      mixture             3.0  1.903173  0.971148   \n",
       "1313    3      ARMA(2,2)      mixture             3.0  2.310542  1.021845   \n",
       "1315    4      ARMA(2,2)      mixture             3.0  1.324103  0.968827   \n",
       "1317    5      ARMA(2,2)      mixture             3.0  1.522689  1.011090   \n",
       "\n",
       "      Block Bootstrapping    DeepAR  EnCQR-LSTM      LSPM     LSPMW      MCPS  \\\n",
       "0                0.248447  0.263419    0.306622  0.440706  0.431452  0.285427   \n",
       "2                0.254264  0.273001    0.565522  0.470424  0.474111  0.285430   \n",
       "4                0.258388  0.315765    0.269452  0.520070  0.517876  0.337990   \n",
       "6                0.254453  0.289443    0.269285  0.287989  0.288111  0.282999   \n",
       "8                0.254842  0.272827    0.639437  0.763960  0.753066  0.308347   \n",
       "...                   ...       ...         ...       ...       ...       ...   \n",
       "1309             0.953857  1.116455    1.053269  2.030504  2.165650  0.990087   \n",
       "1311             0.954440  1.005615    1.518301  1.431610  1.522051  1.141614   \n",
       "1313             0.976235  1.002865    1.615073  1.026140  1.036051  1.484601   \n",
       "1315             0.961514  0.977739    1.072897  1.453428  1.530595  1.125230   \n",
       "1317             0.960863  1.001003    1.123328  1.061991  1.038016  1.179879   \n",
       "\n",
       "      Sieve Bootstrap         Mejor Modelo            Escenario  \n",
       "0            0.248691  Block Bootstrapping  Estacionario_Lineal  \n",
       "2            0.254193      Sieve Bootstrap  Estacionario_Lineal  \n",
       "4            0.258039      Sieve Bootstrap  Estacionario_Lineal  \n",
       "6            0.254655  Block Bootstrapping  Estacionario_Lineal  \n",
       "8            0.254952  Block Bootstrapping  Estacionario_Lineal  \n",
       "...               ...                  ...                  ...  \n",
       "1309         0.954156  Block Bootstrapping  Estacionario_Lineal  \n",
       "1311         0.954065      Sieve Bootstrap  Estacionario_Lineal  \n",
       "1313         0.962417      Sieve Bootstrap  Estacionario_Lineal  \n",
       "1315         0.960919      Sieve Bootstrap  Estacionario_Lineal  \n",
       "1317         0.962942  Block Bootstrapping  Estacionario_Lineal  \n",
       "\n",
       "[600 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "    \n",
    "estacionario = pd.read_excel(\"./Datos/estacionario.xlsx\")\n",
    "\n",
    "estacionario = estacionario.drop_duplicates()\n",
    "estacionario = estacionario[estacionario[\"Paso\"] != \"Promedio\"]\n",
    "\n",
    "def determinar_tipo_modelo_mejorado(row):\n",
    "    \"\"\"\n",
    "    Determina el tipo de modelo (AR, MA, ARMA) y su orden a partir de los valores\n",
    "    en las columnas 'Valores de AR' y 'Valores MA'.\n",
    "    \"\"\"\n",
    "    ar_str = str(row['Valores de AR'])\n",
    "    ma_str = str(row['Valores MA'])\n",
    "    \n",
    "    # Expresi√≥n regular para encontrar n√∫meros (enteros o decimales, positivos o negativos)\n",
    "    regex_numeros = r'-?\\d+\\.?\\d*'\n",
    "    \n",
    "    # Cuenta cu√°ntos n√∫meros v√°lidos hay en cada string\n",
    "    p = len(re.findall(regex_numeros, ar_str))\n",
    "    q = len(re.findall(regex_numeros, ma_str))\n",
    "    \n",
    "    if p > 0 and q == 0:\n",
    "        return f\"AR({p})\"\n",
    "    elif p == 0 and q > 0:\n",
    "        return f\"MA({q})\"\n",
    "    elif p > 0 and q > 0:\n",
    "        return f\"ARMA({p},{q})\"\n",
    "    else:\n",
    "        return None # O \"Ruido Blanco\" si p=0 y q=0\n",
    "\n",
    "# Aplica la funci√≥n mejorada para crear la columna \"Tipo de Modelo\"\n",
    "estacionario['Tipo de Modelo'] = estacionario.apply(determinar_tipo_modelo_mejorado, axis=1)\n",
    "\n",
    "# Imprime los valores √∫nicos de la columna Tipo de modelo para verificar\n",
    "print(\"Valores √∫nicos encontrados en 'Tipo de Modelo':\")\n",
    "print(estacionario['Tipo de Modelo'].unique())\n",
    "\n",
    "# Ordena las columnas 'Paso' y 'Tipo de modelo' al inicio\n",
    "cols = estacionario.columns.tolist()\n",
    "# Aseguramos que las columnas existan antes de moverlas\n",
    "if 'Paso' in cols:\n",
    "    cols.insert(0, cols.pop(cols.index('Paso')))\n",
    "if 'Tipo de Modelo' in cols:\n",
    "    cols.insert(1, cols.pop(cols.index('Tipo de Modelo')))\n",
    "\n",
    "estacionario = estacionario.reindex(columns=cols)\n",
    "\n",
    "\n",
    "# Borra las columnas originales 'Valores de AR' y 'Valores MA'\n",
    "estacionario = estacionario.drop(columns=['Valores de AR', 'Valores MA'])\n",
    "estacionario[\"Escenario\"] = \"Estacionario_Lineal\"\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "estacionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba423eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paso</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Distribuci√≥n</th>\n",
       "      <th>Varianza error</th>\n",
       "      <th>AREPD</th>\n",
       "      <th>AV-MCPS</th>\n",
       "      <th>Block Bootstrapping</th>\n",
       "      <th>DeepAR</th>\n",
       "      <th>EnCQR-LSTM</th>\n",
       "      <th>LSPM</th>\n",
       "      <th>LSPMW</th>\n",
       "      <th>MCPS</th>\n",
       "      <th>Sieve Bootstrap</th>\n",
       "      <th>Mejor Modelo</th>\n",
       "      <th>Escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.860823</td>\n",
       "      <td>0.258474</td>\n",
       "      <td>0.253635</td>\n",
       "      <td>0.319481</td>\n",
       "      <td>0.488711</td>\n",
       "      <td>0.367279</td>\n",
       "      <td>0.360494</td>\n",
       "      <td>0.270816</td>\n",
       "      <td>0.273828</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.244128</td>\n",
       "      <td>0.528968</td>\n",
       "      <td>0.275061</td>\n",
       "      <td>0.438099</td>\n",
       "      <td>0.322919</td>\n",
       "      <td>0.426187</td>\n",
       "      <td>0.430296</td>\n",
       "      <td>0.576792</td>\n",
       "      <td>0.272952</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.799818</td>\n",
       "      <td>0.864295</td>\n",
       "      <td>0.272406</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.396481</td>\n",
       "      <td>0.642530</td>\n",
       "      <td>0.639134</td>\n",
       "      <td>0.269655</td>\n",
       "      <td>0.275661</td>\n",
       "      <td>MCPS</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.912421</td>\n",
       "      <td>0.481159</td>\n",
       "      <td>0.255186</td>\n",
       "      <td>0.291577</td>\n",
       "      <td>0.495882</td>\n",
       "      <td>0.341570</td>\n",
       "      <td>0.341227</td>\n",
       "      <td>0.533788</td>\n",
       "      <td>0.275948</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ARIMA(0,1,0)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.822771</td>\n",
       "      <td>0.792130</td>\n",
       "      <td>0.257461</td>\n",
       "      <td>0.658698</td>\n",
       "      <td>1.291283</td>\n",
       "      <td>0.981902</td>\n",
       "      <td>0.969842</td>\n",
       "      <td>1.455485</td>\n",
       "      <td>0.338116</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76.766114</td>\n",
       "      <td>5.668568</td>\n",
       "      <td>0.965836</td>\n",
       "      <td>7.254422</td>\n",
       "      <td>13.176312</td>\n",
       "      <td>4.421885</td>\n",
       "      <td>4.173484</td>\n",
       "      <td>20.231134</td>\n",
       "      <td>2.612414</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>2</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.630681</td>\n",
       "      <td>6.161741</td>\n",
       "      <td>0.974398</td>\n",
       "      <td>8.767931</td>\n",
       "      <td>9.287902</td>\n",
       "      <td>1.733689</td>\n",
       "      <td>1.596168</td>\n",
       "      <td>21.251698</td>\n",
       "      <td>1.956761</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>3</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86.539087</td>\n",
       "      <td>10.452450</td>\n",
       "      <td>0.982561</td>\n",
       "      <td>24.631292</td>\n",
       "      <td>18.639842</td>\n",
       "      <td>6.195609</td>\n",
       "      <td>5.953120</td>\n",
       "      <td>23.480752</td>\n",
       "      <td>3.623684</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>4</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>93.057798</td>\n",
       "      <td>13.911382</td>\n",
       "      <td>0.958507</td>\n",
       "      <td>27.567728</td>\n",
       "      <td>17.852720</td>\n",
       "      <td>7.288522</td>\n",
       "      <td>6.952830</td>\n",
       "      <td>28.286507</td>\n",
       "      <td>4.681807</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>5</td>\n",
       "      <td>ARIMA(2,1,2)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.120410</td>\n",
       "      <td>13.899543</td>\n",
       "      <td>0.955009</td>\n",
       "      <td>17.064311</td>\n",
       "      <td>8.945594</td>\n",
       "      <td>4.716275</td>\n",
       "      <td>4.320090</td>\n",
       "      <td>31.056370</td>\n",
       "      <td>4.177879</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Estacionario_Lineal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Paso Tipo de Modelo Distribuci√≥n  Varianza error      AREPD    AV-MCPS  \\\n",
       "0      1   ARIMA(0,1,0)       normal             0.2   1.860823   0.258474   \n",
       "1      2   ARIMA(0,1,0)       normal             0.2   1.244128   0.528968   \n",
       "2      3   ARIMA(0,1,0)       normal             0.2   1.799818   0.864295   \n",
       "3      4   ARIMA(0,1,0)       normal             0.2   1.912421   0.481159   \n",
       "4      5   ARIMA(0,1,0)       normal             0.2   2.822771   0.792130   \n",
       "..   ...            ...          ...             ...        ...        ...   \n",
       "834    1   ARIMA(2,1,2)      mixture             3.0  76.766114   5.668568   \n",
       "835    2   ARIMA(2,1,2)      mixture             3.0  80.630681   6.161741   \n",
       "836    3   ARIMA(2,1,2)      mixture             3.0  86.539087  10.452450   \n",
       "837    4   ARIMA(2,1,2)      mixture             3.0  93.057798  13.911382   \n",
       "838    5   ARIMA(2,1,2)      mixture             3.0  99.120410  13.899543   \n",
       "\n",
       "     Block Bootstrapping     DeepAR  EnCQR-LSTM      LSPM     LSPMW  \\\n",
       "0               0.253635   0.319481    0.488711  0.367279  0.360494   \n",
       "1               0.275061   0.438099    0.322919  0.426187  0.430296   \n",
       "2               0.272406   0.291500    0.396481  0.642530  0.639134   \n",
       "3               0.255186   0.291577    0.495882  0.341570  0.341227   \n",
       "4               0.257461   0.658698    1.291283  0.981902  0.969842   \n",
       "..                   ...        ...         ...       ...       ...   \n",
       "834             0.965836   7.254422   13.176312  4.421885  4.173484   \n",
       "835             0.974398   8.767931    9.287902  1.733689  1.596168   \n",
       "836             0.982561  24.631292   18.639842  6.195609  5.953120   \n",
       "837             0.958507  27.567728   17.852720  7.288522  6.952830   \n",
       "838             0.955009  17.064311    8.945594  4.716275  4.320090   \n",
       "\n",
       "          MCPS  Sieve Bootstrap         Mejor Modelo               Escenario  \n",
       "0     0.270816         0.273828  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "1     0.576792         0.272952      Sieve Bootstrap  No_Estacionario_Lineal  \n",
       "2     0.269655         0.275661                 MCPS  No_Estacionario_Lineal  \n",
       "3     0.533788         0.275948  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "4     1.455485         0.338116  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "..         ...              ...                  ...                     ...  \n",
       "834  20.231134         2.612414  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "835  21.251698         1.956761  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "836  23.480752         3.623684  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "837  28.286507         4.681807  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "838  31.056370         4.177879  Block Bootstrapping  No_Estacionario_Lineal  \n",
       "\n",
       "[700 rows x 15 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_estacionario = pd.read_excel(\"./Datos/no_estacionario.xlsx\")\n",
    "no_estacionario.drop(columns=['Valores de AR', 'Valores MA'], inplace=True)\n",
    "no_estacionario[\"Escenario\"] = \"No_Estacionario_Lineal\"\n",
    "no_estacionario = no_estacionario[no_estacionario[\"Paso\"] != \"Promedio\"]\n",
    "no_estacionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e7ce88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paso</th>\n",
       "      <th>Tipo de Modelo</th>\n",
       "      <th>Distribuci√≥n</th>\n",
       "      <th>Varianza error</th>\n",
       "      <th>AREPD</th>\n",
       "      <th>AV-MCPS</th>\n",
       "      <th>Block Bootstrapping</th>\n",
       "      <th>DeepAR</th>\n",
       "      <th>EnCQR-LSTM</th>\n",
       "      <th>LSPM</th>\n",
       "      <th>LSPMW</th>\n",
       "      <th>MCPS</th>\n",
       "      <th>Sieve Bootstrap</th>\n",
       "      <th>Mejor Modelo</th>\n",
       "      <th>Escenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.257043</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.251524</td>\n",
       "      <td>0.263274</td>\n",
       "      <td>0.257984</td>\n",
       "      <td>0.285655</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>0.257015</td>\n",
       "      <td>0.251188</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.305723</td>\n",
       "      <td>0.383340</td>\n",
       "      <td>0.288529</td>\n",
       "      <td>0.297164</td>\n",
       "      <td>0.324101</td>\n",
       "      <td>0.316846</td>\n",
       "      <td>0.319675</td>\n",
       "      <td>0.347319</td>\n",
       "      <td>0.290022</td>\n",
       "      <td>Block Bootstrapping</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.292055</td>\n",
       "      <td>0.258555</td>\n",
       "      <td>0.287265</td>\n",
       "      <td>0.275374</td>\n",
       "      <td>0.278881</td>\n",
       "      <td>0.320347</td>\n",
       "      <td>0.320181</td>\n",
       "      <td>0.270736</td>\n",
       "      <td>0.262183</td>\n",
       "      <td>AV-MCPS</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.298469</td>\n",
       "      <td>0.269290</td>\n",
       "      <td>0.263802</td>\n",
       "      <td>0.255605</td>\n",
       "      <td>0.270449</td>\n",
       "      <td>0.290893</td>\n",
       "      <td>0.290581</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.258734</td>\n",
       "      <td>DeepAR</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SETAR(2,1)</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.298007</td>\n",
       "      <td>0.368342</td>\n",
       "      <td>0.501202</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>0.326254</td>\n",
       "      <td>0.329508</td>\n",
       "      <td>0.423889</td>\n",
       "      <td>0.442319</td>\n",
       "      <td>AREPD</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>1</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.164445</td>\n",
       "      <td>0.992519</td>\n",
       "      <td>0.962026</td>\n",
       "      <td>0.989297</td>\n",
       "      <td>1.046459</td>\n",
       "      <td>0.971555</td>\n",
       "      <td>1.076860</td>\n",
       "      <td>1.003262</td>\n",
       "      <td>0.961513</td>\n",
       "      <td>Sieve Bootstrap</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>2</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.191648</td>\n",
       "      <td>1.034591</td>\n",
       "      <td>0.986347</td>\n",
       "      <td>1.077081</td>\n",
       "      <td>0.972263</td>\n",
       "      <td>0.957417</td>\n",
       "      <td>0.986525</td>\n",
       "      <td>0.963721</td>\n",
       "      <td>0.984072</td>\n",
       "      <td>LSPM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>3</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.193252</td>\n",
       "      <td>1.387456</td>\n",
       "      <td>1.012627</td>\n",
       "      <td>0.981861</td>\n",
       "      <td>0.955903</td>\n",
       "      <td>0.987603</td>\n",
       "      <td>0.977201</td>\n",
       "      <td>1.041540</td>\n",
       "      <td>1.009812</td>\n",
       "      <td>EnCQR-LSTM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>4</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.229893</td>\n",
       "      <td>1.182221</td>\n",
       "      <td>1.124342</td>\n",
       "      <td>0.983326</td>\n",
       "      <td>0.960088</td>\n",
       "      <td>1.036372</td>\n",
       "      <td>0.978720</td>\n",
       "      <td>1.029875</td>\n",
       "      <td>1.103310</td>\n",
       "      <td>EnCQR-LSTM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>5</td>\n",
       "      <td>SETAR(2,3)</td>\n",
       "      <td>mixture</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.149135</td>\n",
       "      <td>1.068179</td>\n",
       "      <td>1.046778</td>\n",
       "      <td>0.999776</td>\n",
       "      <td>0.961731</td>\n",
       "      <td>0.961208</td>\n",
       "      <td>0.977492</td>\n",
       "      <td>1.069917</td>\n",
       "      <td>1.059325</td>\n",
       "      <td>LSPM</td>\n",
       "      <td>No_Lineal_Estacionario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Paso Tipo de Modelo Distribuci√≥n  Varianza error     AREPD   AV-MCPS  \\\n",
       "0      1     SETAR(2,1)       normal             0.2  0.257043  0.253521   \n",
       "1      2     SETAR(2,1)       normal             0.2  0.305723  0.383340   \n",
       "2      3     SETAR(2,1)       normal             0.2  0.292055  0.258555   \n",
       "3      4     SETAR(2,1)       normal             0.2  0.298469  0.269290   \n",
       "4      5     SETAR(2,1)       normal             0.2  0.298007  0.368342   \n",
       "..   ...            ...          ...             ...       ...       ...   \n",
       "834    1     SETAR(2,3)      mixture             3.0  1.164445  0.992519   \n",
       "835    2     SETAR(2,3)      mixture             3.0  1.191648  1.034591   \n",
       "836    3     SETAR(2,3)      mixture             3.0  1.193252  1.387456   \n",
       "837    4     SETAR(2,3)      mixture             3.0  1.229893  1.182221   \n",
       "838    5     SETAR(2,3)      mixture             3.0  1.149135  1.068179   \n",
       "\n",
       "     Block Bootstrapping    DeepAR  EnCQR-LSTM      LSPM     LSPMW      MCPS  \\\n",
       "0               0.251524  0.263274    0.257984  0.285655  0.282110  0.257015   \n",
       "1               0.288529  0.297164    0.324101  0.316846  0.319675  0.347319   \n",
       "2               0.287265  0.275374    0.278881  0.320347  0.320181  0.270736   \n",
       "3               0.263802  0.255605    0.270449  0.290893  0.290581  0.329900   \n",
       "4               0.501202  0.323900    0.348571  0.326254  0.329508  0.423889   \n",
       "..                   ...       ...         ...       ...       ...       ...   \n",
       "834             0.962026  0.989297    1.046459  0.971555  1.076860  1.003262   \n",
       "835             0.986347  1.077081    0.972263  0.957417  0.986525  0.963721   \n",
       "836             1.012627  0.981861    0.955903  0.987603  0.977201  1.041540   \n",
       "837             1.124342  0.983326    0.960088  1.036372  0.978720  1.029875   \n",
       "838             1.046778  0.999776    0.961731  0.961208  0.977492  1.069917   \n",
       "\n",
       "     Sieve Bootstrap         Mejor Modelo               Escenario  \n",
       "0           0.251188      Sieve Bootstrap  No_Lineal_Estacionario  \n",
       "1           0.290022  Block Bootstrapping  No_Lineal_Estacionario  \n",
       "2           0.262183              AV-MCPS  No_Lineal_Estacionario  \n",
       "3           0.258734               DeepAR  No_Lineal_Estacionario  \n",
       "4           0.442319                AREPD  No_Lineal_Estacionario  \n",
       "..               ...                  ...                     ...  \n",
       "834         0.961513      Sieve Bootstrap  No_Lineal_Estacionario  \n",
       "835         0.984072                 LSPM  No_Lineal_Estacionario  \n",
       "836         1.009812           EnCQR-LSTM  No_Lineal_Estacionario  \n",
       "837         1.103310           EnCQR-LSTM  No_Lineal_Estacionario  \n",
       "838         1.059325                 LSPM  No_Lineal_Estacionario  \n",
       "\n",
       "[700 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_lineal = pd.read_excel(\"./Datos/no_lineal.xlsx\")\n",
    "no_lineal = no_lineal[no_lineal[\"Paso\"] != \"Promedio\"]\n",
    "no_lineal[\"Escenario\"] = \"No_Lineal_Estacionario\"\n",
    "no_lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f2c36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une los tres DataFrames en uno solo uno debajo de otro\n",
    "df_all = pd.concat([estacionario, no_estacionario, no_lineal], ignore_index=True)\n",
    "# Guarda el DataFrame combinado en un archivo Excel\n",
    "df_all.to_excel(\"./Datos/datos_combinados.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf7f35",
   "metadata": {},
   "source": [
    "# Analisis con la correcion del profe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac9a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio 'resultados_completos_media_mediana' creado.\n",
      "‚úì Datos cargados exitosamente.\n",
      "\n",
      "================================================================================\n",
      "--- INICIANDO AN√ÅLISIS BASADO EN LA MEAN ---\n",
      "================================================================================\n",
      " -> 1. Analizando por estacionariedad (mean)...\n",
      " -> 2. Analizando por linealidad (mean)...\n",
      " -> 3. Generando heatmaps generales (mean)...\n",
      " -> 4. Analizando ECRPS vs Varianza (mean)...\n",
      " -> 5. Analizando ECRPS vs Paso (mean)...\n",
      " -> Analizando robustez y estabilidad (basado en mean)...\n",
      "\n",
      "================================================================================\n",
      "--- INICIANDO AN√ÅLISIS BASADO EN LA MEDIAN ---\n",
      "================================================================================\n",
      " -> 1. Analizando por estacionariedad (median)...\n",
      " -> 2. Analizando por linealidad (median)...\n",
      " -> 3. Generando heatmaps generales (median)...\n",
      " -> 4. Analizando ECRPS vs Varianza (median)...\n",
      " -> 5. Analizando ECRPS vs Paso (median)...\n",
      " -> Analizando robustez y estabilidad (basado en median)...\n",
      "\n",
      "================================================================================\n",
      "--- INICIANDO AN√ÅLISIS INDEPENDIENTES DE AGREGACI√ìN ---\n",
      "================================================================================\n",
      " -> Generando gr√°ficos de densidad individuales (an√°lisis √∫nico)...\n",
      " -> Realizando Test de Diebold-Mariano (an√°lisis √∫nico)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pleal\\AppData\\Local\\Temp\\ipykernel_16608\\4105642524.py:199: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annot_matrix = result_matrix.applymap(lambda x: {1: 'Gana', -1: 'Pierde', 0: 'Empate'}[x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì An√°lisis completo. Resultados guardados en la carpeta 'resultados_completos_media_mediana'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "CARPETA_RESULTADOS = \"resultados_completos_media_mediana\"\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "ESCENARIOS_ESTACIONARIOS = ['Estacionario_Lineal', 'No_Lineal_Estacionario']\n",
    "ESCENARIOS_NO_ESTACIONARIOS = ['No_Estacionario_Lineal']\n",
    "ESCENARIOS_LINEALES = ['Estacionario_Lineal', 'No_Estacionario_Lineal']\n",
    "ESCENARIOS_NO_LINEALES = ['No_Lineal_Estacionario']\n",
    "\n",
    "# ============================================================================\n",
    "# CLASE PARA TEST ESTAD√çSTICO\n",
    "# ============================================================================\n",
    "class DieboldMarianoTest:\n",
    "    @staticmethod\n",
    "    def dm_test(errors1, errors2, h=1, power=2):\n",
    "        # Implementaci√≥n del test... (sin cambios)\n",
    "        errors1, errors2 = np.array(errors1), np.array(errors2)\n",
    "        loss_diff = (errors1**power) - (errors2**power)\n",
    "        mean_diff = np.mean(loss_diff)\n",
    "        n = len(loss_diff)\n",
    "        gamma0 = np.var(loss_diff, ddof=1)\n",
    "        if h > 1:\n",
    "            gamma_sum = sum((1 - k/h) * np.cov(loss_diff[:-k], loss_diff[k:])[0, 1] for k in range(1, h))\n",
    "            variance = (gamma0 + 2 * gamma_sum) / n\n",
    "        else:\n",
    "            variance = gamma0 / n\n",
    "        dm_stat = mean_diff / np.sqrt(variance) if variance > 0 else 0\n",
    "        p_value = 2 * (1 - stats.norm.cdf(np.abs(dm_stat)))\n",
    "        return dm_stat, p_value\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE AN√ÅLISIS Y VISUALIZACI√ìN (MODIFICADAS)\n",
    "# ============================================================================\n",
    "def crear_directorio_resultados(nombre_carpeta):\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "        print(f\"Directorio '{nombre_carpeta}' creado.\")\n",
    "\n",
    "def guardar_grafico(nombre_archivo):\n",
    "    ruta_completa = os.path.join(CARPETA_RESULTADOS, nombre_archivo)\n",
    "    plt.savefig(ruta_completa, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def graficar_comparacion_barras(promedios1, promedios2, orden, etiqueta1, etiqueta2, agg_method, nombre_archivo):\n",
    "    \"\"\"Grafica la comparaci√≥n de barras para media o mediana.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    x = np.arange(len(orden))\n",
    "    width = 0.35\n",
    "    bars1 = ax.bar(x - width/2, promedios1[orden], width, label=etiqueta1, alpha=0.8, color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, promedios2[orden], width, label=etiqueta2, alpha=0.8, color='#e74c3c')\n",
    "    \n",
    "    ylabel = f'ECRPS {\"Promedio\" if agg_method == \"mean\" else \"Mediano\"} (menor es mejor)'\n",
    "    titulo = f'Comparaci√≥n de Desempe√±o ({agg_method.capitalize()})'\n",
    "    \n",
    "    ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(orden, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2., height, f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(nombre_archivo)\n",
    "\n",
    "def generar_heatmap(data, agg_method, titulo_sufijo, nombre_archivo, figsize=(14, 8)):\n",
    "    \"\"\"Genera un heatmap basado en media o mediana.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cbar_label = f'ECRPS {\"Promedio\" if agg_method == \"mean\" else \"Mediano\"}'\n",
    "    titulo = f'Heatmap: {titulo_sufijo} ({agg_method.capitalize()})'\n",
    "    \n",
    "    sns.heatmap(data, annot=True, fmt='.3f', cmap='RdYlGn_r',\n",
    "                cbar_kws={'label': cbar_label},\n",
    "                linewidths=0.5, linecolor='gray', ax=ax)\n",
    "    ax.set_title(titulo, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(data.index.name, fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(nombre_archivo)\n",
    "\n",
    "def graficar_evolucion_metrica_por_tipo(df, metrica_eje_x, agg_method, xlabel, nombre_archivo_sufijo):\n",
    "    \"\"\"Genera gr√°ficos de evoluci√≥n para cada tipo de modelo, usando media o mediana.\"\"\"\n",
    "    valores_unicos = sorted(df[metrica_eje_x].unique())\n",
    "    tipos_modelo_unicos = df['Tipo de Modelo'].unique()\n",
    "    \n",
    "    for tipo in tipos_modelo_unicos:\n",
    "        df_tipo = df[df['Tipo de Modelo'] == tipo]\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        for modelo in MODELOS:\n",
    "            agregados = [df_tipo[df_tipo[metrica_eje_x] == val][modelo].agg(agg_method) for val in valores_unicos]\n",
    "            if not all(np.isnan(agregados)):\n",
    "                ax.plot(valores_unicos, agregados, marker='o', linewidth=2, markersize=8, label=modelo, alpha=0.8)\n",
    "        \n",
    "        ylabel = f'ECRPS {\"Promedio\" if agg_method == \"mean\" else \"Mediano\"}'\n",
    "        titulo = f'ECRPS vs {metrica_eje_x} ({agg_method.capitalize()}) - Tipo: {tipo}'\n",
    "        \n",
    "        ax.set_xlabel(xlabel, fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(titulo, fontsize=13, fontweight='bold', pad=15)\n",
    "        ax.legend(fontsize=9, loc='best', ncol=2)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        if metrica_eje_x == 'Paso':\n",
    "            ax.set_xticks(valores_unicos)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        nombre_archivo_tipo = f'ecrps_vs_{nombre_archivo_sufijo}_tipo_{tipo.replace(\" \", \"_\").lower()}_{agg_method}.png'\n",
    "        guardar_grafico(nombre_archivo_tipo)\n",
    "\n",
    "def analizar_robustez_estabilidad(df, agg_method):\n",
    "    \"\"\"Calcula y grafica m√©tricas de robustez y estabilidad.\"\"\"\n",
    "    print(f\" -> Analizando robustez y estabilidad (basado en {agg_method})...\")\n",
    "    \n",
    "    if agg_method == 'mean':\n",
    "        # An√°lisis basado en la media (como antes)\n",
    "        metrics = [{'Modelo': m, 'Centralidad': df[m].mean(), 'Dispersion': df[m].std()} for m in MODELOS]\n",
    "        df_robust = pd.DataFrame(metrics)\n",
    "        label_centralidad = 'ECRPS Promedio (Rendimiento)'\n",
    "        label_dispersion = 'Desviaci√≥n Est√°ndar (Estabilidad)'\n",
    "        titulo_compromiso = 'Compromiso Rendimiento vs. Estabilidad (Media vs Std)'\n",
    "        \n",
    "    else: # agg_method == 'median'\n",
    "        # An√°lisis basado en la mediana (m√°s robusto a outliers)\n",
    "        metrics = [{'Modelo': m, 'Centralidad': df[m].median(), 'Dispersion': df[m].quantile(0.75) - df[m].quantile(0.25)} for m in MODELOS]\n",
    "        df_robust = pd.DataFrame(metrics)\n",
    "        label_centralidad = 'ECRPS Mediano (Rendimiento T√≠pico)'\n",
    "        label_dispersion = 'Rango Intercuart√≠lico (IQR - Estabilidad Robusta)'\n",
    "        titulo_compromiso = 'Compromiso Rendimiento vs. Estabilidad (Mediana vs IQR)'\n",
    "\n",
    "    # Gr√°fico de dispersi√≥n Rendimiento vs Estabilidad\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df_robust, x='Centralidad', y='Dispersion', hue='Modelo', s=150, alpha=0.8, ax=ax)\n",
    "    for _, row in df_robust.iterrows():\n",
    "        ax.text(row['Centralidad'], row['Dispersion'], row['Modelo'], fontsize=9, ha='left', va='bottom')\n",
    "    ax.set_xlabel(label_centralidad, fontweight='bold')\n",
    "    ax.set_ylabel(label_dispersion, fontweight='bold')\n",
    "    ax.set_title(titulo_compromiso, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(title='Modelos', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(f\"6_compromiso_rendimiento_estabilidad_{agg_method}.png\")\n",
    "\n",
    "# --- Funciones que no dependen de la agregaci√≥n (se ejecutan una sola vez) ---\n",
    "def graficar_densidades_individuales(df):\n",
    "    \"\"\"Crea un gr√°fico de densidad individual para cada modelo.\"\"\"\n",
    "    print(\" -> Generando gr√°ficos de densidad individuales (an√°lisis √∫nico)...\")\n",
    "    all_ecrps_values = df[MODELOS].values.flatten()\n",
    "    xlim_max = np.quantile(all_ecrps_values[~np.isnan(all_ecrps_values)], 0.995)\n",
    "    for modelo in MODELOS:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        sns.kdeplot(df[modelo].dropna(), fill=True, color='teal', ax=ax, lw=2.5)\n",
    "        mean_val, median_val = df[modelo].mean(), df[modelo].median()\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', label=f'Media: {mean_val:.3f}')\n",
    "        ax.axvline(median_val, color='green', linestyle=':', label=f'Mediana: {median_val:.3f}')\n",
    "        ax.set_title(f'Distribuci√≥n del ECRPS - Modelo: {modelo}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('ECRPS', fontweight='bold')\n",
    "        ax.set_ylabel('Densidad', fontweight='bold')\n",
    "        ax.set_xlim(left=0, right=xlim_max)\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        guardar_grafico(f\"7_densidad_{modelo.replace(' ', '_').lower()}.png\")\n",
    "\n",
    "def realizar_test_diebold_mariano(df):\n",
    "    \"\"\"Realiza el test de Diebold-Mariano con correcci√≥n de Bonferroni.\"\"\"\n",
    "    print(\" -> Realizando Test de Diebold-Mariano (an√°lisis √∫nico)...\")\n",
    "    pairs = list(combinations(MODELOS, 2))\n",
    "    alpha_bonferroni = 0.05 / len(pairs)\n",
    "    dm_results = []\n",
    "    for m1, m2 in pairs:\n",
    "        e1, e2 = df[m1].dropna(), df[m2].dropna()\n",
    "        min_len = min(len(e1), len(e2))\n",
    "        _, p_value = DieboldMarianoTest.dm_test(e1[:min_len], e2[:min_len])\n",
    "        winner = 'Empate' if p_value >= alpha_bonferroni else (m1 if df[m1].mean() < df[m2].mean() else m2)\n",
    "        dm_results.append({'Modelo_1': m1, 'Modelo_2': m2, 'Ganador_Bonferroni': winner})\n",
    "    \n",
    "    # Heatmap de resultados\n",
    "    result_matrix = pd.DataFrame(index=MODELOS, columns=MODELOS, data=0)\n",
    "    for _, row in pd.DataFrame(dm_results).iterrows():\n",
    "        if row['Ganador_Bonferroni'] == row['Modelo_1']:\n",
    "            result_matrix.loc[row['Modelo_1'], row['Modelo_2']], result_matrix.loc[row['Modelo_2'], row['Modelo_1']] = 1, -1\n",
    "        elif row['Ganador_Bonferroni'] == row['Modelo_2']:\n",
    "            result_matrix.loc[row['Modelo_1'], row['Modelo_2']], result_matrix.loc[row['Modelo_2'], row['Modelo_1']] = -1, 1\n",
    "\n",
    "    annot_matrix = result_matrix.applymap(lambda x: {1: 'Gana', -1: 'Pierde', 0: 'Empate'}[x])\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(result_matrix.astype(float), annot=annot_matrix, fmt='s', cmap=['red', 'lightgray', 'green'], cbar=False, ax=ax)\n",
    "    ax.set_title('Resultado Test Diebold-Mariano (con correcci√≥n de Bonferroni)', fontweight='bold')\n",
    "    guardar_grafico(\"8_dm_heatmap_bonferroni.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ============================================================================\n",
    "def main():\n",
    "    crear_directorio_resultados(CARPETA_RESULTADOS)\n",
    "    try:\n",
    "        df = pd.read_excel(RUTA_DATOS)\n",
    "        print(\"‚úì Datos cargados exitosamente.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontr√≥ el archivo en la ruta '{RUTA_DATOS}'.\")\n",
    "        return\n",
    "\n",
    "    # Bucle principal para ejecutar an√°lisis por media y mediana\n",
    "    for agg_method in ['mean', 'median']:\n",
    "        print(f\"\\n{'='*80}\\n--- INICIANDO AN√ÅLISIS BASADO EN LA {agg_method.upper()} ---\\n{'='*80}\")\n",
    "\n",
    "        # --- AN√ÅLISIS 1: ESTACIONARIEDAD ---\n",
    "        print(f\" -> 1. Analizando por estacionariedad ({agg_method})...\")\n",
    "        df_est = df[df['Escenario'].isin(ESCENARIOS_ESTACIONARIOS)]\n",
    "        df_no_est = df[df['Escenario'].isin(ESCENARIOS_NO_ESTACIONARIOS)]\n",
    "        agregados_est = df_est[MODELOS].agg(agg_method)\n",
    "        agregados_no_est = df_no_est[MODELOS].agg(agg_method)\n",
    "        orden_est = (agregados_est + agregados_no_est).sort_values().index\n",
    "        graficar_comparacion_barras(agregados_est, agregados_no_est, orden_est, 'Estacionarios', 'No Estacionarios', agg_method, f'1_comparacion_estacionariedad_{agg_method}.png')\n",
    "        \n",
    "        # --- AN√ÅLISIS 2: LINEALIDAD ---\n",
    "        print(f\" -> 2. Analizando por linealidad ({agg_method})...\")\n",
    "        df_lin = df[df['Escenario'].isin(ESCENARIOS_LINEALES)]\n",
    "        df_no_lin = df[df['Escenario'].isin(ESCENARIOS_NO_LINEALES)]\n",
    "        agregados_lin = df_lin[MODELOS].agg(agg_method)\n",
    "        agregados_no_lin = df_no_lin[MODELOS].agg(agg_method)\n",
    "        orden_lin = (agregados_lin + agregados_no_lin).sort_values().index\n",
    "        graficar_comparacion_barras(agregados_lin, agregados_no_lin, orden_lin, 'Lineales', 'No Lineales', agg_method, f'2_comparacion_linealidad_{agg_method}.png')\n",
    "\n",
    "        # --- AN√ÅLISIS 3: HEATMAPS GENERALES ---\n",
    "        print(f\" -> 3. Generando heatmaps generales ({agg_method})...\")\n",
    "        heatmap_esc_df = df.groupby('Escenario')[MODELOS].agg(agg_method)\n",
    "        generar_heatmap(heatmap_esc_df, agg_method, 'Desempe√±o por Escenario', f'3_heatmap_escenario_{agg_method}.png', figsize=(14, 6))\n",
    "        heatmap_dist_df = df.groupby('Distribuci√≥n')[MODELOS].agg(agg_method)\n",
    "        generar_heatmap(heatmap_dist_df, agg_method, 'Desempe√±o por Distribuci√≥n', f'3_heatmap_distribucion_{agg_method}.png')\n",
    "\n",
    "        # --- AN√ÅLISIS 4 & 5: EVOLUCI√ìN VS VARIANZA Y PASO ---\n",
    "        print(f\" -> 4. Analizando ECRPS vs Varianza ({agg_method})...\")\n",
    "        graficar_evolucion_metrica_por_tipo(df, 'Varianza error', agg_method, 'Varianza error', 'varianza')\n",
    "        print(f\" -> 5. Analizando ECRPS vs Paso ({agg_method})...\")\n",
    "        graficar_evolucion_metrica_por_tipo(df, 'Paso', agg_method, 'Paso (Horizonte)', 'paso')\n",
    "        \n",
    "        # --- AN√ÅLISIS 6: ROBUSTEZ Y ESTABILIDAD ---\n",
    "        analizar_robustez_estabilidad(df, agg_method)\n",
    "\n",
    "    # --- AN√ÅLISIS QUE SE EJECUTAN UNA SOLA VEZ ---\n",
    "    print(f\"\\n{'='*80}\\n--- INICIANDO AN√ÅLISIS INDEPENDIENTES DE AGREGACI√ìN ---\\n{'='*80}\")\n",
    "    # --- AN√ÅLISIS 7: DENSIDAD DE ERRORES ---\n",
    "    graficar_densidades_individuales(df)\n",
    "    \n",
    "    # --- AN√ÅLISIS 8: TEST DE DIEBOLD-MARIANO ---\n",
    "    realizar_test_diebold_mariano(df)\n",
    "    \n",
    "    print(f\"\\n‚úì An√°lisis completo. Resultados guardados en la carpeta '{CARPETA_RESULTADOS}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f9926",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b3b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio 'resultados_meta_modelo' creado.\n",
      "1. Cargando y preparando los datos para el meta-modelo...\n",
      "2. Realizando preprocesamiento (One-Hot Encoding)...\n",
      "\n",
      "3. Entrenando y evaluando los modelos recomendadores...\n",
      "\n",
      "--- Analizando: √Årbol de Decisi√≥n ---\n",
      "Precisi√≥n: 45.83%\n",
      "Reporte de Clasificaci√≥n:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              AREPD       0.00      0.00      0.00         9\n",
      "            AV-MCPS       0.00      0.00      0.00        22\n",
      "Block Bootstrapping       0.50      0.88      0.64       286\n",
      "             DeepAR       0.00      0.00      0.00        31\n",
      "         EnCQR-LSTM       0.18      0.29      0.22        35\n",
      "               LSPM       0.00      0.00      0.00        42\n",
      "              LSPMW       0.00      0.00      0.00        16\n",
      "               MCPS       0.00      0.00      0.00        20\n",
      "    Sieve Bootstrap       0.29      0.09      0.13       139\n",
      "\n",
      "           accuracy                           0.46       600\n",
      "          macro avg       0.11      0.14      0.11       600\n",
      "       weighted avg       0.32      0.46      0.35       600\n",
      "\n",
      "Generando visualizaciones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pleal\\Documents\\Unal\\Tesis\\Codigo\\Prediccion_Probabilistica\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pleal\\Documents\\Unal\\Tesis\\Codigo\\Prediccion_Probabilistica\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pleal\\Documents\\Unal\\Tesis\\Codigo\\Prediccion_Probabilistica\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Gr√°fico guardado en: resultados_meta_modelo\\feature_importance_√°rbol_de_decisi√≥n.png\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\confusion_matrix_√°rbol_de_decisi√≥n.png\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\decision_tree_visualization.png\n",
      "\n",
      "--- Analizando: Gradient Boosting ---\n",
      "Precisi√≥n: 43.17%\n",
      "Reporte de Clasificaci√≥n:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              AREPD       0.00      0.00      0.00         9\n",
      "            AV-MCPS       0.00      0.00      0.00        22\n",
      "Block Bootstrapping       0.58      0.71      0.64       286\n",
      "             DeepAR       0.21      0.26      0.23        31\n",
      "         EnCQR-LSTM       0.21      0.20      0.20        35\n",
      "               LSPM       0.18      0.10      0.12        42\n",
      "              LSPMW       0.12      0.06      0.08        16\n",
      "               MCPS       0.00      0.00      0.00        20\n",
      "    Sieve Bootstrap       0.30      0.26      0.28       139\n",
      "\n",
      "           accuracy                           0.43       600\n",
      "          macro avg       0.18      0.18      0.17       600\n",
      "       weighted avg       0.39      0.43      0.40       600\n",
      "\n",
      "Generando visualizaciones...\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\feature_importance_gradient_boosting.png\n",
      " -> Gr√°fico guardado en: resultados_meta_modelo\\confusion_matrix_gradient_boosting.png\n",
      "\n",
      "‚úì An√°lisis del meta-modelo completado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "CARPETA_RESULTADOS = \"resultados_meta_modelo\"\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR', \n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ============================================================================\n",
    "def crear_directorio_resultados(nombre_carpeta):\n",
    "    \"\"\"Crea la carpeta de resultados si no existe.\"\"\"\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "        print(f\"Directorio '{nombre_carpeta}' creado.\")\n",
    "\n",
    "def guardar_grafico(nombre_archivo):\n",
    "    \"\"\"Guarda la figura actual en un archivo y la cierra.\"\"\"\n",
    "    ruta_completa = os.path.join(CARPETA_RESULTADOS, nombre_archivo)\n",
    "    plt.savefig(ruta_completa, dpi=300, bbox_inches='tight')\n",
    "    print(f\" -> Gr√°fico guardado en: {ruta_completa}\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Grafica la importancia de las caracter√≠sticas del modelo.\"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    df_importance = pd.DataFrame({\n",
    "        'Caracter√≠stica': feature_names,\n",
    "        'Importancia': importances\n",
    "    }).sort_values(by='Importancia', ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(df_importance['Caracter√≠stica'], df_importance['Importancia'], color='steelblue')\n",
    "    ax.set_xlabel('Importancia')\n",
    "    ax.set_title(f'Importancia de Caracter√≠sticas - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(f\"feature_importance_{model_name.replace(' ', '_').lower()}.png\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, class_labels):\n",
    "    \"\"\"Grafica la matriz de confusi√≥n normalizada.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels, normalize='true')\n",
    "    df_cm = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicci√≥n del Recomendador', fontweight='bold')\n",
    "    ax.set_ylabel('Mejor Modelo Real', fontweight='bold')\n",
    "    ax.set_title(f'Matriz de Confusi√≥n Normalizada - {model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    guardar_grafico(f\"confusion_matrix_{model_name.replace(' ', '_').lower()}.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal para crear y analizar el meta-modelo.\"\"\"\n",
    "    crear_directorio_resultados(CARPETA_RESULTADOS)\n",
    "    \n",
    "    # 1. Cargar y preparar los datos\n",
    "    print(\"1. Cargando y preparando los datos para el meta-modelo...\")\n",
    "    try:\n",
    "        df = pd.read_excel(RUTA_DATOS)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontr√≥ el archivo en la ruta '{RUTA_DATOS}'.\")\n",
    "        return\n",
    "\n",
    "    features = ['Escenario', 'Distribuci√≥n', 'Varianza error', 'Paso', 'Tipo de Modelo']\n",
    "    df_meta = df[features + MODELOS].copy()\n",
    "    df_meta['Mejor_Modelo'] = df_meta[MODELOS].idxmin(axis=1)\n",
    "    df_meta.dropna(subset=features, inplace=True)\n",
    "\n",
    "    X = df_meta[features]\n",
    "    y = df_meta['Mejor_Modelo']\n",
    "    \n",
    "    # 2. Preprocesamiento de caracter√≠sticas\n",
    "    print(\"2. Realizando preprocesamiento (One-Hot Encoding)...\")\n",
    "    categorical_features = ['Escenario', 'Distribuci√≥n', 'Tipo de Modelo']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    X_encoded_cats = encoder.fit_transform(X[categorical_features])\n",
    "    \n",
    "    # Obtener los nombres de las nuevas columnas codificadas\n",
    "    encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "    \n",
    "    # Combinar caracter√≠sticas num√©ricas y codificadas\n",
    "    X_numeric = X.drop(columns=categorical_features)\n",
    "    X_processed = np.hstack((X_numeric.values, X_encoded_cats))\n",
    "    \n",
    "    # Nombres de todas las caracter√≠sticas finales\n",
    "    final_feature_names = list(X_numeric.columns) + list(encoded_feature_names)\n",
    "\n",
    "    # 3. Dividir en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 4. Definir y entrenar los modelos\n",
    "    print(\"\\n3. Entrenando y evaluando los modelos recomendadores...\")\n",
    "    models_to_train = {\n",
    "        \"√Årbol de Decisi√≥n\": DecisionTreeClassifier(max_depth=5, min_samples_leaf=20, random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42)\n",
    "    }\n",
    "    \n",
    "    class_labels = sorted(y.unique())\n",
    "\n",
    "    for name, model in models_to_train.items():\n",
    "        print(f\"\\n--- Analizando: {name} ---\")\n",
    "        \n",
    "        # Entrenar\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluar y mostrar reporte\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Precisi√≥n: {accuracy:.2%}\")\n",
    "        print(\"Reporte de Clasificaci√≥n:\")\n",
    "        print(classification_report(y_test, y_pred, labels=class_labels))\n",
    "        \n",
    "        # Generar visualizaciones\n",
    "        print(\"Generando visualizaciones...\")\n",
    "        plot_feature_importance(model, final_feature_names, name)\n",
    "        plot_confusion_matrix(y_test, y_pred, name, class_labels)\n",
    "        \n",
    "        # Visualizar el √°rbol de decisi√≥n si corresponde\n",
    "        if name == \"√Årbol de Decisi√≥n\":\n",
    "            fig, ax = plt.subplots(figsize=(25, 15))\n",
    "            plot_tree(model, feature_names=final_feature_names, class_names=class_labels, \n",
    "                      filled=True, rounded=True, fontsize=10, ax=ax)\n",
    "            ax.set_title(\"√Årbol de Decisi√≥n para Recomendaci√≥n de Modelos\", fontsize=20)\n",
    "            guardar_grafico(\"decision_tree_visualization.png\")\n",
    "\n",
    "    print(\"\\n‚úì An√°lisis del meta-modelo completado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1dff2",
   "metadata": {},
   "source": [
    "# Analisis Escalonado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7c3a4",
   "metadata": {},
   "source": [
    "## Analisis Especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536136e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREGUNTA 1: AN√ÅLISIS DEL PUNTO DE QUIEBRE DE AREPD\n",
      "================================================================================\n",
      "\n",
      "üìä Datos disponibles:\n",
      "   ‚Ä¢ Distribuci√≥n Normal: 0 observaciones\n",
      "   ‚Ä¢ Distribuci√≥n Mixture: 0 observaciones\n",
      "   ‚Ä¢ Niveles de varianza: 4\n",
      "   ‚Ä¢ Rango: 0.2000 a 3.0000\n",
      "\n",
      "‚ùå ERROR: No hay datos v√°lidos para el an√°lisis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "DIR_SALIDA = \"./resultados_preguntas_profundizacion\"\n",
    "\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR',\n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "\n",
    "COLORES_MODELOS = {\n",
    "    'AREPD': '#e41a1c',\n",
    "    'AV-MCPS': '#377eb8',\n",
    "    'Block Bootstrapping': '#4daf4a',\n",
    "    'DeepAR': '#984ea3',\n",
    "    'EnCQR-LSTM': '#ff7f00',\n",
    "    'LSPM': '#ffff33',\n",
    "    'LSPMW': '#a65628',\n",
    "    'MCPS': '#f781bf',\n",
    "    'Sieve Bootstrap': '#999999'\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLASE PRINCIPAL DE AN√ÅLISIS - PREGUNTAS DE PROFUNDIZACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "class AnalizadorPreguntasProfundizacion:\n",
    "    \"\"\"An√°lisis espec√≠fico para responder preguntas de profundizaci√≥n\"\"\"\n",
    "\n",
    "    def __init__(self, ruta_datos):\n",
    "        \"\"\"Inicializa el analizador\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"AN√ÅLISIS DE PREGUNTAS DE PROFUNDIZACI√ìN\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        self.df = pd.read_excel(ruta_datos)\n",
    "        self.modelos = MODELOS\n",
    "        self.dir_salida = Path(DIR_SALIDA)\n",
    "        self.dir_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Extraer caracter√≠sticas del escenario\n",
    "        self._extraer_caracteristicas()\n",
    "\n",
    "        print(f\"‚úì Datos cargados: {self.df.shape[0]} filas, {self.df.shape[1]} columnas\")\n",
    "        print(f\"‚úì Modelos a analizar: {len(self.modelos)}\")\n",
    "        print(f\"‚úì Directorio de salida: {self.dir_salida}\")\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    def _extraer_caracteristicas(self):\n",
    "        \"\"\"Extrae caracter√≠sticas individuales del escenario\"\"\"\n",
    "        self.df['Estacionario'] = self.df['Escenario'].apply(\n",
    "            lambda x: 'Estacionario' if 'Estacionario' in x and 'No_Estacionario' not in x else 'No Estacionario'\n",
    "        )\n",
    "\n",
    "        self.df['Lineal'] = self.df['Escenario'].apply(\n",
    "            lambda x: 'Lineal' if 'Lineal' in x and 'No_Lineal' not in x else 'No Lineal'\n",
    "        )\n",
    "\n",
    "    def ejecutar_analisis_completo(self):\n",
    "        \"\"\"Ejecuta todos los an√°lisis para las preguntas\"\"\"\n",
    "        print(\"\\n\" + \"üî¨\" * 40 + \"\\n\")\n",
    "\n",
    "        # Pregunta 1: Punto de quiebre de AREPD\n",
    "        print(\"1Ô∏è‚É£  Pregunta 1: Punto de quiebre de AREPD...\")\n",
    "        self._pregunta_1_punto_quiebre_arepd()\n",
    "\n",
    "        # Pregunta 2: Robustez de Block Bootstrapping vs Sieve Bootstrap\n",
    "        print(\"\\n2Ô∏è‚É£  Pregunta 2: Zona de dominio Block Bootstrapping...\")\n",
    "        self._pregunta_2_zona_dominio_bb()\n",
    "\n",
    "        # Pregunta 3: Deterioro acelerado AV-MCPS\n",
    "        print(\"\\n3Ô∏è‚É£  Pregunta 3: Deterioro de AV-MCPS por horizonte...\")\n",
    "        self._pregunta_3_deterioro_av_mcps()\n",
    "\n",
    "        # Pregunta 4: Penalizaci√≥n Normal multiplicativa\n",
    "        print(\"\\n4Ô∏è‚É£  Pregunta 4: Efecto multiplicativo distribuci√≥n Normal...\")\n",
    "        self._pregunta_4_penalizacion_normal()\n",
    "\n",
    "        # Pregunta 5: Frontera de colapso Deep Learning\n",
    "        print(\"\\n5Ô∏è‚É£  Pregunta 5: Frontera de colapso Deep Learning...\")\n",
    "        self._pregunta_5_frontera_dl()\n",
    "\n",
    "        # Pregunta 6: Consistencia \"Mejor Modelo\"\n",
    "        print(\"\\n6Ô∏è‚É£  Pregunta 6: Validaci√≥n de 'Mejor Modelo'...\")\n",
    "        self._pregunta_6_consistencia_mejor_modelo()\n",
    "\n",
    "        # Pregunta 7: An√°lisis de segunda derivada\n",
    "        print(\"\\n7Ô∏è‚É£  Pregunta 7: Aceleraci√≥n del deterioro...\")\n",
    "        self._pregunta_7_segunda_derivada()\n",
    "\n",
    "        # Pregunta 8: Interacci√≥n No Linealidad √ó Varianza\n",
    "        print(\"\\n8Ô∏è‚É£  Pregunta 8: Colapso LSPM con varianza alta...\")\n",
    "        self._pregunta_8_interaccion_nolineal_varianza()\n",
    "\n",
    "        # Pregunta 9: Mapa de decisi√≥n operacional\n",
    "        print(\"\\n9Ô∏è‚É£  Pregunta 9: Mapa de decisi√≥n operacional...\")\n",
    "        self._pregunta_9_mapa_decision()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ AN√ÅLISIS DE PREGUNTAS COMPLETO\")\n",
    "        print(f\"üìÅ Resultados guardados en: {self.dir_salida}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 1: PUNTO DE QUIEBRE DE AREPD\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_1_punto_quiebre_arepd(self):\n",
    "        \"\"\"\n",
    "        ¬øExiste un punto de quiebre espec√≠fico en la varianza del error donde \n",
    "        AREPD comienza su deterioro catastr√≥fico?\n",
    "        ¬øEste punto es consistente entre distribuciones Normal y Mixture?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Filtrar distribuciones de inter√©s\n",
    "        df_normal = self.df[self.df['Distribuci√≥n'] == 'Normal'].copy()\n",
    "        df_mixture = self.df[self.df['Distribuci√≥n'] == 'Mixture'].copy()\n",
    "        \n",
    "        # Calcular rendimiento promedio por varianza para AREPD\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "        \n",
    "        arepd_normal = []\n",
    "        arepd_mixture = []\n",
    "        otros_normal = []\n",
    "        otros_mixture = []\n",
    "        \n",
    "        for var in varianzas:\n",
    "            # AREPD\n",
    "            arepd_normal.append(df_normal[df_normal['Varianza error'] == var]['AREPD'].mean())\n",
    "            arepd_mixture.append(df_mixture[df_mixture['Varianza error'] == var]['AREPD'].mean())\n",
    "            \n",
    "            # Promedio de otros modelos robustos para comparaci√≥n\n",
    "            otros_modelos = ['Block Bootstrapping', 'Sieve Bootstrap', 'LSPM']\n",
    "            otros_normal.append(df_normal[df_normal['Varianza error'] == var][otros_modelos].mean().mean())\n",
    "            otros_mixture.append(df_mixture[df_mixture['Varianza error'] == var][otros_modelos].mean().mean())\n",
    "        \n",
    "        # FIGURA 1.1: Evoluci√≥n de AREPD vs modelos robustos\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # Normal\n",
    "        ax1.plot(varianzas, arepd_normal, 'o-', label='AREPD', \n",
    "                color=COLORES_MODELOS['AREPD'], linewidth=3, markersize=10)\n",
    "        ax1.plot(varianzas, otros_normal, 's--', label='Promedio Modelos Robustos',\n",
    "                color='green', linewidth=2, markersize=8, alpha=0.7)\n",
    "        ax1.set_xlabel('Varianza del Error', fontweight='bold', fontsize=12)\n",
    "        ax1.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "        ax1.set_title('Distribuci√≥n Normal: Punto de Quiebre AREPD', \n",
    "                     fontweight='bold', fontsize=13)\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mixture\n",
    "        ax2.plot(varianzas, arepd_mixture, 'o-', label='AREPD',\n",
    "                color=COLORES_MODELOS['AREPD'], linewidth=3, markersize=10)\n",
    "        ax2.plot(varianzas, otros_mixture, 's--', label='Promedio Modelos Robustos',\n",
    "                color='green', linewidth=2, markersize=8, alpha=0.7)\n",
    "        ax2.set_xlabel('Varianza del Error', fontweight='bold', fontsize=12)\n",
    "        ax2.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "        ax2.set_title('Distribuci√≥n Mixture: Punto de Quiebre AREPD',\n",
    "                     fontweight='bold', fontsize=13)\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P1_1_punto_quiebre_arepd_comparativo.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 1.2: Tasa de deterioro incremental\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Calcular tasas de cambio\n",
    "        tasas_normal = np.diff(arepd_normal) / np.diff(varianzas)\n",
    "        tasas_mixture = np.diff(arepd_mixture) / np.diff(varianzas)\n",
    "        var_medias = [(varianzas[i] + varianzas[i+1])/2 for i in range(len(varianzas)-1)]\n",
    "        \n",
    "        ax.plot(var_medias, tasas_normal, 'o-', label='Normal', \n",
    "               color='red', linewidth=3, markersize=10)\n",
    "        ax.plot(var_medias, tasas_mixture, 's-', label='Mixture',\n",
    "               color='orange', linewidth=3, markersize=10)\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "        ax.set_xlabel('Varianza del Error (punto medio)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Tasa de Deterioro (ŒîECRPS/ŒîVarianza)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('AREPD: Aceleraci√≥n del Deterioro por Distribuci√≥n\\n(Mayor pendiente = Colapso m√°s r√°pido)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=12, loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Identificar punto de m√°xima aceleraci√≥n\n",
    "        max_accel_normal_idx = np.argmax(tasas_normal)\n",
    "        max_accel_mixture_idx = np.argmax(tasas_mixture)\n",
    "        \n",
    "        ax.annotate(f'M√°xima aceleraci√≥n\\nVarianza ‚âà {var_medias[max_accel_normal_idx]:.3f}',\n",
    "                   xy=(var_medias[max_accel_normal_idx], tasas_normal[max_accel_normal_idx]),\n",
    "                   xytext=(10, 20), textcoords='offset points',\n",
    "                   bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "                   arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='red', lw=2),\n",
    "                   fontsize=10, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P1_2_tasa_deterioro_arepd.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Calcular umbral de quiebre (donde la tasa supera 2x la mediana)\n",
    "        umbral_normal = var_medias[max_accel_normal_idx] if len(var_medias) > 0 else None\n",
    "        umbral_mixture = var_medias[max_accel_mixture_idx] if len(var_medias) > 0 else None\n",
    "        \n",
    "        print(f\"   ‚úì Punto de quiebre AREPD (Normal): Varianza ‚âà {umbral_normal:.3f}\")\n",
    "        print(f\"   ‚úì Punto de quiebre AREPD (Mixture): Varianza ‚âà {umbral_mixture:.3f}\")\n",
    "        print(f\"   ‚úì Diferencia entre distribuciones: {abs(umbral_normal - umbral_mixture):.3f}\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 2: ZONA DE DOMINIO BLOCK BOOTSTRAPPING\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_2_zona_dominio_bb(self):\n",
    "        \"\"\"\n",
    "        ¬øEn qu√© condiciones EXACTAS Block Bootstrapping supera a Sieve Bootstrap?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Crear DataFrame de comparaci√≥n directa\n",
    "        df_comp = self.df.copy()\n",
    "        df_comp['BB_mejor'] = df_comp['Block Bootstrapping'] < df_comp['Sieve Bootstrap']\n",
    "        df_comp['Diferencia'] = df_comp['Sieve Bootstrap'] - df_comp['Block Bootstrapping']\n",
    "        \n",
    "        # FIGURA 2.1: Mapa de calor de superioridad\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        escenarios_principales = [\n",
    "            ('Estacionario', 'Lineal'),\n",
    "            ('Estacionario', 'No Lineal'),\n",
    "            ('No Estacionario', 'Lineal'),\n",
    "            ('No Estacionario', 'No Lineal')\n",
    "        ]\n",
    "        \n",
    "        for idx, (est, lin) in enumerate(escenarios_principales):\n",
    "            ax = axes[idx]\n",
    "            df_esc = df_comp[(df_comp['Estacionario'] == est) & (df_comp['Lineal'] == lin)]\n",
    "            \n",
    "            # Crear matriz de diferencias\n",
    "            pivot = df_esc.pivot_table(\n",
    "                values='Diferencia',\n",
    "                index='Distribuci√≥n',\n",
    "                columns='Varianza error',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn', center=0,\n",
    "                       ax=ax, cbar_kws={'label': 'SB - BB (>0 = BB mejor)'},\n",
    "                       linewidths=1, linecolor='gray', vmin=-0.02, vmax=0.02)\n",
    "            ax.set_title(f'{est} + {lin}', fontweight='bold', fontsize=12)\n",
    "            ax.set_xlabel('Varianza Error', fontweight='bold')\n",
    "            ax.set_ylabel('Distribuci√≥n', fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('Zona de Dominio: Block Bootstrapping vs Sieve Bootstrap\\n(Verde = BB domina, Rojo = SB domina)',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P2_1_zona_dominio_bb_heatmap.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 2.2: Frecuencia de dominio por condiciones\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Calcular % de casos donde BB es mejor\n",
    "        resultados_dominio = []\n",
    "        for est in ['Estacionario', 'No Estacionario']:\n",
    "            for lin in ['Lineal', 'No Lineal']:\n",
    "                df_esc = df_comp[(df_comp['Estacionario'] == est) & (df_comp['Lineal'] == lin)]\n",
    "                pct_bb_mejor = (df_esc['BB_mejor'].sum() / len(df_esc) * 100) if len(df_esc) > 0 else 0\n",
    "                resultados_dominio.append({\n",
    "                    'Escenario': f'{est[:3]}+{lin[:3]}',\n",
    "                    'Completo': f'{est} + {lin}',\n",
    "                    'Pct_BB_Mejor': pct_bb_mejor\n",
    "                })\n",
    "        \n",
    "        df_dominio = pd.DataFrame(resultados_dominio).sort_values('Pct_BB_Mejor', ascending=False)\n",
    "        \n",
    "        colors = ['green' if x > 50 else 'red' for x in df_dominio['Pct_BB_Mejor']]\n",
    "        bars = ax.barh(df_dominio['Escenario'], df_dominio['Pct_BB_Mejor'],\n",
    "                      color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axvline(50, color='black', linestyle='--', linewidth=2, label='50% (Equilibrio)')\n",
    "        ax.set_xlabel('% de casos donde BB supera a SB', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Frecuencia de Dominio de Block Bootstrapping\\n(>50% = BB generalmente mejor)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.set_xlim(0, 100)\n",
    "        \n",
    "        for i, (bar, val) in enumerate(zip(bars, df_dominio['Pct_BB_Mejor'])):\n",
    "            ax.text(val + 2, i, f'{val:.1f}%', va='center', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P2_2_frecuencia_dominio_bb.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úì BB domina en: {df_comp['BB_mejor'].sum()} / {len(df_comp)} casos ({df_comp['BB_mejor'].sum()/len(df_comp)*100:.1f}%)\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 3: DETERIORO AV-MCPS POR HORIZONTE\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_3_deterioro_av_mcps(self):\n",
    "        \"\"\"\n",
    "        ¬øEl deterioro de AV-MCPS es lineal, cuadr√°tico o exponencial?\n",
    "        ¬øCambia seg√∫n el nivel de varianza?\n",
    "        \"\"\"\n",
    "        \n",
    "        pasos = sorted(self.df['Paso'].unique())\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "        \n",
    "        # FIGURA 3.1: Ajuste de curvas de deterioro\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Seleccionar niveles de varianza representativos\n",
    "        if len(varianzas) >= 4:\n",
    "            var_seleccionadas = [varianzas[0], varianzas[len(varianzas)//3], \n",
    "                                varianzas[2*len(varianzas)//3], varianzas[-1]]\n",
    "        else:\n",
    "            var_seleccionadas = varianzas\n",
    "        \n",
    "        modelos_comparacion = ['AV-MCPS', 'LSPM', 'Block Bootstrapping']\n",
    "        \n",
    "        for idx, var in enumerate(var_seleccionadas[:4]):\n",
    "            ax = axes[idx]\n",
    "            df_var = self.df[self.df['Varianza error'] == var]\n",
    "            \n",
    "            for modelo in modelos_comparacion:\n",
    "                valores = [df_var[df_var['Paso'] == p][modelo].mean() for p in pasos]\n",
    "                ax.plot(pasos, valores, 'o-', label=modelo, \n",
    "                       linewidth=2.5, markersize=8, color=COLORES_MODELOS[modelo])\n",
    "            \n",
    "            ax.set_xlabel('Horizonte (Paso)', fontweight='bold', fontsize=11)\n",
    "            ax.set_ylabel('ECRPS', fontweight='bold', fontsize=11)\n",
    "            ax.set_title(f'Varianza = {var:.3f}', fontweight='bold', fontsize=12)\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Evoluci√≥n del Deterioro por Horizonte: AV-MCPS vs Modelos Estables',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P3_1_deterioro_av_mcps_curvas.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 3.2: An√°lisis de tipo de crecimiento\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Calcular R¬≤ para diferentes tipos de ajuste\n",
    "        tipos_ajuste = []\n",
    "        \n",
    "        for var in varianzas:\n",
    "            df_var = self.df[self.df['Varianza error'] == var]\n",
    "            valores_av = [df_var[df_var['Paso'] == p]['AV-MCPS'].mean() for p in pasos]\n",
    "            \n",
    "            x = np.array(pasos)\n",
    "            y = np.array(valores_av)\n",
    "            \n",
    "            # Ajuste lineal\n",
    "            p_lin = np.polyfit(x, y, 1)\n",
    "            y_lin = np.polyval(p_lin, x)\n",
    "            r2_lin = 1 - (np.sum((y - y_lin)**2) / np.sum((y - np.mean(y))**2))\n",
    "            \n",
    "            # Ajuste cuadr√°tico\n",
    "            p_quad = np.polyfit(x, y, 2)\n",
    "            y_quad = np.polyval(p_quad, x)\n",
    "            r2_quad = 1 - (np.sum((y - y_quad)**2) / np.sum((y - np.mean(y))**2))\n",
    "            \n",
    "            # Ajuste exponencial (logar√≠tmico)\n",
    "            try:\n",
    "                z = np.polyfit(x, np.log(y + 1e-10), 1)\n",
    "                y_exp = np.exp(np.polyval(z, x))\n",
    "                r2_exp = 1 - (np.sum((y - y_exp)**2) / np.sum((y - np.mean(y))**2))\n",
    "            except:\n",
    "                r2_exp = 0\n",
    "            \n",
    "            mejor_ajuste = max([('Lineal', r2_lin), ('Cuadr√°tico', r2_quad), ('Exponencial', r2_exp)], \n",
    "                              key=lambda x: x[1])\n",
    "            \n",
    "            tipos_ajuste.append({\n",
    "                'Varianza': var,\n",
    "                'R2_Lineal': r2_lin,\n",
    "                'R2_Cuadratico': r2_quad,\n",
    "                'R2_Exponencial': r2_exp,\n",
    "                'Mejor': mejor_ajuste[0]\n",
    "            })\n",
    "        \n",
    "        df_ajustes = pd.DataFrame(tipos_ajuste)\n",
    "        \n",
    "        x_pos = np.arange(len(varianzas))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax.bar(x_pos - width, df_ajustes['R2_Lineal'], width, label='Lineal', alpha=0.8)\n",
    "        ax.bar(x_pos, df_ajustes['R2_Cuadratico'], width, label='Cuadr√°tico', alpha=0.8)\n",
    "        ax.bar(x_pos + width, df_ajustes['R2_Exponencial'], width, label='Exponencial', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Varianza del Error', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('R¬≤ (Bondad de Ajuste)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('AV-MCPS: Tipo de Deterioro por Nivel de Varianza\\n(R¬≤ m√°s alto = Mejor ajuste)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels([f'{v:.3f}' for v in varianzas], rotation=45)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P3_2_tipo_deterioro_av_mcps.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úì Tipo de deterioro predominante: {df_ajustes['Mejor'].mode()[0]}\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 4: PENALIZACI√ìN NORMAL MULTIPLICATIVA\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_4_penalizacion_normal(self):\n",
    "        \"\"\"\n",
    "        ¬øLa penalizaci√≥n de la distribuci√≥n Normal es aditiva o multiplicativa \n",
    "        con la no-estacionariedad?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calcular deterioro por distribuci√≥n en cada escenario\n",
    "        modelos_analisis = ['DeepAR', 'MCPS', 'LSPM', 'Block Bootstrapping']\n",
    "        \n",
    "        # FIGURA 4.1: Efecto aditivo vs multiplicativo\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        resultados_interaccion = []\n",
    "        \n",
    "        for idx, modelo in enumerate(modelos_analisis):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Calcular penalizaci√≥n por escenario\n",
    "            penalizaciones = []\n",
    "            \n",
    "            for est in ['Estacionario', 'No Estacionario']:\n",
    "                df_est = self.df[self.df['Estacionario'] == est]\n",
    "                \n",
    "                # Rendimiento en Normal vs t-student\n",
    "                rend_normal = df_est[df_est['Distribuci√≥n'] == 'Normal'][modelo].mean()\n",
    "                rend_tstudent = df_est[df_est['Distribuci√≥n'] == 't-student'][modelo].mean()\n",
    "                \n",
    "                deterioro = ((rend_normal - rend_tstudent) / rend_tstudent) * 100\n",
    "                penalizaciones.append({\n",
    "                    'Estacionariedad': est,\n",
    "                    'Deterioro_pct': deterioro\n",
    "                })\n",
    "            \n",
    "            df_pen = pd.DataFrame(penalizaciones)\n",
    "            \n",
    "            # Calcular raz√≥n de efectos\n",
    "            det_estacionario = df_pen[df_pen['Estacionariedad'] == 'Estacionario']['Deterioro_pct'].values[0]\n",
    "            det_no_estacionario = df_pen[df_pen['Estacionariedad'] == 'No Estacionario']['Deterioro_pct'].values[0]\n",
    "            \n",
    "            razon = det_no_estacionario / det_estacionario if det_estacionario != 0 else 0\n",
    "            es_multiplicativo = razon > 1.5  # Si el efecto es >50% mayor, es multiplicativo\n",
    "            \n",
    "            resultados_interaccion.append({\n",
    "                'Modelo': modelo,\n",
    "                'Razon': razon,\n",
    "                'Tipo': 'Multiplicativo' if es_multiplicativo else 'Aditivo'\n",
    "            })\n",
    "            \n",
    "            # Visualizaci√≥n\n",
    "            colors = ['lightblue', 'coral']\n",
    "            bars = ax.bar(df_pen['Estacionariedad'], df_pen['Deterioro_pct'], \n",
    "                         color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "            ax.set_ylabel('Deterioro por Normal (%)', fontweight='bold', fontsize=11)\n",
    "            ax.set_title(f'{modelo}\\nRaz√≥n: {razon:.2f}x ({(\"MULTIPLICATIVO\" if es_multiplicativo else \"ADITIVO\")})',\n",
    "                        fontweight='bold', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "            \n",
    "            for bar, val in zip(bars, df_pen['Deterioro_pct']):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                       f'{val:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.suptitle('Interacci√≥n: Distribuci√≥n Normal √ó No-Estacionariedad\\n(Raz√≥n >1.5 = Efecto Multiplicativo)',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P4_1_penalizacion_normal_interaccion.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 4.2: Resumen de tipos de interacci√≥n\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        df_interaccion = pd.DataFrame(resultados_interaccion).sort_values('Razon', ascending=False)\n",
    "        \n",
    "        colors_tipo = ['red' if x == 'Multiplicativo' else 'green' for x in df_interaccion['Tipo']]\n",
    "        bars = ax.barh(df_interaccion['Modelo'], df_interaccion['Razon'],\n",
    "                      color=colors_tipo, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axvline(1.5, color='black', linestyle='--', linewidth=2, label='Umbral Multiplicativo (1.5x)')\n",
    "        ax.axvline(1.0, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "        ax.set_xlabel('Raz√≥n de Efectos (No-Est / Est)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Clasificaci√≥n del Tipo de Interacci√≥n por Modelo\\n(Rojo = Multiplicativo, Verde = Aditivo)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6662db",
   "metadata": {},
   "source": [
    "## Analisis General Corregido*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ce1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà          AN√ÅLISIS COMPLETO DE BASE DE DATOS - VERSI√ìN MEJORADA         ‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "\n",
      "================================================================================\n",
      "INICIANDO AN√ÅLISIS COMPLETO DE BASE DE DATOS - VERSI√ìN MEJORADA\n",
      "================================================================================\n",
      "\n",
      "‚úì Caracter√≠sticas extra√≠das:\n",
      "  - Estacionariedad: ['Estacionario' 'No Estacionario']\n",
      "  - Linealidad: ['Lineal' 'No Lineal']\n",
      "  - Tipos de Modelo: ['AR(1)' 'AR(2)' 'MA(1)' 'MA(2)' 'ARMA(1,1)' 'ARMA(2,2)' 'ARIMA(0,1,0)'\n",
      " 'ARIMA(1,1,0)' 'ARIMA(2,1,0)' 'ARIMA(0,1,1)' 'ARIMA(0,1,2)'\n",
      " 'ARIMA(1,1,1)' 'ARIMA(2,1,2)' 'SETAR(2,1)' 'TAR(2,1)' 'EXPAR(2,1)'\n",
      " 'BILINEAR(1)' 'SETAR(2,2)' 'TAR(2,2)' 'SETAR(2,3)']\n",
      "  - Distribuciones: ['normal' 'uniform' 'exponential' 't-student' 'mixture']\n",
      "  - Varianzas: [np.float64(0.2), np.float64(0.5), np.float64(1.0), np.float64(3.0)]\n",
      "  - Pasos: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "‚úì Datos cargados: 2000 filas, 17 columnas\n",
      "‚úì Modelos a analizar: 9\n",
      "‚úì Directorio de salida: resultados_base_completa_mejorado\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨\n",
      "\n",
      "1Ô∏è‚É£  Analizando impacto de Estacionariedad...\n",
      "   ‚úì 2 figuras generadas para estacionariedad\n",
      "\n",
      "2Ô∏è‚É£  Analizando impacto de Linealidad...\n",
      "   ‚úì 2 figuras generadas para linealidad\n",
      "\n",
      "3Ô∏è‚É£  Analizando efecto del Modelo Generador...\n",
      "   ‚úì 2 figuras generadas para modelo generador\n",
      "\n",
      "4Ô∏è‚É£  Analizando influencia de Distribuci√≥n...\n",
      "   ‚úì 2 figuras generadas para distribuci√≥n\n",
      "\n",
      "5Ô∏è‚É£  Analizando impacto de Varianza...\n",
      "   ‚úì 2 figuras generadas para varianza\n",
      "\n",
      "6Ô∏è‚É£  Analizando deterioro por Horizonte...\n",
      "   ‚úì 2 figuras generadas para horizonte\n",
      "\n",
      "7Ô∏è‚É£  Analizando Robustez y Estabilidad...\n",
      "   ‚úì 1 figura generada para robustez\n",
      "\n",
      "8Ô∏è‚É£  Analizando Diferencias Estad√≠sticamente Significativas...\n",
      "\n",
      "================================================================================\n",
      "REALIZANDO TEST DE DIEBOLD-MARIANO\n",
      "================================================================================\n",
      "\n",
      "   N√∫mero de comparaciones: 36\n",
      "   Alpha corregido (Bonferroni): 0.001389\n",
      "   Comparaciones significativas: 35\n",
      "\n",
      "   ‚úì Ranking guardado: Top 3\n",
      "      1. Block Bootstrapping - Score: 8 (V:8, D:0, E:0)\n",
      "      2. Sieve Bootstrap - Score: 6 (V:7, D:1, E:0)\n",
      "      3. LSPM - Score: 4 (V:6, D:2, E:0)\n",
      "\n",
      "   ‚úì 1 figura generada para significancia\n",
      "\n",
      "\n",
      "9Ô∏è‚É£  Analizando Impacto de Caracter√≠sticas (Permutation Importance)...\n",
      "   Calculando PFI para el modelo: AREPD\n",
      "   Entrenando meta-modelo para AREPD...\n",
      "   ‚úì Meta-modelo entrenado para AREPD.\n",
      "   ‚úì PFI calculado para AREPD\n",
      "\n",
      "   Calculando PFI para el modelo: AV-MCPS\n",
      "   Entrenando meta-modelo para AV-MCPS...\n",
      "   ‚úì Meta-modelo entrenado para AV-MCPS.\n",
      "   ‚úì PFI calculado para AV-MCPS\n",
      "\n",
      "   Calculando PFI para el modelo: Block Bootstrapping\n",
      "   Entrenando meta-modelo para Block Bootstrapping...\n",
      "   ‚úì Meta-modelo entrenado para Block Bootstrapping.\n",
      "   ‚úì PFI calculado para Block Bootstrapping\n",
      "\n",
      "   Calculando PFI para el modelo: DeepAR\n",
      "   Entrenando meta-modelo para DeepAR...\n",
      "   ‚úì Meta-modelo entrenado para DeepAR.\n",
      "   ‚úì PFI calculado para DeepAR\n",
      "\n",
      "   Calculando PFI para el modelo: EnCQR-LSTM\n",
      "   Entrenando meta-modelo para EnCQR-LSTM...\n",
      "   ‚úì Meta-modelo entrenado para EnCQR-LSTM.\n",
      "   ‚úì PFI calculado para EnCQR-LSTM\n",
      "\n",
      "   Calculando PFI para el modelo: LSPM\n",
      "   Entrenando meta-modelo para LSPM...\n",
      "   ‚úì Meta-modelo entrenado para LSPM.\n",
      "   ‚úì PFI calculado para LSPM\n",
      "\n",
      "   Calculando PFI para el modelo: LSPMW\n",
      "   Entrenando meta-modelo para LSPMW...\n",
      "   ‚úì Meta-modelo entrenado para LSPMW.\n",
      "   ‚úì PFI calculado para LSPMW\n",
      "\n",
      "   Calculando PFI para el modelo: MCPS\n",
      "   Entrenando meta-modelo para MCPS...\n",
      "   ‚úì Meta-modelo entrenado para MCPS.\n",
      "   ‚úì PFI calculado para MCPS\n",
      "\n",
      "   Calculando PFI para el modelo: Sieve Bootstrap\n",
      "   Entrenando meta-modelo para Sieve Bootstrap...\n",
      "   ‚úì Meta-modelo entrenado para Sieve Bootstrap.\n",
      "   ‚úì PFI calculado para Sieve Bootstrap\n",
      "\n",
      "   ‚úì PFI calculado para todos los modelos.\n",
      "   ‚úì 1 figura (PFI Global Promedio) generada para impacto de caracter√≠sticas.\n",
      "\n",
      "\n",
      "üîü Analizando Variabilidad (PDP e ICE)...\n",
      "   Generando PDP/ICE para el modelo con mejor ranking: Block Bootstrapping\n",
      "      Generando PDP/ICE para 'Varianza error'...\n",
      "      ‚úì PDP/ICE generado para 'Varianza error'.\n",
      "      Generando PDP/ICE para 'Tipo de Modelo'...\n",
      "      ‚ùå Error generando PDP/ICE para 'Tipo de Modelo': can't multiply sequence by non-int of type 'float'\n",
      "      Generando PDP/ICE para 'Paso'...\n",
      "      ‚úì PDP/ICE generado para 'Paso'.\n",
      "   ‚úì PDP e ICE generados para las caracter√≠sticas m√°s importantes del mejor modelo.\n",
      "\n",
      "\n",
      "‚ú® Generando Resumen Ejecutivo...\n",
      "\n",
      "================================================================================\n",
      "GENERANDO RESUMEN EJECUTIVO\n",
      "================================================================================\n",
      "\n",
      "   ‚úì 2 figuras generadas para resumen ejecutivo\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚úÖ AN√ÅLISIS COMPLETO FINALIZADO\n",
      "üìÅ Resultados guardados en: resultados_base_completa_mejorado\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà                    ‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE                       ‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üìä TOTAL DE FIGURAS GENERADAS: 19 im√°genes PNG\n",
      "\n",
      "üìÅ ESTRUCTURA DE RESULTADOS:\n",
      "   ./resultados_base_completa_mejorado/\n",
      "   ‚îú‚îÄ‚îÄ 1.1: Estacionariedad - Comparativo\n",
      "   ‚îú‚îÄ‚îÄ 1.2: Estacionariedad - Cambio Relativo\n",
      "   ‚îú‚îÄ‚îÄ 2.1: Linealidad - Comparativo\n",
      "   ‚îú‚îÄ‚îÄ 2.2: Linealidad - Cambio Relativo\n",
      "   ‚îú‚îÄ‚îÄ 3.2: Modelo Generador - Z-Score\n",
      "   ‚îú‚îÄ‚îÄ 3.3: Modelo Generador - Variabilidad\n",
      "   ‚îú‚îÄ‚îÄ 4.1: Distribuci√≥n - Heatmap ECRPS\n",
      "   ‚îú‚îÄ‚îÄ 4.2: Distribuci√≥n - Heatmap Variabilidad\n",
      "   ‚îú‚îÄ‚îÄ 5.1: Varianza - Tendencias\n",
      "   ‚îú‚îÄ‚îÄ 5.2: Varianza - Tasa de Crecimiento\n",
      "   ‚îú‚îÄ‚îÄ 6.1: Horizonte - Evoluci√≥n\n",
      "   ‚îú‚îÄ‚îÄ 6.2: Horizonte - Tasa de Deterioro\n",
      "   ‚îú‚îÄ‚îÄ 7.2: Robustez - Coeficiente de Variaci√≥n\n",
      "   ‚îú‚îÄ‚îÄ 8.2: Significancia - Matriz de Superioridad\n",
      "   ‚îú‚îÄ‚îÄ 9.1: PFI Global Promedio (NUEVO)\n",
      "   ‚îú‚îÄ‚îÄ 9.2: Resumen - Perfil Multidimensional\n",
      "   ‚îú‚îÄ‚îÄ 9.3: Resumen - Impacto de Caracter√≠sticas (PFI Normalizado) (MODIFICADO)\n",
      "   ‚îî‚îÄ‚îÄ 10.1: PDP e ICE para el Mejor Modelo (x3) (NUEVO)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "DIR_SALIDA = \"./resultados_base_completa_mejorado\"\n",
    "\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR',\n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "\n",
    "# Colores √∫nicos para 9 modelos\n",
    "COLORES_MODELOS = {\n",
    "    'AREPD': '#e41a1c',\n",
    "    'AV-MCPS': '#377eb8',\n",
    "    'Block Bootstrapping': '#4daf4a',\n",
    "    'DeepAR': '#984ea3',\n",
    "    'EnCQR-LSTM': '#ff7f00',\n",
    "    'LSPM': '#ffff33',\n",
    "    'LSPMW': '#a65628',\n",
    "    'MCPS': '#f781bf',\n",
    "    'Sieve Bootstrap': '#999999'\n",
    "}\n",
    "\n",
    "# Caracter√≠sticas para el meta-modelo\n",
    "CARACTERISTICAS_META_MODELO = [\n",
    "    'Estacionario', 'Lineal', 'Tipo de Modelo',\n",
    "    'Distribuci√≥n', 'Varianza error', 'Paso'\n",
    "]\n",
    "CARACTERISTICAS_NUMERICAS_META_MODELO = ['Varianza error', 'Paso']\n",
    "CARACTERISTICAS_CATEGORICAS_META_MODELO = [\n",
    "    'Estacionario', 'Lineal', 'Tipo de Modelo', 'Distribuci√≥n'\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES AUXILIARES - TEST DIEBOLD-MARIANO (Sin cambios)\n",
    "# ============================================================================\n",
    "\n",
    "def diebold_mariano_test(errores1, errores2, h=1, alternative='two-sided'):\n",
    "    \"\"\"Test de Diebold-Mariano para comparar precisi√≥n de pron√≥sticos\"\"\"\n",
    "    e1 = np.asarray(errores1)\n",
    "    e2 = np.asarray(errores2)\n",
    "\n",
    "    if len(e1) != len(e2):\n",
    "        raise ValueError(\"Los vectores de errores deben tener la misma longitud\")\n",
    "\n",
    "    n = len(e1)\n",
    "    d = e1 - e2\n",
    "    d_mean = np.mean(d)\n",
    "\n",
    "    # Varianza con correcci√≥n de autocorrelaci√≥n\n",
    "    gamma_0 = np.var(d, ddof=1)\n",
    "    gamma_sum = 0\n",
    "    for k in range(1, h):\n",
    "        if k < n:\n",
    "            gamma_k = np.mean((d[:-k] - d_mean) * (d[k:] - d_mean))\n",
    "            gamma_sum += 2 * gamma_k\n",
    "\n",
    "    var_d = (gamma_0 + gamma_sum) / n\n",
    "\n",
    "    # Correcci√≥n de Harvey-Leybourne-Newbold\n",
    "    hlnc = np.sqrt((n + 1 - 2 * h + h * (h - 1) / n) / n)\n",
    "\n",
    "    if var_d > 0:\n",
    "        dm_stat = d_mean / np.sqrt(var_d)\n",
    "        dm_stat_corrected = dm_stat * hlnc\n",
    "    else:\n",
    "        dm_stat = 0\n",
    "        dm_stat_corrected = 0\n",
    "\n",
    "    # P-valor\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = 2 * (1 - stats.t.cdf(abs(dm_stat_corrected), df=n - 1))\n",
    "    elif alternative == 'less':\n",
    "        p_value = stats.t.cdf(dm_stat_corrected, df=n - 1)\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1 - stats.t.cdf(dm_stat_corrected, df=n - 1)\n",
    "    else:\n",
    "        raise ValueError(\"alternative debe ser 'two-sided', 'less' o 'greater'\")\n",
    "\n",
    "    return {\n",
    "        'dm_statistic': dm_stat,\n",
    "        'dm_statistic_corrected': dm_stat_corrected,\n",
    "        'p_value': p_value,\n",
    "        'mean_diff': d_mean,\n",
    "        'modelo1_mejor': d_mean < 0,\n",
    "        'n': n\n",
    "    }\n",
    "\n",
    "\n",
    "def comparaciones_multiples_dm(df, modelos, alpha=0.05):\n",
    "    \"\"\"Comparaciones m√∫ltiples con correcci√≥n de Bonferroni\"\"\"\n",
    "    n_comparaciones = len(list(combinations(modelos, 2)))\n",
    "    alpha_bonferroni = alpha / n_comparaciones\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for modelo1, modelo2 in combinations(modelos, 2):\n",
    "        try:\n",
    "            dm_result = diebold_mariano_test(\n",
    "                df[modelo1].values,\n",
    "                df[modelo2].values,\n",
    "                h=1,\n",
    "                alternative='two-sided'\n",
    "            )\n",
    "\n",
    "            significativo = dm_result['p_value'] < alpha_bonferroni\n",
    "\n",
    "            if significativo:\n",
    "                if dm_result['mean_diff'] < 0:\n",
    "                    ganador = modelo1\n",
    "                else:\n",
    "                    ganador = modelo2\n",
    "            else:\n",
    "                ganador = \"No hay diferencia\"\n",
    "\n",
    "            resultados.append({\n",
    "                'Modelo_1': modelo1,\n",
    "                'Modelo_2': modelo2,\n",
    "                'DM_Statistic': dm_result['dm_statistic_corrected'],\n",
    "                'p_value': dm_result['p_value'],\n",
    "                'p_value_bonferroni': alpha_bonferroni,\n",
    "                'Significativo': significativo,\n",
    "                'Ganador': ganador,\n",
    "                'Diff_Media': dm_result['mean_diff']\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(f\"Error en DM test entre {modelo1} y {modelo2}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(resultados), alpha_bonferroni\n",
    "\n",
    "\n",
    "def calcular_ranking_dm(df_comparaciones, modelos):\n",
    "    \"\"\"Calcula ranking basado en resultados DM\"\"\"\n",
    "    n = len(modelos)\n",
    "    matriz = pd.DataFrame(np.zeros((n, n)), index=modelos, columns=modelos)\n",
    "\n",
    "    for _, row in df_comparaciones.iterrows():\n",
    "        m1, m2 = row['Modelo_1'], row['Modelo_2']\n",
    "        if row['Significativo']:\n",
    "            if row['Ganador'] == m1:\n",
    "                matriz.loc[m1, m2] = 1\n",
    "                matriz.loc[m2, m1] = -1\n",
    "            elif row['Ganador'] == m2:\n",
    "                matriz.loc[m2, m1] = 1\n",
    "                matriz.loc[m1, m2] = -1\n",
    "\n",
    "    ranking_data = []\n",
    "    for modelo in modelos:\n",
    "        victorias = (matriz.loc[modelo] == 1).sum()\n",
    "        derrotas = (matriz.loc[modelo] == -1).sum()\n",
    "        empates = (matriz.loc[modelo] == 0).sum() - 1 # Excluir la comparaci√≥n consigo mismo\n",
    "        score = victorias - derrotas\n",
    "        total_comparaciones = victorias + derrotas + empates if (victorias + derrotas + empates) > 0 else 1 # Evitar division by zero\n",
    "        pct_victorias = (victorias / total_comparaciones * 100) if total_comparaciones > 0 else 0\n",
    "\n",
    "\n",
    "        ranking_data.append({\n",
    "            'Modelo': modelo,\n",
    "            'Victorias': int(victorias),\n",
    "            'Derrotas': int(derrotas),\n",
    "            'Empates': int(empates),\n",
    "            'Score': int(score),\n",
    "            'Pct_Victorias': round(pct_victorias, 2)\n",
    "        })\n",
    "\n",
    "    df_ranking = pd.DataFrame(ranking_data)\n",
    "    df_ranking = df_ranking.sort_values('Score', ascending=False).reset_index(drop=True)\n",
    "    df_ranking['Rank'] = range(1, len(df_ranking) + 1)\n",
    "\n",
    "    return df_ranking, matriz\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLASE PRINCIPAL DE AN√ÅLISIS - MEJORADA\n",
    "# ============================================================================\n",
    "\n",
    "class AnalizadorBaseCompleta:\n",
    "    \"\"\"An√°lisis completo de la base de datos en 8 dimensiones + PFI/PDP/ICE\"\"\"\n",
    "\n",
    "    def __init__(self, ruta_datos):\n",
    "        \"\"\"Inicializa el analizador\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"INICIANDO AN√ÅLISIS COMPLETO DE BASE DE DATOS - VERSI√ìN MEJORADA\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        self.df = pd.read_excel(ruta_datos)\n",
    "        self.modelos = MODELOS\n",
    "        self.dir_salida = Path(DIR_SALIDA)\n",
    "        self.dir_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Extraer caracter√≠sticas del escenario\n",
    "        self._extraer_caracteristicas()\n",
    "\n",
    "        # Preprocesar datos para meta-modelo\n",
    "        self.preprocessor, self.X_processed = self._preprocess_meta_features()\n",
    "        self.meta_models = {} # Almacenar meta-modelos entrenados\n",
    "        self.pfi_results = {} # Almacenar resultados de PFI\n",
    "\n",
    "        print(f\"‚úì Datos cargados: {self.df.shape[0]} filas, {self.df.shape[1]} columnas\")\n",
    "        print(f\"‚úì Modelos a analizar: {len(self.modelos)}\")\n",
    "        print(f\"‚úì Directorio de salida: {self.dir_salida}\")\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    def _extraer_caracteristicas(self):\n",
    "        \"\"\"Extrae caracter√≠sticas individuales del escenario (sin cambios)\"\"\"\n",
    "        self.df['Estacionario'] = self.df['Escenario'].apply(\n",
    "            lambda x: 'Estacionario' if 'Estacionario' in x and 'No_Estacionario' not in x else 'No Estacionario'\n",
    "        )\n",
    "\n",
    "        self.df['Lineal'] = self.df['Escenario'].apply(\n",
    "            lambda x: 'Lineal' if 'Lineal' in x and 'No_Lineal' not in x else 'No Lineal'\n",
    "        )\n",
    "\n",
    "        print(\"‚úì Caracter√≠sticas extra√≠das:\")\n",
    "        print(f\"  - Estacionariedad: {self.df['Estacionario'].unique()}\")\n",
    "        print(f\"  - Linealidad: {self.df['Lineal'].unique()}\")\n",
    "        print(f\"  - Tipos de Modelo: {self.df['Tipo de Modelo'].unique()}\")\n",
    "        print(f\"  - Distribuciones: {self.df['Distribuci√≥n'].unique()}\")\n",
    "        print(f\"  - Varianzas: {sorted(self.df['Varianza error'].unique())}\")\n",
    "        print(f\"  - Pasos: {sorted(self.df['Paso'].unique())}\")\n",
    "\n",
    "    def _preprocess_meta_features(self):\n",
    "        \"\"\"\n",
    "        Preprocesa las caracter√≠sticas para el meta-modelo (OneHotEncoding para categ√≥ricas).\n",
    "        \"\"\"\n",
    "        numeric_features = CARACTERISTICAS_NUMERICAS_META_MODELO\n",
    "        categorical_features = CARACTERISTICAS_CATEGORICAS_META_MODELO\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', 'passthrough', numeric_features),\n",
    "                # A√ëADIR sparse_output=False AQU√ç PARA SOLUCIONAR EL ERROR\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "        \n",
    "        # Ajustar y transformar X\n",
    "        X = self.df[CARACTERISTICAS_META_MODELO]\n",
    "        X_processed_array = preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Obtener nombres de las caracter√≠sticas preprocesadas para PFI/PDP\n",
    "        # Se debe usar get_feature_names_out para versiones recientes de sklearn\n",
    "        try:\n",
    "            ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "        except AttributeError:\n",
    "             # Fallback para versiones m√°s antiguas\n",
    "            ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names(categorical_features)\n",
    "            \n",
    "        feature_names = numeric_features + list(ohe_feature_names)\n",
    "\n",
    "        # Ahora la creaci√≥n del DataFrame funcionar√°\n",
    "        return preprocessor, pd.DataFrame(X_processed_array, columns=feature_names)\n",
    "\n",
    "\n",
    "    def _train_meta_model(self, target_model_name):\n",
    "        \"\"\"\n",
    "        Entrena un RandomForestRegressor para predecir el error de un modelo de pron√≥stico\n",
    "        basado en las caracter√≠sticas de la simulaci√≥n.\n",
    "        \"\"\"\n",
    "        if target_model_name in self.meta_models:\n",
    "            return self.meta_models[target_model_name]\n",
    "\n",
    "        print(f\"   Entrenando meta-modelo para {target_model_name}...\")\n",
    "        y = self.df[target_model_name]\n",
    "        \n",
    "        # Usamos un pipeline simplificado para que PFI/PDP puedan trabajar con el preprocesador\n",
    "        # directamente si fuera necesario, aunque aqu√≠ ya pasamos X_processed.\n",
    "        # En este caso, el preprocessor ya se us√≥ para obtener X_processed.\n",
    "        # Creamos solo el modelo RandomForestRegressor.\n",
    "        \n",
    "        meta_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        meta_model.fit(self.X_processed, y)\n",
    "        self.meta_models[target_model_name] = meta_model\n",
    "        print(f\"   ‚úì Meta-modelo entrenado para {target_model_name}.\")\n",
    "        return meta_model\n",
    "\n",
    "    def ejecutar_analisis_completo(self):\n",
    "        \"\"\"Ejecuta todos los an√°lisis, incluyendo los nuevos\"\"\"\n",
    "        print(\"\\n\" + \"üî¨\" * 40 + \"\\n\")\n",
    "\n",
    "        # 1. Impacto de Estacionariedad\n",
    "        print(\"1Ô∏è‚É£  Analizando impacto de Estacionariedad...\")\n",
    "        self._analisis_estacionariedad()\n",
    "\n",
    "        # 2. Impacto de Linealidad\n",
    "        print(\"2Ô∏è‚É£  Analizando impacto de Linealidad...\")\n",
    "        self._analisis_linealidad()\n",
    "\n",
    "        # 3. Efecto del Modelo Generador\n",
    "        print(\"3Ô∏è‚É£  Analizando efecto del Modelo Generador...\")\n",
    "        self._analisis_modelo_generador()\n",
    "\n",
    "        # 4. Influencia de Distribuci√≥n\n",
    "        print(\"4Ô∏è‚É£  Analizando influencia de Distribuci√≥n...\")\n",
    "        self._analisis_distribucion()\n",
    "\n",
    "        # 5. Impacto de Varianza\n",
    "        print(\"5Ô∏è‚É£  Analizando impacto de Varianza...\")\n",
    "        self._analisis_varianza()\n",
    "\n",
    "        # 6. Deterioro por Horizonte\n",
    "        print(\"6Ô∏è‚É£  Analizando deterioro por Horizonte...\")\n",
    "        self._analisis_horizonte()\n",
    "\n",
    "        # 7. Robustez y Estabilidad\n",
    "        print(\"7Ô∏è‚É£  Analizando Robustez y Estabilidad...\")\n",
    "        self._analisis_robustez()\n",
    "\n",
    "        # 8. Diferencias Estad√≠sticamente Significativas\n",
    "        print(\"8Ô∏è‚É£  Analizando Diferencias Estad√≠sticamente Significativas...\")\n",
    "        self._analisis_significancia()\n",
    "\n",
    "        # NUEVO: 9. An√°lisis de Impacto de Caracter√≠sticas con PFI\n",
    "        print(\"\\n9Ô∏è‚É£  Analizando Impacto de Caracter√≠sticas (Permutation Importance)...\")\n",
    "        self._analisis_impacto_pfi()\n",
    "\n",
    "        # NUEVO: 10. An√°lisis de Variabilidad con PDP e ICE\n",
    "        print(\"\\nüîü Analizando Variabilidad (PDP e ICE)...\")\n",
    "        self._analisis_variabilidad_pdp_ice()\n",
    "\n",
    "        # Resumen ejecutivo\n",
    "        print(\"\\n‚ú® Generando Resumen Ejecutivo...\")\n",
    "        self._generar_resumen_ejecutivo()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ AN√ÅLISIS COMPLETO FINALIZADO\")\n",
    "        print(f\"üìÅ Resultados guardados en: {self.dir_salida}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 1. IMPACTO DE ESTACIONARIEDAD (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_estacionariedad(self):\n",
    "        \"\"\"Analiza el impacto de la estacionariedad\"\"\"\n",
    "\n",
    "        # Calcular estad√≠sticas por estacionariedad\n",
    "        stats_est = []\n",
    "        for modelo in self.modelos:\n",
    "            for est in ['Estacionario', 'No Estacionario']:\n",
    "                df_subset = self.df[self.df['Estacionario'] == est]\n",
    "                stats_est.append({\n",
    "                    'Modelo': modelo,\n",
    "                    'Estacionariedad': est,\n",
    "                    'Media': df_subset[modelo].mean(),\n",
    "                    'Std': df_subset[modelo].std(),\n",
    "                    'Mediana': df_subset[modelo].median()\n",
    "                })\n",
    "\n",
    "        df_stats = pd.DataFrame(stats_est)\n",
    "\n",
    "        # FIGURA 1.1: Barras comparativas\n",
    "        fig, ax = plt.subplots(figsize=(14, 9))\n",
    "        pivot_media = df_stats.pivot(index='Modelo', columns='Estacionariedad', values='Media')\n",
    "        pivot_media = pivot_media.sort_values('Estacionario')\n",
    "\n",
    "        x = np.arange(len(pivot_media))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width / 2, pivot_media['Estacionario'], width,\n",
    "               label='Estacionario', color='lightblue', edgecolor='black', linewidth=1.5)\n",
    "        ax.bar(x + width / 2, pivot_media['No Estacionario'], width,\n",
    "               label='No Estacionario', color='lightcoral', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "        ax.set_xlabel('Modelos', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Impacto de Estacionariedad: ECRPS Comparativo',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(pivot_media.index, rotation=45, ha='right', fontsize=11)\n",
    "        ax.legend(fontsize=11, loc='best')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '1_1_estacionariedad_comparativo.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 1.2: Cambio relativo (barras horizontales)\n",
    "        fig, ax = plt.subplots(figsize=(12, 9))\n",
    "        cambio_rel = ((pivot_media['No Estacionario'] - pivot_media['Estacionario']) /\n",
    "                      pivot_media['Estacionario'] * 100)\n",
    "        cambio_rel = cambio_rel.sort_values()\n",
    "\n",
    "        colors = ['green' if x < 0 else 'red' for x in cambio_rel.values]\n",
    "        bars = ax.barh(cambio_rel.index, cambio_rel.values, color=colors,\n",
    "                       alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "        ax.set_xlabel('Cambio Relativo (%)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Modelos', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Deterioro en Datos No Estacionarios\\n(Negativo = Mejora, Positivo = Deterioro)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        for i, (bar, val) in enumerate(zip(bars, cambio_rel.values)):\n",
    "            ax.text(val + (3 if val > 0 else -3), i, f'{val:.1f}%',\n",
    "                    va='center', ha='left' if val > 0 else 'right',\n",
    "                    fontweight='bold', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '1_2_estacionariedad_cambio_relativo.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para estacionariedad\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. IMPACTO DE LINEALIDAD (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_linealidad(self):\n",
    "        \"\"\"Analiza el impacto de la linealidad\"\"\"\n",
    "\n",
    "        # Calcular estad√≠sticas\n",
    "        stats_lin = []\n",
    "        for modelo in self.modelos:\n",
    "            for lin in ['Lineal', 'No Lineal']:\n",
    "                df_subset = self.df[self.df['Lineal'] == lin]\n",
    "                stats_lin.append({\n",
    "                    'Modelo': modelo,\n",
    "                    'Linealidad': lin,\n",
    "                    'Media': df_subset[modelo].mean(),\n",
    "                    'Std': df_subset[modelo].std(),\n",
    "                    'Mediana': df_subset[modelo].median()\n",
    "                })\n",
    "\n",
    "        df_stats = pd.DataFrame(stats_lin)\n",
    "\n",
    "        # FIGURA 2.1: Barras comparativas\n",
    "        fig, ax = plt.subplots(figsize=(14, 9))\n",
    "        pivot_media = df_stats.pivot(index='Modelo', columns='Linealidad', values='Media')\n",
    "        pivot_media = pivot_media.sort_values('Lineal')\n",
    "\n",
    "        x = np.arange(len(pivot_media))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width / 2, pivot_media['Lineal'], width,\n",
    "               label='Lineal', color='lightgreen', edgecolor='black', linewidth=1.5)\n",
    "        ax.bar(x + width / 2, pivot_media['No Lineal'], width,\n",
    "               label='No Lineal', color='orange', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "        ax.set_xlabel('Modelos', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Impacto de Linealidad: ECRPS Comparativo',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(pivot_media.index, rotation=45, ha='right', fontsize=11)\n",
    "        ax.legend(fontsize=11, loc='best')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '2_1_linealidad_comparativo.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 2.2: Cambio relativo\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        cambio_rel = ((pivot_media['No Lineal'] - pivot_media['Lineal']) /\n",
    "                      pivot_media['Lineal'] * 100)\n",
    "        cambio_rel = cambio_rel.sort_values()\n",
    "\n",
    "        colors = ['green' if x < 0 else 'red' for x in cambio_rel.values]\n",
    "        bars = ax.barh(cambio_rel.index, cambio_rel.values, color=colors,\n",
    "                       alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "        ax.set_xlabel('Cambio Relativo (%)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Modelos', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Deterioro en Datos No Lineales\\n(Negativo = Mejora, Positivo = Deterioro)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        # Ajustar m√°rgenes del eje x para dar espacio a las etiquetas\n",
    "        x_min = min(cambio_rel.values)\n",
    "        x_max = max(cambio_rel.values)\n",
    "        x_range = x_max - x_min\n",
    "        ax.set_xlim(x_min - x_range * 0.15, x_max + x_range * 0.15)\n",
    "\n",
    "        for i, (bar, val) in enumerate(zip(bars, cambio_rel.values)):\n",
    "            offset = x_range * 0.02\n",
    "            ax.text(val + (offset if val > 0 else -offset), i, f'{val:.1f}%',\n",
    "                    va='center', ha='left' if val > 0 else 'right',\n",
    "                    fontweight='bold', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '2_2_linealidad_cambio_relativo.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para linealidad\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. EFECTO DEL MODELO GENERADOR (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_modelo_generador(self):\n",
    "        \"\"\"Analiza el efecto del modelo generador de datos\"\"\"\n",
    "\n",
    "        pivot_media = self.df.groupby('Tipo de Modelo')[self.modelos].mean()\n",
    "        tipos = self.df['Tipo de Modelo'].unique()\n",
    "\n",
    "        # FIGURA 3.2: Heatmap normalizado (Z-scores)\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "        pivot_norm = pivot_media.T.sub(pivot_media.T.mean(axis=1), axis=0).div(pivot_media.T.std(axis=1), axis=0)\n",
    "\n",
    "        sns.heatmap(pivot_norm, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "                    ax=ax, cbar_kws={'label': 'Z-Score'},\n",
    "                    linewidths=0.5, linecolor='gray', vmin=-2, vmax=2)\n",
    "        ax.set_xlabel('Tipo de Modelo Generador', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Modelo de Predicci√≥n', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('ECRPS Relativo (Z-Score por Modelo)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax.tick_params(axis='y', rotation=0, labelsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '3_2_modelo_generador_zscore.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 3.3: Variabilidad por tipo\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        rankings = []\n",
    "        for tipo in tipos:\n",
    "            df_tipo = self.df[self.df['Tipo de Modelo'] == tipo]\n",
    "            medias = df_tipo[self.modelos].mean().sort_values()\n",
    "            rankings.append({\n",
    "                'Tipo': tipo,\n",
    "                'Mejor_Modelo': medias.index[0],\n",
    "                'Mejor_ECRPS': medias.values[0],\n",
    "                'Peor_Modelo': medias.index[-1],\n",
    "                'Peor_ECRPS': medias.values[-1],\n",
    "                'Rango': medias.values[-1] - medias.values[0]\n",
    "            })\n",
    "\n",
    "        df_rankings = pd.DataFrame(rankings).sort_values('Rango', ascending=False)\n",
    "\n",
    "        y_pos = np.arange(len(df_rankings))\n",
    "        bars = ax.barh(y_pos, df_rankings['Rango'].values,\n",
    "                       color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(df_rankings['Tipo'].values, fontsize=10)\n",
    "        ax.set_xlabel('Rango de ECRPS (Max - Min)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Variabilidad por Tipo de Generador',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        for i, (bar, val) in enumerate(zip(bars, df_rankings['Rango'].values)):\n",
    "            ax.text(val + 0.001, i, f'{val:.3f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '3_3_modelo_generador_variabilidad.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para modelo generador\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 4. INFLUENCIA DE LA DISTRIBUCI√ìN (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_distribucion(self):\n",
    "        \"\"\"Analiza la influencia de la distribuci√≥n de errores\"\"\"\n",
    "\n",
    "        pivot_media = self.df.groupby('Distribuci√≥n')[self.modelos].mean()\n",
    "        pivot_std = self.df.groupby('Distribuci√≥n')[self.modelos].std()\n",
    "\n",
    "        # FIGURA 4.1: Heatmap de rendimiento\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "        sns.heatmap(pivot_media.T, annot=True, fmt='.3f', cmap='RdYlGn_r',\n",
    "                    ax=ax, cbar_kws={'label': 'ECRPS Promedio'},\n",
    "                    linewidths=0.5, linecolor='gray')\n",
    "        ax.set_xlabel('Distribuci√≥n de Errores', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Modelo de Predicci√≥n', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('ECRPS por Distribuci√≥n de Errores',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax.tick_params(axis='y', rotation=0, labelsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '4_1_distribucion_heatmap_rendimiento.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 4.2: Heatmap de variabilidad\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "        sns.heatmap(pivot_std.T, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                    ax=ax, cbar_kws={'label': 'Desviaci√≥n Est√°ndar'},\n",
    "                    linewidths=0.5, linecolor='gray')\n",
    "        ax.set_xlabel('Distribuci√≥n de Errores', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Modelo de Predicci√≥n', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Variabilidad por Distribuci√≥n de Errores',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax.tick_params(axis='y', rotation=0, labelsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '4_2_distribucion_heatmap_variabilidad.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para distribuci√≥n\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 5. IMPACTO DE VARIANZA (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_varianza(self):\n",
    "        \"\"\"Analiza el impacto del nivel de varianza (ruido)\"\"\"\n",
    "\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "\n",
    "        # FIGURA 5.1: L√≠neas de tendencia\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "        for modelo in self.modelos:\n",
    "            medias = [self.df[self.df['Varianza error'] == v][modelo].mean()\n",
    "                      for v in varianzas]\n",
    "            ax.plot(varianzas, medias, marker='o', label=modelo,\n",
    "                    linewidth=2.5, markersize=8, color=COLORES_MODELOS[modelo])\n",
    "\n",
    "        ax.set_xlabel('Nivel de Varianza', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Deterioro con Aumento de Varianza',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(varianzas)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '5_1_varianza_tendencias.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 5.2: Tasa de crecimiento\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        tasas_crecimiento = {}\n",
    "        for modelo in self.modelos:\n",
    "            medias = [self.df[self.df['Varianza error'] == v][modelo].mean()\n",
    "                      for v in varianzas]\n",
    "            if len(medias) > 1:\n",
    "                pendiente = (medias[-1] - medias[0]) / (varianzas[-1] - varianzas[0])\n",
    "                tasas_crecimiento[modelo] = pendiente\n",
    "\n",
    "        tc_sorted = dict(sorted(tasas_crecimiento.items(), key=lambda x: x[1]))\n",
    "\n",
    "        colors_tc = ['green' if v < np.median(list(tc_sorted.values())) else 'red'\n",
    "                     for v in tc_sorted.values()]\n",
    "        bars = ax.barh(range(len(tc_sorted)), list(tc_sorted.values()),\n",
    "                       color=colors_tc, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax.set_yticks(range(len(tc_sorted)))\n",
    "        ax.set_yticklabels(list(tc_sorted.keys()), fontsize=10)\n",
    "        ax.set_xlabel('Tasa de Crecimiento del Error', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Sensibilidad al Ruido\\n(Menor = M√°s Robusto)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.axvline(np.median(list(tc_sorted.values())), color='black',\n",
    "                   linestyle='--', linewidth=2, label='Mediana')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.legend(fontsize=11)\n",
    "\n",
    "        for i, (bar, val) in enumerate(zip(bars, tc_sorted.values())):\n",
    "            ax.text(val + (0.0001 if val > 0 else -0.0001), i, f'{val:.4f}',\n",
    "                    va='center', ha='left' if val > 0 else 'right',\n",
    "                    fontweight='bold', fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '5_2_varianza_tasa_crecimiento.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para varianza\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 6. DETERIORO POR HORIZONTE (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_horizonte(self):\n",
    "        \"\"\"Analiza el deterioro del rendimiento con el horizonte de predicci√≥n\"\"\"\n",
    "\n",
    "        pasos = sorted(self.df['Paso'].unique())\n",
    "\n",
    "        # FIGURA 6.1: Evoluci√≥n paso a paso\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "        for modelo in self.modelos:\n",
    "            medias = [self.df[self.df['Paso'] == p][modelo].mean() for p in pasos]\n",
    "            ax.plot(pasos, medias, marker='o', label=modelo,\n",
    "                    linewidth=2.5, markersize=8, color=COLORES_MODELOS[modelo])\n",
    "\n",
    "        ax.set_xlabel('Horizonte de Predicci√≥n (Paso)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Evoluci√≥n del ECRPS por Horizonte',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(pasos)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '6_1_horizonte_evolucion.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 6.2: Tasa de deterioro\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        tasas_deterioro = {}\n",
    "        for modelo in self.modelos:\n",
    "            medias = [self.df[self.df['Paso'] == p][modelo].mean() for p in pasos]\n",
    "            if len(medias) > 1:\n",
    "                pendiente = (medias[-1] - medias[0]) / (pasos[-1] - pasos[0])\n",
    "                tasas_deterioro[modelo] = pendiente\n",
    "\n",
    "        td_sorted = dict(sorted(tasas_deterioro.items(), key=lambda x: x[1]))\n",
    "\n",
    "        colors_td = ['green' if v < np.median(list(td_sorted.values())) else 'red'\n",
    "                     for v in td_sorted.values()]\n",
    "        bars = ax.barh(range(len(td_sorted)), list(td_sorted.values()),\n",
    "                       color=colors_td, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax.set_yticks(range(len(td_sorted)))\n",
    "        ax.set_yticklabels(list(td_sorted.keys()), fontsize=10)\n",
    "        ax.set_xlabel('Tasa de Deterioro por Paso', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Velocidad de Deterioro\\n(Menor = M√°s Estable)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        for i, (bar, val) in enumerate(zip(bars, td_sorted.values())):\n",
    "            ax.text(val + (0.0001 if val > 0 else -0.0001), i, f'{val:.4f}',\n",
    "                    va='center', ha='left' if val > 0 else 'right',\n",
    "                    fontweight='bold', fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '6_2_horizonte_tasa_deterioro.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para horizonte\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 7. ROBUSTEZ Y ESTABILIDAD (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_robustez(self):\n",
    "        \"\"\"Analiza la robustez y estabilidad de los modelos\"\"\"\n",
    "\n",
    "        # Calcular m√©tricas de robustez\n",
    "        metricas_robustez = []\n",
    "\n",
    "        for modelo in self.modelos:\n",
    "            std_global = self.df[modelo].std()\n",
    "            cv = (self.df[modelo].std() / self.df[modelo].mean()) * 100\n",
    "            q75, q25 = self.df[modelo].quantile([0.75, 0.25])\n",
    "            iqr = q75 - q25\n",
    "            std_entre_escenarios = self.df.groupby('Escenario')[modelo].mean().std()\n",
    "            std_entre_dist = self.df.groupby('Distribuci√≥n')[modelo].mean().std()\n",
    "            std_entre_var = self.df.groupby('Varianza error')[modelo].mean().std()\n",
    "\n",
    "            metricas_robustez.append({\n",
    "                'Modelo': modelo,\n",
    "                'Std_Global': std_global,\n",
    "                'CV': cv,\n",
    "                'IQR': iqr,\n",
    "                'Std_Escenarios': std_entre_escenarios,\n",
    "                'Std_Distribuciones': std_entre_dist,\n",
    "                'Std_Varianzas': std_entre_var\n",
    "            })\n",
    "\n",
    "        df_robustez = pd.DataFrame(metricas_robustez)\n",
    "\n",
    "        # FIGURA 7.2: Coeficiente de variaci√≥n\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        df_sorted = df_robustez.sort_values('CV')\n",
    "        colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, len(df_sorted)))\n",
    "        bars = ax.barh(df_sorted['Modelo'], df_sorted['CV'],\n",
    "                       color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "        ax.set_xlabel('Coeficiente de Variaci√≥n (%)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Variabilidad Relativa\\n(Menor = M√°s Consistente)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        for i, (bar, val) in enumerate(zip(bars, df_sorted['CV'].values)):\n",
    "            ax.text(val + 1, i, f'{val:.1f}%', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '7_2_robustez_coef_variacion.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Guardar para usar despu√©s\n",
    "        self.df_robustez = df_robustez\n",
    "\n",
    "        print(\"   ‚úì 1 figura generada para robustez\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 8. DIFERENCIAS ESTAD√çSTICAMENTE SIGNIFICATIVAS (Sin cambios funcionales, solo prints)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_significancia(self):\n",
    "        \"\"\"An√°lisis de diferencias estad√≠sticamente significativas con Test DM\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"REALIZANDO TEST DE DIEBOLD-MARIANO\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        # Realizar comparaciones m√∫ltiples\n",
    "        df_comparaciones, alpha_bonf = comparaciones_multiples_dm(\n",
    "            self.df, self.modelos, alpha=0.05\n",
    "        )\n",
    "\n",
    "        print(f\"   N√∫mero de comparaciones: {len(df_comparaciones)}\")\n",
    "        print(f\"   Alpha corregido (Bonferroni): {alpha_bonf:.6f}\")\n",
    "        print(f\"   Comparaciones significativas: {df_comparaciones['Significativo'].sum()}\")\n",
    "\n",
    "        # Calcular ranking\n",
    "        df_ranking, matriz_sup = calcular_ranking_dm(df_comparaciones, self.modelos)\n",
    "\n",
    "        # FIGURA 8.2: Matriz de superioridad\n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "        sns.heatmap(matriz_sup, annot=True, fmt='.0f', cmap='RdYlGn',\n",
    "                    center=0, ax=ax, cbar_kws={'label': 'Superioridad'},\n",
    "                    vmin=-1, vmax=1, linewidths=1, linecolor='gray',\n",
    "                    annot_kws={'fontsize': 10, 'fontweight': 'bold'})\n",
    "        ax.set_title('Matriz de Superioridad\\n(1=Superior, -1=Inferior, 0=Sin diferencia)',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.set_xlabel('Modelo Comparado', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Modelo', fontsize=12, fontweight='bold')\n",
    "        ax.tick_params(labelsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '8_2_significancia_matriz_superioridad.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Guardar para usar despu√©s\n",
    "        self.df_ranking = df_ranking\n",
    "        self.df_comparaciones = df_comparaciones\n",
    "\n",
    "        print(f\"\\n   ‚úì Ranking guardado: Top 3\")\n",
    "        for i, row in df_ranking.head(3).iterrows():\n",
    "            print(f\"      {row['Rank']}. {row['Modelo']} - Score: {row['Score']} \"\n",
    "                  f\"(V:{row['Victorias']}, D:{row['Derrotas']}, E:{row['Empates']})\")\n",
    "\n",
    "        print(\"\\n   ‚úì 1 figura generada para significancia\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 9. NUEVO: AN√ÅLISIS DE IMPACTO DE CARACTER√çSTICAS CON PFI\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_impacto_pfi(self):\n",
    "        \"\"\"\n",
    "        Calcula la Permutation Feature Importance (PFI) para cada modelo\n",
    "        y guarda los resultados.\n",
    "        \"\"\"\n",
    "        all_pfi_scores = {}\n",
    "\n",
    "        for modelo in self.modelos:\n",
    "            print(f\"   Calculando PFI para el modelo: {modelo}\")\n",
    "            meta_model = self._train_meta_model(modelo)\n",
    "            \n",
    "            # Utilizar permutation_importance de sklearn\n",
    "            # n_repeats es el n√∫mero de veces que se permuta una caracter√≠stica\n",
    "            # random_state para reproducibilidad\n",
    "            # scoring: 'neg_mean_squared_error' si es regresi√≥n y queremos minimizar error, \n",
    "            #           'r2' si queremos maximizar r2. Para errores, el valor m√°s bajo es mejor, \n",
    "            #           as√≠ que un scoring que aumente con un peor modelo es m√°s intuitivo para el \"impacto\".\n",
    "            #           Si el error es MSE, neg_mean_squared_error es adecuado.\n",
    "            \n",
    "            result = permutation_importance(\n",
    "                meta_model, self.X_processed, self.df[modelo],\n",
    "                n_repeats=10, random_state=42, n_jobs=-1,\n",
    "                scoring='neg_mean_squared_error' # Usar neg_mean_squared_error\n",
    "            )\n",
    "            \n",
    "            # Los valores importances_mean ya est√°n en la escala de la m√©trica de scoring.\n",
    "            # Como usamos neg_mean_squared_error, un valor m√°s negativo significa mejor rendimiento.\n",
    "            # La importancia es la disminuci√≥n en el scoring cuando la caracter√≠stica se permuta.\n",
    "            # Una mayor disminuci√≥n (m√°s positiva) significa m√°s importancia.\n",
    "            \n",
    "            sorted_idx = result.importances_mean.argsort()[::-1] # Ordenar de mayor a menor importancia\n",
    "\n",
    "            model_pfi = {}\n",
    "            for i in sorted_idx:\n",
    "                feature_name = self.X_processed.columns[i]\n",
    "                model_pfi[feature_name] = result.importances_mean[i]\n",
    "\n",
    "            all_pfi_scores[modelo] = model_pfi\n",
    "            print(f\"   ‚úì PFI calculado para {modelo}\\n\")\n",
    "\n",
    "        self.pfi_results = all_pfi_scores\n",
    "        print(\"   ‚úì PFI calculado para todos los modelos.\")\n",
    "\n",
    "        # Opcional: Graficar el PFI promedio global\n",
    "        if self.pfi_results:\n",
    "            # Calcular el PFI promedio para cada caracter√≠stica en todos los modelos\n",
    "            avg_pfi = pd.DataFrame(self.pfi_results).mean(axis=1).sort_values(ascending=True)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            bars = ax.barh(avg_pfi.index, avg_pfi.values, color='steelblue', alpha=0.7)\n",
    "            ax.set_xlabel('Importancia de Permutaci√≥n (Disminuci√≥n en Neg. MSE)', fontweight='bold', fontsize=12)\n",
    "            ax.set_title('Importancia Global de Caracter√≠sticas (Promedio PFI)', fontweight='bold', fontsize=14, pad=20)\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "            for i, (bar, val) in enumerate(zip(bars, avg_pfi.values)):\n",
    "                ax.text(val, i, f' {val:.4f}', va='center', ha='left', fontweight='bold', fontsize=10)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.dir_salida / '9_1_pfi_global_promedio.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"   ‚úì 1 figura (PFI Global Promedio) generada para impacto de caracter√≠sticas.\\n\")\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # 10. NUEVO: AN√ÅLISIS DE VARIABILIDAD CON PDP e ICE\n",
    "    # ========================================================================\n",
    "\n",
    "    def _analisis_variabilidad_pdp_ice(self):\n",
    "        \"\"\"\n",
    "        Genera Partial Dependence Plots (PDP) e Individual Conditional Expectation (ICE) plots\n",
    "        para el modelo con mejor ranking y sus caracter√≠sticas m√°s importantes.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'df_ranking') or self.df_ranking.empty:\n",
    "            print(\"   ‚ö†Ô∏è No se encontr√≥ el ranking de modelos. Saltando PDP/ICE.\")\n",
    "            return\n",
    "\n",
    "        # Seleccionar el modelo con el mejor ranking DM\n",
    "        best_model_name = self.df_ranking.iloc[0]['Modelo']\n",
    "        print(f\"   Generando PDP/ICE para el modelo con mejor ranking: {best_model_name}\")\n",
    "\n",
    "        meta_model = self._train_meta_model(best_model_name)\n",
    "\n",
    "        # Obtener las caracter√≠sticas m√°s importantes de PFI para este modelo\n",
    "        if best_model_name in self.pfi_results:\n",
    "            sorted_features = list(self.pfi_results[best_model_name].keys())\n",
    "            # Tomar las top N caracter√≠sticas para visualizaci√≥n\n",
    "            top_features_for_pdp = sorted_features[:min(3, len(sorted_features))]\n",
    "            \n",
    "            # Asegurarse de que las caracter√≠sticas elegidas para PDP existen en X_processed\n",
    "            # Y que no sean caracter√≠sticas one-hot-encoded si queremos el nombre original\n",
    "            \n",
    "            # Mapear nombres de caracter√≠sticas de OneHotEncoder a sus originales\n",
    "            original_feature_names = CARACTERISTICAS_META_MODELO\n",
    "            processed_feature_names = self.X_processed.columns\n",
    "            \n",
    "            features_to_plot = []\n",
    "            for f in top_features_for_pdp:\n",
    "                # Si es una caracter√≠stica num√©rica, se usa directamente\n",
    "                if f in CARACTERISTICAS_NUMERICAS_META_MODELO:\n",
    "                    features_to_plot.append(f)\n",
    "                # Si es una caracter√≠stica categ√≥rica (posiblemente one-hot encoded), \n",
    "                # Intentar mapearla a su nombre original.\n",
    "                else:\n",
    "                    for orig_cat_feat in CARACTERISTICAS_CATEGORICAS_META_MODELO:\n",
    "                        if f.startswith(orig_cat_feat + '_'): # Es una columna one-hot\n",
    "                            if orig_cat_feat not in features_to_plot: # A√±adir solo la caracter√≠stica original una vez\n",
    "                                features_to_plot.append(orig_cat_feat)\n",
    "                            break\n",
    "                    else: # Si no se encontr√≥ como categ√≥rica original, a√±adir tal cual\n",
    "                        if f not in features_to_plot:\n",
    "                            features_to_plot.append(f)\n",
    "            \n",
    "            # Asegurarse de que no haya duplicados y que sean caracter√≠sticas v√°lidas para PDP\n",
    "            features_to_plot = list(dict.fromkeys(features_to_plot)) # Eliminar duplicados manteniendo el orden\n",
    "            \n",
    "            # Filtrar a solo caracter√≠sticas presentes en CARACTERISTICAS_META_MODELO (las originales)\n",
    "            features_to_plot = [f for f in features_to_plot if f in CARACTERISTICAS_META_MODELO]\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No se encontraron resultados PFI para {best_model_name}. Usando caracter√≠sticas predefinidas.\")\n",
    "            features_to_plot = ['Varianza error', 'Paso', 'Distribuci√≥n'] # fallback\n",
    "\n",
    "        # Convertir caracter√≠sticas categ√≥ricas originales a √≠ndices para PDP si no est√°n en X_processed\n",
    "        # PartialDependenceDisplay puede manejar ColumnTransformer si se pasa el pipeline completo.\n",
    "        # Aqu√≠, usaremos X_processed y el modelo directamente, por lo que las categor√≠as deben ser tratadas.\n",
    "        # Para caracter√≠sticas categ√≥ricas, PDP puede agrupar autom√°ticamente si se le da el nombre de la columna original\n",
    "        # y si X es el DataFrame original pre-transformado.\n",
    "        \n",
    "        # Para simplificar la visualizaci√≥n con PartialDependenceDisplay y el X_processed\n",
    "        # Se puede intentar plotear las columnas one-hot si son las m√°s importantes\n",
    "        # o agruparlas de nuevo al nombre original.\n",
    "        # Aqu√≠ intentaremos plotear las caracter√≠sticas originales del CARACTERISTICAS_META_MODELO\n",
    "\n",
    "        pdp_indices = []\n",
    "        for feat_name in features_to_plot:\n",
    "            if feat_name in CARACTERISTICAS_NUMERICAS_META_MODELO:\n",
    "                # Obtener el √≠ndice de la columna num√©rica en X_processed\n",
    "                try:\n",
    "                    pdp_indices.append(self.X_processed.columns.get_loc(feat_name))\n",
    "                except KeyError:\n",
    "                    # Fallback si el nombre exacto de la columna num√©rica no est√° en X_processed por alguna raz√≥n\n",
    "                    pdp_indices.append(CARACTERISTICAS_NUMERICAS_META_MODELO.index(feat_name))\n",
    "\n",
    "            elif feat_name in CARACTERISTICAS_CATEGORICAS_META_MODELO:\n",
    "                # Para categ√≥ricas, PartialDependenceDisplay puede tomar el nombre de la caracter√≠stica original\n",
    "                # si se le pasa el ColumnTransformer y el pipeline completo.\n",
    "                # Como ya preprocesamos X_processed, necesitamos identificar las columnas OHE correspondientes\n",
    "                # para agruparlas, o pasar el preprocessor al display.\n",
    "                # Usaremos la estrategia de identificar las columnas OHE.\n",
    "                \n",
    "                # En este caso particular, PartialDependenceDisplay es m√°s f√°cil de usar si se le pasa\n",
    "                # el objeto preprocessor y el modelo dentro de un pipeline.\n",
    "                # Adaptaremos la llamada.\n",
    "\n",
    "                # Si es categ√≥rica, necesitamos pasar el √≠ndice de las columnas one-hot encoded correspondientes\n",
    "                # O si estamos graficando solo una, podemos usar su nombre original y dejar que el ColumnTransformer lo maneje\n",
    "                \n",
    "                # Una forma m√°s sencilla con ColumnTransformer es pasar el pipeline completo al PartialDependenceDisplay\n",
    "                # Creamos un pipeline que incluye el preprocesador y el meta_model\n",
    "                \n",
    "                feature_indices = []\n",
    "                for col_name in self.X_processed.columns:\n",
    "                    if col_name.startswith(feat_name + '_'):\n",
    "                        feature_indices.append(self.X_processed.columns.get_loc(col_name))\n",
    "                \n",
    "                if feature_indices:\n",
    "                    pdp_indices.append(feature_indices)\n",
    "                else: # Si no se encontraron columnas OHE, es posible que sea una caracter√≠stica de fallback o un error\n",
    "                    # Intentar a√±adirla por su nombre, si PartialDependenceDisplay puede resolverlo\n",
    "                    if feat_name in self.X_processed.columns:\n",
    "                        pdp_indices.append(self.X_processed.columns.get_loc(feat_name))\n",
    "                    \n",
    "            else: # Fallback para caracter√≠sticas que no se mapearon bien\n",
    "                print(f\"   Advertencia: No se pudo mapear '{feat_name}' para PDP/ICE.\")\n",
    "\n",
    "        if not pdp_indices:\n",
    "            print(\"   ‚ö†Ô∏è No se encontraron caracter√≠sticas v√°lidas para generar PDP/ICE. Saltando.\")\n",
    "            return\n",
    "        \n",
    "        # Crear un pipeline para PartialDependenceDisplay\n",
    "        full_pipeline = Pipeline(steps=[('preprocessor', self.preprocessor),\n",
    "                                        ('regressor', meta_model)])\n",
    "\n",
    "        # Ajustar el pipeline (ya lo hemos hecho con X_processed, pero para PartialDependenceDisplay se necesita el pipeline)\n",
    "        # Necesitamos el X original y el y original para el pipeline completo\n",
    "        X_original_for_pipeline = self.df[CARACTERISTICAS_META_MODELO]\n",
    "        y_original_for_pipeline = self.df[best_model_name]\n",
    "        full_pipeline.fit(X_original_for_pipeline, y_original_for_pipeline)\n",
    "\n",
    "\n",
    "        # Generar PDP e ICE plots\n",
    "        for i, feature_name in enumerate(features_to_plot):\n",
    "            print(f\"      Generando PDP/ICE para '{feature_name}'...\")\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # PartialDependenceDisplay puede manejar tanto nombres de caracter√≠sticas originales\n",
    "            # como √≠ndices de caracter√≠sticas ya transformadas.\n",
    "            # Al pasar el pipeline completo y el X_original, puede aplicar el preprocessor internamente.\n",
    "\n",
    "            try:\n",
    "                # Si la caracter√≠stica es num√©rica, podemos usar n_jobs=-1 para acelerar\n",
    "                # Si es categ√≥rica, no se usa grid_resolution tan directamente\n",
    "                \n",
    "                # Determinar si es una caracter√≠stica num√©rica o categ√≥rica original\n",
    "                is_categorical = feature_name in CARACTERISTICAS_CATEGORICAS_META_MODELO\n",
    "\n",
    "                if is_categorical:\n",
    "                    # Para categ√≥ricas, PartialDependenceDisplay toma los nombres originales\n",
    "                    # y puede hacer un \"grid\" sobre las categor√≠as √∫nicas.\n",
    "                    # El `kind='both'` intenta mostrar ICE y PDP\n",
    "                    PartialDependenceDisplay.from_estimator(\n",
    "                        full_pipeline,\n",
    "                        X_original_for_pipeline, # Pasar el DataFrame original\n",
    "                        features=[feature_name], # Pasar el nombre de la caracter√≠stica original\n",
    "                        kind='both', # Muestra PDP e ICE\n",
    "                        ax=ax,\n",
    "                        feature_names=CARACTERISTICAS_META_MODELO, # Nombres de las caracter√≠sticas originales\n",
    "                        ice_lines_kw={\"color\": \"darkblue\", \"alpha\": 0.2, \"linewidth\": 0.8},\n",
    "                        pd_line_kw={\"color\": \"red\", \"linewidth\": 4, \"alpha\": 0.8, \"label\": \"PDP (Mean)\"},\n",
    "                        n_jobs=-1 # Usar todos los cores\n",
    "                    )\n",
    "                else: # Num√©rica\n",
    "                     PartialDependenceDisplay.from_estimator(\n",
    "                        full_pipeline,\n",
    "                        X_original_for_pipeline,\n",
    "                        features=[feature_name],\n",
    "                        kind='both',\n",
    "                        ax=ax,\n",
    "                        feature_names=CARACTERISTICAS_META_MODELO,\n",
    "                        ice_lines_kw={\"color\": \"darkblue\", \"alpha\": 0.2, \"linewidth\": 0.8},\n",
    "                        pd_line_kw={\"color\": \"red\", \"linewidth\": 4, \"alpha\": 0.8, \"label\": \"PDP (Mean)\"},\n",
    "                        grid_resolution=30, # Puntos en la rejilla para num√©ricas\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                \n",
    "                ax.set_title(f'PDP e ICE para {best_model_name}: {feature_name}', fontweight='bold', fontsize=14, pad=20)\n",
    "                ax.set_xlabel(feature_name, fontweight='bold', fontsize=12)\n",
    "                ax.set_ylabel(f'Predicci√≥n de Error para {best_model_name}', fontweight='bold', fontsize=12)\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(self.dir_salida / f'10_1_pdp_ice_{best_model_name}_{feature_name.replace(\" \", \"_\")}.png',\n",
    "                            dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"      ‚úì PDP/ICE generado para '{feature_name}'.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Error generando PDP/ICE para '{feature_name}': {e}\")\n",
    "                plt.close() # Cerrar figura en caso de error\n",
    "\n",
    "        print(\"   ‚úì PDP e ICE generados para las caracter√≠sticas m√°s importantes del mejor modelo.\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 9. RESUMEN EJECUTIVO (Actualizado para usar PFI)\n",
    "    # ========================================================================\n",
    "\n",
    "    def _generar_resumen_ejecutivo(self):\n",
    "        \"\"\"Genera un resumen ejecutivo consolidado, ahora con PFI para impacto.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"GENERANDO RESUMEN EJECUTIVO\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        pasos = sorted(self.df['Paso'].unique())\n",
    "        distribuciones = self.df['Distribuci√≥n'].unique()\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "\n",
    "        # FIGURA 9.2: Comparaci√≥n multidimensional (sin cambios)\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "        top5_modelos = self.df_ranking.head(5)['Modelo'].tolist()\n",
    "\n",
    "        # Nuevas dimensiones\n",
    "        caracteristicas_eval = ['Ranking DM', 'Estabilidad por Paso',\n",
    "                                'Estabilidad por Distribuci√≥n', 'Estabilidad por Varianza']\n",
    "        matriz_resumen = []\n",
    "\n",
    "        for modelo in top5_modelos:\n",
    "            fila = []\n",
    "\n",
    "            # 1. Ranking DM (normalizado)\n",
    "            rank_pos = self.df_ranking[self.df_ranking['Modelo'] == modelo].index[0]\n",
    "            # Invertir y normalizar: un rank 1 debe ser 100, un rank N debe ser 0.\n",
    "            rank_norm = 100 * (1 - rank_pos / (len(self.df_ranking) - 1)) if len(self.df_ranking) > 1 else 100\n",
    "            fila.append(rank_norm)\n",
    "\n",
    "            # 2. Estabilidad por Paso (inversa de la std)\n",
    "            stds_paso = [self.df[self.df['Paso'] == p][modelo].std() for p in pasos]\n",
    "            min_std_paso_all_models = min([np.mean([self.df[self.df['Paso'] == p][m].std() for p in pasos]) for m in self.modelos if len(pasos) > 0]) if len(pasos) > 0 else 0\n",
    "            max_std_paso_all_models = max([np.mean([self.df[self.df['Paso'] == p][m].std() for p in pasos]) for m in self.modelos if len(pasos) > 0]) if len(pasos) > 0 else 1\n",
    "            if max_std_paso_all_models == min_std_paso_all_models:\n",
    "                est_paso = 100\n",
    "            else:\n",
    "                est_paso = 100 * (1 - (np.mean(stds_paso) - min_std_paso_all_models) / (max_std_paso_all_models - min_std_paso_all_models))\n",
    "            fila.append(est_paso)\n",
    "\n",
    "            # 3. Estabilidad por Distribuci√≥n\n",
    "            stds_dist = [self.df[self.df['Distribuci√≥n'] == d][modelo].std() for d in distribuciones]\n",
    "            min_std_dist_all_models = min([np.mean([self.df[self.df['Distribuci√≥n'] == d][m].std() for d in distribuciones]) for m in self.modelos if len(distribuciones) > 0]) if len(distribuciones) > 0 else 0\n",
    "            max_std_dist_all_models = max([np.mean([self.df[self.df['Distribuci√≥n'] == d][m].std() for d in distribuciones]) for m in self.modelos if len(distribuciones) > 0]) if len(distribuciones) > 0 else 1\n",
    "            if max_std_dist_all_models == min_std_dist_all_models:\n",
    "                est_dist = 100\n",
    "            else:\n",
    "                est_dist = 100 * (1 - (np.mean(stds_dist) - min_std_dist_all_models) / (max_std_dist_all_models - min_std_dist_all_models))\n",
    "            fila.append(est_dist)\n",
    "\n",
    "            # 4. Estabilidad por Varianza\n",
    "            stds_var = [self.df[self.df['Varianza error'] == v][modelo].std() for v in varianzas]\n",
    "            min_std_var_all_models = min([np.mean([self.df[self.df['Varianza error'] == v][m].std() for v in varianzas]) for m in self.modelos if len(varianzas) > 0]) if len(varianzas) > 0 else 0\n",
    "            max_std_var_all_models = max([np.mean([self.df[self.df['Varianza error'] == v][m].std() for v in varianzas]) for m in self.modelos if len(varianzas) > 0]) if len(varianzas) > 0 else 1\n",
    "            if max_std_var_all_models == min_std_var_all_models:\n",
    "                est_var = 100\n",
    "            else:\n",
    "                est_var = 100 * (1 - (np.mean(stds_var) - min_std_var_all_models) / (max_std_var_all_models - min_std_var_all_models))\n",
    "            fila.append(est_var)\n",
    "\n",
    "            matriz_resumen.append(fila)\n",
    "\n",
    "        df_heatmap = pd.DataFrame(matriz_resumen, columns=caracteristicas_eval, index=top5_modelos)\n",
    "\n",
    "        sns.heatmap(df_heatmap, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "                    ax=ax, cbar_kws={'label': 'Score Normalizado (0-100)'},\n",
    "                    linewidths=2, linecolor='white', vmin=0, vmax=100,\n",
    "                    annot_kws={'fontsize': 11, 'fontweight': 'bold'})\n",
    "        ax.set_title('Perfil Multidimensional - Top 5 Modelos',\n",
    "                     fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.set_xlabel('Dimensi√≥n de Evaluaci√≥n', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Modelo', fontweight='bold', fontsize=12)\n",
    "        ax.tick_params(labelsize=11)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '9_2_resumen_perfil_multidimensional.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # FIGURA 9.3: Impacto de caracter√≠sticas - AHORA CON PFI\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        if self.pfi_results:\n",
    "            # Calcular el PFI promedio para cada caracter√≠stica en todos los modelos\n",
    "            # ya se calcul√≥ en _analisis_impacto_pfi, aqu√≠ lo recuperamos\n",
    "            avg_pfi_series = pd.DataFrame(self.pfi_results).mean(axis=1)\n",
    "\n",
    "            # Asegurar que el total sea 100% para este gr√°fico\n",
    "            total_pfi = avg_pfi_series.sum()\n",
    "            if total_pfi > 0:\n",
    "                impactos_norm = (avg_pfi_series / total_pfi) * 100\n",
    "            else:\n",
    "                impactos_norm = pd.Series(0.0, index=avg_pfi_series.index)\n",
    "\n",
    "\n",
    "            # Ordenar por impacto\n",
    "            impactos_sorted = impactos_norm.sort_values(ascending=True) # Ascending para barh\n",
    "\n",
    "            nombres_imp = impactos_sorted.index\n",
    "            valores_imp = impactos_sorted.values\n",
    "\n",
    "            colors_imp = plt.cm.Reds(np.linspace(0.3, 0.9, len(impactos_sorted)))\n",
    "            bars = ax.barh(nombres_imp, valores_imp, color=colors_imp, alpha=0.8,\n",
    "                           edgecolor='black', linewidth=1.5)\n",
    "            ax.set_xlabel('Impacto Normalizado PFI (%)', fontweight='bold', fontsize=12)\n",
    "            ax.set_title('Impacto de Caracter√≠sticas en el ECRPS (PFI Promedio)\\n(Total = 100%)',\n",
    "                         fontweight='bold', fontsize=14, pad=20)\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            ax.set_xlim(0, max(valores_imp) * 1.15)\n",
    "\n",
    "            for i, (bar, val) in enumerate(zip(bars, valores_imp)):\n",
    "                ax.text(val + 1, i, f' {val:.1f}%', va='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "            # Agregar suma total\n",
    "            ax.text(0.98, 0.02, f'Suma Total: {sum(valores_imp):.1f}%',\n",
    "                    transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
    "                    ha='right', va='bottom',\n",
    "                    bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"No hay datos de PFI para mostrar.\",\n",
    "                    horizontalalignment='center', verticalalignment='center',\n",
    "                    transform=ax.transAxes, fontsize=12, color='red')\n",
    "            ax.set_title('Impacto de Caracter√≠sticas en el ECRPS (PFI Promedio)\\n(Total = 100%)',\n",
    "                         fontweight='bold', fontsize=14, pad=20)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / '9_3_resumen_impacto_caracteristicas.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"   ‚úì 2 figuras generadas para resumen ejecutivo\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCI√ìN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal de ejecuci√≥n\"\"\"\n",
    "    print(\"\\n\" + \"‚ñà\" * 80)\n",
    "    print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "    print(\"‚ñà\" + \" \" * 10 + \"AN√ÅLISIS COMPLETO DE BASE DE DATOS - VERSI√ìN MEJORADA\" + \" \" * 9 + \"‚ñà\")\n",
    "    print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "    print(\"‚ñà\" * 80 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        # Crear instancia del analizador\n",
    "        analizador = AnalizadorBaseCompleta(RUTA_DATOS)\n",
    "\n",
    "        # Ejecutar an√°lisis completo\n",
    "        analizador.ejecutar_analisis_completo()\n",
    "\n",
    "        print(\"\\n\" + \"‚ñà\" * 80)\n",
    "        print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "        print(\"‚ñà\" + \" \" * 20 + \"‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE\" + \" \" * 23 + \"‚ñà\")\n",
    "        print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "        print(\"‚ñà\" * 80 + \"\\n\")\n",
    "\n",
    "        total_figures = 15 # Figuras originales\n",
    "        if hasattr(analizador, 'pfi_results') and analizador.pfi_results:\n",
    "            total_figures += 1 # PFI global promedio\n",
    "        if hasattr(analizador, 'df_ranking') and not analizador.df_ranking.empty:\n",
    "            best_model = analizador.df_ranking.iloc[0]['Modelo']\n",
    "            if best_model in analizador.pfi_results:\n",
    "                num_pdp_ice_plots = min(3, len(list(analizador.pfi_results[best_model].keys())))\n",
    "                total_figures += num_pdp_ice_plots\n",
    "        \n",
    "        print(f\"üìä TOTAL DE FIGURAS GENERADAS: {total_figures} im√°genes PNG\")\n",
    "        print(\"\\nüìÅ ESTRUCTURA DE RESULTADOS:\")\n",
    "        print(f\"   {DIR_SALIDA}/\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 1.1: Estacionariedad - Comparativo\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 1.2: Estacionariedad - Cambio Relativo\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 2.1: Linealidad - Comparativo\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 2.2: Linealidad - Cambio Relativo\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 3.2: Modelo Generador - Z-Score\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 3.3: Modelo Generador - Variabilidad\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 4.1: Distribuci√≥n - Heatmap ECRPS\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 4.2: Distribuci√≥n - Heatmap Variabilidad\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 5.1: Varianza - Tendencias\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 5.2: Varianza - Tasa de Crecimiento\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 6.1: Horizonte - Evoluci√≥n\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 6.2: Horizonte - Tasa de Deterioro\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 7.2: Robustez - Coeficiente de Variaci√≥n\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 8.2: Significancia - Matriz de Superioridad\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 9.1: PFI Global Promedio (NUEVO)\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 9.2: Resumen - Perfil Multidimensional\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ 9.3: Resumen - Impacto de Caracter√≠sticas (PFI Normalizado) (MODIFICADO)\")\n",
    "        print(\"   ‚îî‚îÄ‚îÄ 10.1: PDP e ICE para el Mejor Modelo (x3) (NUEVO)\")\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå ERROR: No se encontr√≥ el archivo {RUTA_DATOS}\")\n",
    "        print(\"   Por favor, verifica que el archivo existe y la ruta es correcta.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR INESPERADO: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f818263",
   "metadata": {},
   "source": [
    "# Preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648b4035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà               AN√ÅLISIS DE PREGUNTAS DE PROFUNDIZACI√ìN                       ‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS DE PREGUNTAS DE PROFUNDIZACI√ìN\n",
      "================================================================================\n",
      "\n",
      "‚úì Datos cargados: 2000 filas, 17 columnas\n",
      "‚úì Modelos a analizar: 9\n",
      "‚úì Directorio de salida: resultados_preguntas_profundizacion\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨üî¨\n",
      "\n",
      "1Ô∏è‚É£  Pregunta 1: Punto de quiebre de AREPD...\n",
      "--- An√°lisis de Punto de Quiebre para AREPD ---\n",
      "   ‚úì Punto de quiebre AREPD (Normal): Varianza ‚âà 0.350\n",
      "   ‚úì Punto de quiebre AREPD (Uniform): Varianza ‚âà 0.750\n",
      "   ‚úì Punto de quiebre AREPD (Exponential): Varianza ‚âà 0.350\n",
      "   ‚úì Punto de quiebre AREPD (T-student): Varianza ‚âà 0.750\n",
      "   ‚úì Punto de quiebre AREPD (Mixture): Varianza ‚âà 0.350\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "2Ô∏è‚É£  Pregunta 2: Zona de dominio Block Bootstrapping...\n",
      "   ‚úì BB domina en: 1267 / 2000 casos (63.3%)\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "3Ô∏è‚É£  Pregunta 3: Deterioro de AV-MCPS por horizonte...\n",
      "   ‚úì Tipo de deterioro predominante: Cuadr√°tico\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "4Ô∏è‚É£  Pregunta 4: Efecto multiplicativo distribuci√≥n Normal...\n",
      "   ‚úì Modelos con efecto multiplicativo: ['MCPS']\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "5Ô∏è‚É£  Pregunta 5: Frontera de colapso Deep Learning...\n",
      "   ‚úì Escenarios cr√≠ticos identificados: 1\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "6Ô∏è‚É£  Pregunta 6: Validaci√≥n de 'Mejor Modelo'...\n",
      "   ‚úì Tasa de coincidencia global: 100.0%\n",
      "   ‚úì Distribuci√≥n con mayor consistencia: exponential (100.0%)\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "7Ô∏è‚É£  Pregunta 7: Aceleraci√≥n del deterioro...\n",
      "   ‚úì Modelos con deterioro acelerado: 1\n",
      "   ‚úì Modelos con deterioro desacelerado: 7\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "8Ô∏è‚É£  Pregunta 8: Colapso LSPM con varianza alta...\n",
      "   ‚úì Cambio de ranking LSPM: +0.0 posiciones\n",
      "   ‚úì Cambio de ranking LSPMW: +0.0 posiciones\n",
      "   ‚úì LSPM/LSPMW mantienen ventaja incluso con alta varianza\n",
      "   ‚úì 2 figuras generadas\n",
      "\n",
      "\n",
      "9Ô∏è‚É£  Pregunta 9: Mapa de decisi√≥n operacional...\n",
      "   ‚úì Modelo m√°s robusto: Block Bootstrapping\n",
      "   ‚úì Reglas de alta confianza (>70%): 3\n",
      "   ‚úì Archivo CSV generado: reglas_decision.csv\n",
      "   ‚úì 3 figuras generadas\n",
      "\n",
      "\n",
      "================================================================================\n",
      "‚úÖ AN√ÅLISIS DE PREGUNTAS COMPLETO\n",
      "üìÅ Resultados guardados en: resultados_preguntas_profundizacion\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà                    ‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE                       ‚ñà\n",
      "‚ñà                                                                              ‚ñà\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üìä RESUMEN DE FIGURAS GENERADAS POR PREGUNTA:\n",
      "\n",
      "   Pregunta 1 (Punto de quiebre AREPD): 2 figuras\n",
      "   Pregunta 2 (Zona de dominio BB): 2 figuras\n",
      "   Pregunta 3 (Deterioro AV-MCPS): 2 figuras\n",
      "   Pregunta 4 (Penalizaci√≥n Normal): 2 figuras\n",
      "   Pregunta 5 (Frontera DL): 2 figuras\n",
      "   Pregunta 6 (Consistencia Mejor Modelo): 2 figuras\n",
      "   Pregunta 7 (Segunda derivada): 2 figuras\n",
      "   Pregunta 8 (Interacci√≥n No-Lineal √ó Varianza): 2 figuras\n",
      "   Pregunta 9 (Mapa de decisi√≥n): 3 figuras\n",
      "\n",
      "   üìÅ TOTAL: 19 figuras PNG + 1 archivo CSV (reglas_decision.csv)\n",
      "\n",
      "üìÅ ESTRUCTURA DE RESULTADOS:\n",
      "   ./resultados_preguntas_profundizacion/\n",
      "   ‚îú‚îÄ‚îÄ P1_1: Punto de quiebre AREPD - Comparativo\n",
      "   ‚îú‚îÄ‚îÄ P1_2: Tasa de deterioro AREPD\n",
      "   ‚îú‚îÄ‚îÄ P2_1: Zona de dominio BB - Heatmap\n",
      "   ‚îú‚îÄ‚îÄ P2_2: Frecuencia de dominio BB\n",
      "   ‚îú‚îÄ‚îÄ P3_1: Deterioro AV-MCPS - Curvas\n",
      "   ‚îú‚îÄ‚îÄ P3_2: Tipo de deterioro AV-MCPS\n",
      "   ‚îú‚îÄ‚îÄ P4_1: Penalizaci√≥n Normal - Interacci√≥n\n",
      "   ‚îú‚îÄ‚îÄ P4_2: Clasificaci√≥n de interacci√≥n\n",
      "   ‚îú‚îÄ‚îÄ P5_1: Frontera de colapso DL\n",
      "   ‚îú‚îÄ‚îÄ P5_2: Brecha DL por escenario\n",
      "   ‚îú‚îÄ‚îÄ P6_1: Consistencia Mejor Modelo - Global\n",
      "   ‚îú‚îÄ‚îÄ P6_2: Consistencia por condiciones\n",
      "   ‚îú‚îÄ‚îÄ P7_1: Segunda derivada - Aceleraci√≥n\n",
      "   ‚îú‚îÄ‚îÄ P7_2: Comparaci√≥n de derivadas\n",
      "   ‚îú‚îÄ‚îÄ P8_1: Interacci√≥n No-Lineal √ó Varianza - Ranking\n",
      "   ‚îú‚îÄ‚îÄ P8_2: Cambio de ranking LSPM\n",
      "   ‚îú‚îÄ‚îÄ P9_1: Mapa de decisi√≥n operacional\n",
      "   ‚îú‚îÄ‚îÄ P9_2: Confiabilidad de reglas\n",
      "   ‚îú‚îÄ‚îÄ P9_3: √Årbol de decisi√≥n textual\n",
      "   ‚îî‚îÄ‚îÄ reglas_decision.csv\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "RUTA_DATOS = \"./Datos/datos_combinados.xlsx\"\n",
    "DIR_SALIDA = \"./resultados_preguntas_profundizacion\"\n",
    "\n",
    "MODELOS = ['AREPD', 'AV-MCPS', 'Block Bootstrapping', 'DeepAR',\n",
    "           'EnCQR-LSTM', 'LSPM', 'LSPMW', 'MCPS', 'Sieve Bootstrap']\n",
    "\n",
    "COLORES_MODELOS = {\n",
    "    'AREPD': '#e41a1c',\n",
    "    'AV-MCPS': '#377eb8',\n",
    "    'Block Bootstrapping': '#4daf4a',\n",
    "    'DeepAR': '#984ea3',\n",
    "    'EnCQR-LSTM': '#ff7f00',\n",
    "    'LSPM': '#ffff33',\n",
    "    'LSPMW': '#a65628',\n",
    "    'MCPS': '#f781bf',\n",
    "    'Sieve Bootstrap': '#999999'\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLASE PRINCIPAL DE AN√ÅLISIS - PREGUNTAS DE PROFUNDIZACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "class AnalizadorPreguntasProfundizacion:\n",
    "    \"\"\"An√°lisis espec√≠fico para responder preguntas de profundizaci√≥n\"\"\"\n",
    "\n",
    "    def __init__(self, ruta_datos):\n",
    "        \"\"\"Inicializa el analizador\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"AN√ÅLISIS DE PREGUNTAS DE PROFUNDIZACI√ìN\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        self.df = pd.read_excel(ruta_datos)\n",
    "        self.modelos = MODELOS\n",
    "        self.COLORES_MODELOS = COLORES_MODELOS\n",
    "        self.dir_salida = Path(DIR_SALIDA)\n",
    "        self.dir_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Extraer caracter√≠sticas del escenario\n",
    "        self._extraer_caracteristicas()\n",
    "\n",
    "        print(f\"‚úì Datos cargados: {self.df.shape[0]} filas, {self.df.shape[1]} columnas\")\n",
    "        print(f\"‚úì Modelos a analizar: {len(self.modelos)}\")\n",
    "        print(f\"‚úì Directorio de salida: {self.dir_salida}\")\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    def _extraer_caracteristicas(self):\n",
    "        \"\"\"Extrae caracter√≠sticas individuales del escenario\"\"\"\n",
    "        self.df['Estacionario'] = self.df['Escenario'].apply(\n",
    "            lambda x: 'Estacionario' if 'Estacionario' in x and 'No_Estacionario' not in x else 'No Estacionario'\n",
    "        )\n",
    "\n",
    "        self.df['Lineal'] = self.df['Escenario'].apply(\n",
    "            lambda x: 'Lineal' if 'Lineal' in x and 'No_Lineal' not in x else 'No Lineal'\n",
    "        )\n",
    "\n",
    "    def ejecutar_analisis_completo(self):\n",
    "        \"\"\"Ejecuta todos los an√°lisis para las preguntas\"\"\"\n",
    "        print(\"\\n\" + \"üî¨\" * 40 + \"\\n\")\n",
    "\n",
    "        # Pregunta 1: Punto de quiebre de AREPD\n",
    "        print(\"1Ô∏è‚É£  Pregunta 1: Punto de quiebre de AREPD...\")\n",
    "        self._pregunta_1_punto_quiebre_arepd()\n",
    "\n",
    "        # Pregunta 2: Robustez de Block Bootstrapping vs Sieve Bootstrap\n",
    "        print(\"\\n2Ô∏è‚É£  Pregunta 2: Zona de dominio Block Bootstrapping...\")\n",
    "        self._pregunta_2_zona_dominio_bb()\n",
    "\n",
    "        # Pregunta 3: Deterioro acelerado AV-MCPS\n",
    "        print(\"\\n3Ô∏è‚É£  Pregunta 3: Deterioro de AV-MCPS por horizonte...\")\n",
    "        self._pregunta_3_deterioro_av_mcps()\n",
    "\n",
    "        # Pregunta 4: Penalizaci√≥n Normal multiplicativa\n",
    "        print(\"\\n4Ô∏è‚É£  Pregunta 4: Efecto multiplicativo distribuci√≥n Normal...\")\n",
    "        self._pregunta_4_penalizacion_normal()\n",
    "\n",
    "        # Pregunta 5: Frontera de colapso Deep Learning\n",
    "        print(\"\\n5Ô∏è‚É£  Pregunta 5: Frontera de colapso Deep Learning...\")\n",
    "        self._pregunta_5_frontera_dl()\n",
    "\n",
    "        # Pregunta 6: Consistencia \"Mejor Modelo\"\n",
    "        print(\"\\n6Ô∏è‚É£  Pregunta 6: Validaci√≥n de 'Mejor Modelo'...\")\n",
    "        self._pregunta_6_consistencia_mejor_modelo()\n",
    "\n",
    "        # Pregunta 7: An√°lisis de segunda derivada\n",
    "        print(\"\\n7Ô∏è‚É£  Pregunta 7: Aceleraci√≥n del deterioro...\")\n",
    "        self._pregunta_7_segunda_derivada()\n",
    "\n",
    "        # Pregunta 8: Interacci√≥n No Linealidad √ó Varianza\n",
    "        print(\"\\n8Ô∏è‚É£  Pregunta 8: Colapso LSPM con varianza alta...\")\n",
    "        self._pregunta_8_interaccion_nolineal_varianza()\n",
    "\n",
    "        # Pregunta 9: Mapa de decisi√≥n operacional\n",
    "        print(\"\\n9Ô∏è‚É£  Pregunta 9: Mapa de decisi√≥n operacional...\")\n",
    "        self._pregunta_9_mapa_decision()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ AN√ÅLISIS DE PREGUNTAS COMPLETO\")\n",
    "        print(f\"üìÅ Resultados guardados en: {self.dir_salida}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 1: PUNTO DE QUIEBRE DE AREPD\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_1_punto_quiebre_arepd(self):\n",
    "        \"\"\"\n",
    "        ¬øExiste un punto de quiebre en la varianza del error donde AREPD se deteriora?\n",
    "        ¬øEste punto es consistente entre todas las distribuciones presentes?\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- MODIFICACI√ìN: Detectar todas las distribuciones √∫nicas autom√°ticamente ---\n",
    "        distribuciones = self.df['Distribuci√≥n'].unique()\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "        \n",
    "        # Diccionario para almacenar los resultados de cada distribuci√≥n\n",
    "        resultados_por_dist = {dist: {'arepd': [], 'otros_robustos': []} for dist in distribuciones}\n",
    "        \n",
    "        # Modelos robustos para comparaci√≥n\n",
    "        otros_modelos = ['Block Bootstrapping', 'Sieve Bootstrap', 'LSPM']\n",
    "        \n",
    "        # Calcular el rendimiento promedio para cada distribuci√≥n y varianza\n",
    "        for dist in distribuciones:\n",
    "            df_dist = self.df[self.df['Distribuci√≥n'] == dist]\n",
    "            for var in varianzas:\n",
    "                df_var = df_dist[df_dist['Varianza error'] == var]\n",
    "                \n",
    "                # Rendimiento de AREPD\n",
    "                resultados_por_dist[dist]['arepd'].append(df_var['AREPD'].mean())\n",
    "                \n",
    "                # Rendimiento promedio de otros modelos robustos\n",
    "                resultados_por_dist[dist]['otros_robustos'].append(df_var[otros_modelos].mean().mean())\n",
    "\n",
    "        # --- MODIFICACI√ìN: Gr√°fico din√°mico para N distribuciones ---\n",
    "        # FIGURA 1.1: Evoluci√≥n de AREPD vs modelos robustos por distribuci√≥n\n",
    "        n_dist = len(distribuciones)\n",
    "        n_cols = 2\n",
    "        n_rows = math.ceil(n_dist / n_cols)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(8 * n_cols, 6 * n_rows), squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for idx, dist in enumerate(distribuciones):\n",
    "            ax = axes[idx]\n",
    "            arepd_perf = resultados_por_dist[dist]['arepd']\n",
    "            otros_perf = resultados_por_dist[dist]['otros_robustos']\n",
    "            \n",
    "            ax.plot(varianzas, arepd_perf, 'o-', label='AREPD', \n",
    "                    color=self.COLORES_MODELOS.get('AREPD', 'blue'), linewidth=3, markersize=8)\n",
    "            ax.plot(varianzas, otros_perf, 's--', label='Promedio Modelos Robustos',\n",
    "                    color='green', linewidth=2, markersize=7, alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Varianza del Error', fontweight='bold', fontsize=12)\n",
    "            ax.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "            ax.set_title(f'Distribuci√≥n {dist.capitalize()}: Punto de Quiebre AREPD', \n",
    "                         fontweight='bold', fontsize=13)\n",
    "            ax.legend(fontsize=11)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ocultar ejes no utilizados si el n√∫mero de distribuciones es impar\n",
    "        for idx in range(n_dist, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P1_1_punto_quiebre_arepd_comparativo.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # --- MODIFICACI√ìN: Gr√°fico de comparaci√≥n de tasas con N distribuciones ---\n",
    "        # FIGURA 1.2: Tasa de deterioro incremental\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Usar un ciclo de colores de Matplotlib para distinguir las l√≠neas\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_dist))\n",
    "        var_medias = [(varianzas[i] + varianzas[i+1]) / 2 for i in range(len(varianzas) - 1)]\n",
    "        \n",
    "        puntos_quiebre = {}\n",
    "\n",
    "        for idx, dist in enumerate(distribuciones):\n",
    "            arepd_perf = resultados_por_dist[dist]['arepd']\n",
    "            \n",
    "            # Calcular tasas de cambio (derivada num√©rica)\n",
    "            if len(varianzas) > 1 and np.diff(varianzas).any():\n",
    "                tasas = np.diff(arepd_perf) / np.diff(varianzas)\n",
    "            else:\n",
    "                tasas = []\n",
    "\n",
    "            if len(tasas) > 0:\n",
    "                ax.plot(var_medias, tasas, 'o-', label=f'{dist.capitalize()}', \n",
    "                       color=colors[idx], linewidth=2.5, markersize=8)\n",
    "                \n",
    "                # Identificar y almacenar el punto de m√°xima aceleraci√≥n del deterioro\n",
    "                max_accel_idx = np.argmax(tasas)\n",
    "                puntos_quiebre[dist] = var_medias[max_accel_idx]\n",
    "            else:\n",
    "                puntos_quiebre[dist] = None\n",
    "\n",
    "\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "        ax.set_xlabel('Varianza del Error (punto medio)', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Tasa de Deterioro (ŒîECRPS/ŒîVarianza)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('AREPD: Aceleraci√≥n del Deterioro por Distribuci√≥n\\n(Mayor pendiente = Colapso m√°s r√°pido)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=12, loc='upper left', title='Distribuciones')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P1_2_tasa_deterioro_arepd.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # --- MODIFICACI√ìN: Imprimir resultados para todas las distribuciones ---\n",
    "        print(\"--- An√°lisis de Punto de Quiebre para AREPD ---\")\n",
    "        for dist, umbral in puntos_quiebre.items():\n",
    "            if umbral is not None:\n",
    "                print(f\"   ‚úì Punto de quiebre AREPD ({dist.capitalize()}): Varianza ‚âà {umbral:.3f}\")\n",
    "            else:\n",
    "                print(f\"   ! No se pudo calcular el punto de quiebre para {dist.capitalize()}.\")\n",
    "        \n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 2: ZONA DE DOMINIO BLOCK BOOTSTRAPPING\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_2_zona_dominio_bb(self):\n",
    "        \"\"\"\n",
    "        ¬øEn qu√© condiciones EXACTAS Block Bootstrapping supera a Sieve Bootstrap?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Crear DataFrame de comparaci√≥n directa\n",
    "        df_comp = self.df.copy()\n",
    "        df_comp['BB_mejor'] = df_comp['Block Bootstrapping'] < df_comp['Sieve Bootstrap']\n",
    "        df_comp['Diferencia'] = df_comp['Sieve Bootstrap'] - df_comp['Block Bootstrapping']\n",
    "        \n",
    "        # FIGURA 2.1: Mapa de calor de superioridad\n",
    "        # >>> INICIO DE MODIFICACI√ìN 1 <<<\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        escenarios_principales = [\n",
    "            ('Estacionario', 'Lineal'),\n",
    "            ('Estacionario', 'No Lineal'),\n",
    "            ('No Estacionario', 'Lineal')\n",
    "        ]\n",
    "        # >>> FIN DE MODIFICACI√ìN 1 <<<\n",
    "        \n",
    "        for idx, (est, lin) in enumerate(escenarios_principales):\n",
    "            ax = axes[idx]\n",
    "            df_esc = df_comp[(df_comp['Estacionario'] == est) & (df_comp['Lineal'] == lin)]\n",
    "            \n",
    "            # Crear matriz de diferencias\n",
    "            pivot = df_esc.pivot_table(\n",
    "                values='Diferencia',\n",
    "                index='Distribuci√≥n',\n",
    "                columns='Varianza error',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn', center=0,\n",
    "                       ax=ax, cbar_kws={'label': 'SB - BB (>0 = BB mejor)'},\n",
    "                       linewidths=1, linecolor='gray', vmin=-0.02, vmax=0.02)\n",
    "            ax.set_title(f'{est} + {lin}', fontweight='bold', fontsize=12)\n",
    "            ax.set_xlabel('Varianza Error', fontweight='bold')\n",
    "            ax.set_ylabel('Distribuci√≥n', fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('Zona de Dominio: Block Bootstrapping vs Sieve Bootstrap\\n(Verde = BB domina, Rojo = SB domina)',\n",
    "                    fontweight='bold', fontsize=16, y=1.03)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P2_1_zona_dominio_bb_heatmap.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 2.2: Frecuencia de dominio por condiciones\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Calcular % de casos donde BB es mejor\n",
    "        resultados_dominio = []\n",
    "        # >>> INICIO DE MODIFICACI√ìN 1 (cont.) <<<\n",
    "        # Se usan los mismos 3 escenarios definidos para la figura 2.1\n",
    "        for est, lin in escenarios_principales:\n",
    "            df_esc = df_comp[(df_comp['Estacionario'] == est) & (df_comp['Lineal'] == lin)]\n",
    "            pct_bb_mejor = (df_esc['BB_mejor'].sum() / len(df_esc) * 100) if len(df_esc) > 0 else 0\n",
    "            resultados_dominio.append({\n",
    "                'Escenario': f'{est[:3]}+{lin[:3]}',\n",
    "                'Completo': f'{est} + {lin}',\n",
    "                'Pct_BB_Mejor': pct_bb_mejor\n",
    "            })\n",
    "        # >>> FIN DE MODIFICACI√ìN 1 (cont.) <<<\n",
    "        \n",
    "        df_dominio = pd.DataFrame(resultados_dominio).sort_values('Pct_BB_Mejor', ascending=False)\n",
    "        \n",
    "        colors = ['green' if x > 50 else 'red' for x in df_dominio['Pct_BB_Mejor']]\n",
    "        bars = ax.barh(df_dominio['Escenario'], df_dominio['Pct_BB_Mejor'],\n",
    "                      color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axvline(50, color='black', linestyle='--', linewidth=2, label='50% (Equilibrio)')\n",
    "        ax.set_xlabel('% de casos donde BB supera a SB', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Frecuencia de Dominio de Block Bootstrapping\\n(>50% = BB generalmente mejor)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.set_xlim(0, 100)\n",
    "        \n",
    "        for i, (bar, val) in enumerate(zip(bars, df_dominio['Pct_BB_Mejor'])):\n",
    "            ax.text(val + 2, i, f'{val:.1f}%', va='center', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P2_2_frecuencia_dominio_bb.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úì BB domina en: {df_comp['BB_mejor'].sum()} / {len(df_comp)} casos ({df_comp['BB_mejor'].sum()/len(df_comp)*100:.1f}%)\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 3: DETERIORO AV-MCPS POR HORIZONTE\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_3_deterioro_av_mcps(self):\n",
    "        \"\"\"\n",
    "        ¬øEl deterioro de AV-MCPS es lineal, cuadr√°tico o exponencial?\n",
    "        ¬øCambia seg√∫n el nivel de varianza?\n",
    "        \"\"\"\n",
    "        \n",
    "        pasos = sorted(self.df['Paso'].unique())\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "        \n",
    "        # FIGURA 3.1: Ajuste de curvas de deterioro\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Seleccionar niveles de varianza representativos\n",
    "        if len(varianzas) >= 4:\n",
    "            var_seleccionadas = [varianzas[0], varianzas[len(varianzas)//3], \n",
    "                                varianzas[2*len(varianzas)//3], varianzas[-1]]\n",
    "        else:\n",
    "            var_seleccionadas = varianzas\n",
    "        \n",
    "        modelos_comparacion = ['AV-MCPS', 'LSPM', 'Block Bootstrapping']\n",
    "        \n",
    "        for idx, var in enumerate(var_seleccionadas[:4]):\n",
    "            ax = axes[idx]\n",
    "            df_var = self.df[self.df['Varianza error'] == var]\n",
    "            \n",
    "            for modelo in modelos_comparacion:\n",
    "                valores = [df_var[df_var['Paso'] == p][modelo].mean() for p in pasos]\n",
    "                ax.plot(pasos, valores, 'o-', label=modelo, \n",
    "                       linewidth=2.5, markersize=8, color=COLORES_MODELOS[modelo])\n",
    "            \n",
    "            ax.set_xlabel('Horizonte (Paso)', fontweight='bold', fontsize=11)\n",
    "            ax.set_ylabel('ECRPS', fontweight='bold', fontsize=11)\n",
    "            ax.set_title(f'Varianza = {var:.3f}', fontweight='bold', fontsize=12)\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Evoluci√≥n del Deterioro por Horizonte: AV-MCPS vs Modelos Estables',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P3_1_deterioro_av_mcps_curvas.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 3.2: An√°lisis de tipo de crecimiento\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        # Calcular R¬≤ para diferentes tipos de ajuste\n",
    "        tipos_ajuste = []\n",
    "        \n",
    "        for var in varianzas:\n",
    "            df_var = self.df[self.df['Varianza error'] == var]\n",
    "            valores_av = [df_var[df_var['Paso'] == p]['AV-MCPS'].mean() for p in pasos]\n",
    "            \n",
    "            x = np.array(pasos)\n",
    "            y = np.array(valores_av)\n",
    "            \n",
    "            # Ajuste lineal\n",
    "            p_lin = np.polyfit(x, y, 1)\n",
    "            y_lin = np.polyval(p_lin, x)\n",
    "            r2_lin = 1 - (np.sum((y - y_lin)**2) / np.sum((y - np.mean(y))**2))\n",
    "            \n",
    "            # Ajuste cuadr√°tico\n",
    "            p_quad = np.polyfit(x, y, 2)\n",
    "            y_quad = np.polyval(p_quad, x)\n",
    "            r2_quad = 1 - (np.sum((y - y_quad)**2) / np.sum((y - np.mean(y))**2))\n",
    "            \n",
    "            # Ajuste exponencial (logar√≠tmico)\n",
    "            try:\n",
    "                z = np.polyfit(x, np.log(y + 1e-10), 1)\n",
    "                y_exp = np.exp(np.polyval(z, x))\n",
    "                r2_exp = 1 - (np.sum((y - y_exp)**2) / np.sum((y - np.mean(y))**2))\n",
    "            except:\n",
    "                r2_exp = 0\n",
    "            \n",
    "            mejor_ajuste = max([('Lineal', r2_lin), ('Cuadr√°tico', r2_quad), ('Exponencial', r2_exp)], \n",
    "                              key=lambda x: x[1])\n",
    "            \n",
    "            tipos_ajuste.append({\n",
    "                'Varianza': var,\n",
    "                'R2_Lineal': r2_lin,\n",
    "                'R2_Cuadratico': r2_quad,\n",
    "                'R2_Exponencial': r2_exp,\n",
    "                'Mejor': mejor_ajuste[0]\n",
    "            })\n",
    "        \n",
    "        df_ajustes = pd.DataFrame(tipos_ajuste)\n",
    "        \n",
    "        x_pos = np.arange(len(varianzas))\n",
    "        width = 0.25\n",
    "        \n",
    "        ax.bar(x_pos - width, df_ajustes['R2_Lineal'], width, label='Lineal', alpha=0.8)\n",
    "        ax.bar(x_pos, df_ajustes['R2_Cuadratico'], width, label='Cuadr√°tico', alpha=0.8)\n",
    "        ax.bar(x_pos + width, df_ajustes['R2_Exponencial'], width, label='Exponencial', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Varianza del Error', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('R¬≤ (Bondad de Ajuste)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('AV-MCPS: Tipo de Deterioro por Nivel de Varianza\\n(R¬≤ m√°s alto = Mejor ajuste)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels([f'{v:.3f}' for v in varianzas], rotation=45)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P3_2_tipo_deterioro_av_mcps.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úì Tipo de deterioro predominante: {df_ajustes['Mejor'].mode()[0]}\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 4: PENALIZACI√ìN NORMAL MULTIPLICATIVA\n",
    "    # ========================================================================\n",
    "    def _pregunta_4_penalizacion_normal(self):\n",
    "        \"\"\"\n",
    "        ¬øLa penalizaci√≥n de la distribuci√≥n Normal es aditiva o multiplicativa \n",
    "        con la no-estacionariedad?\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- CORRECCI√ìN: Se definen los nombres reales de los escenarios del Excel ---\n",
    "        # Se usan estos nombres para filtrar el DataFrame correctamente.\n",
    "        escenario_estacionario = 'Estacionario_Lineal'\n",
    "        escenario_no_estacionario = 'No_Estacionario_Lineal'\n",
    "        \n",
    "        # Modelos a analizar\n",
    "        modelos_analisis = ['DeepAR', 'MCPS', 'LSPM', 'Block Bootstrapping']\n",
    "        \n",
    "        # FIGURA 4.1: Efecto aditivo vs multiplicativo\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        resultados_interaccion = []\n",
    "        \n",
    "        for idx, modelo in enumerate(modelos_analisis):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Calcular penalizaci√≥n por escenario\n",
    "            penalizaciones = []\n",
    "            \n",
    "            # --- CORRECCI√ìN: Se itera sobre los escenarios correctos y se usan etiquetas limpias para los gr√°ficos ---\n",
    "            escenarios_a_comparar = {\n",
    "                'Estacionario': escenario_estacionario,\n",
    "                'No Estacionario': escenario_no_estacionario\n",
    "            }\n",
    "            \n",
    "            for etiqueta, nombre_escenario in escenarios_a_comparar.items():\n",
    "                \n",
    "                # --- CORRECCI√ìN: Filtrar por la columna 'Escenario' con los nombres correctos ---\n",
    "                df_escenario = self.df[self.df['Escenario'] == nombre_escenario]\n",
    "                \n",
    "                # --- CORRECCI√ìN: Usar 'normal' y 'mixture' en min√∫sculas ---\n",
    "                rend_normal = df_escenario[df_escenario['Distribuci√≥n'] == 'normal'][modelo].mean()\n",
    "                rend_mixture = df_escenario[df_escenario['Distribuci√≥n'] == 'mixture'][modelo].mean()\n",
    "                \n",
    "                # Calcular el deterioro porcentual. Se a√±ade una guarda contra la divisi√≥n por cero.\n",
    "                if rend_mixture != 0:\n",
    "                    deterioro = ((rend_normal - rend_mixture) / rend_mixture) * 100\n",
    "                else:\n",
    "                    deterioro = 0.0 # O float('inf') si se prefiere, pero 0 es m√°s seguro para graficar\n",
    "                \n",
    "                penalizaciones.append({\n",
    "                    'Estacionariedad': etiqueta, # Usar la etiqueta limpia para el gr√°fico\n",
    "                    'Deterioro_pct': deterioro\n",
    "                })\n",
    "            \n",
    "            df_pen = pd.DataFrame(penalizaciones)\n",
    "            \n",
    "            # Calcular raz√≥n de efectos\n",
    "            det_estacionario = df_pen[df_pen['Estacionariedad'] == 'Estacionario']['Deterioro_pct'].values[0]\n",
    "            det_no_estacionario = df_pen[df_pen['Estacionariedad'] == 'No Estacionario']['Deterioro_pct'].values[0]\n",
    "            \n",
    "            # Evitar divisi√≥n por cero si el deterioro en el caso estacionario fue nulo\n",
    "            razon = det_no_estacionario / det_estacionario if det_estacionario != 0 else 0\n",
    "            es_multiplicativo = razon > 1.5  # Si el efecto es >50% mayor, se considera multiplicativo\n",
    "            \n",
    "            resultados_interaccion.append({\n",
    "                'Modelo': modelo,\n",
    "                'Razon': razon,\n",
    "                'Tipo': 'Multiplicativo' if es_multiplicativo else 'Aditivo'\n",
    "            })\n",
    "            \n",
    "            # Visualizaci√≥n\n",
    "            colors = ['lightblue', 'coral']\n",
    "            bars = ax.bar(df_pen['Estacionariedad'], df_pen['Deterioro_pct'], \n",
    "                         color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "            ax.set_ylabel('Deterioro por Dist. Normal (%)', fontweight='bold', fontsize=11)\n",
    "            ax.set_title(f'{modelo}\\nRaz√≥n: {razon:.2f}x ({(\"MULTIPLICATIVO\" if es_multiplicativo else \"ADITIVO\")})',\n",
    "                        fontweight='bold', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "            \n",
    "            for bar, val in zip(bars, df_pen['Deterioro_pct']):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2., height + 1,\n",
    "                       f'{val:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.suptitle('Interacci√≥n: Distribuci√≥n Normal √ó No-Estacionariedad\\n(Raz√≥n > 1.5 = Efecto Multiplicativo)',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        \n",
    "        # >>> INICIO DE MODIFICACI√ìN 2 <<<\n",
    "        # Se a√±ade m√°s padding para evitar que los elementos se superpongan\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        # >>> FIN DE MODIFICACI√ìN 2 <<<\n",
    "        \n",
    "        plt.savefig(self.dir_salida / 'P4_1_penalizacion_normal_interaccion.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 4.2: Resumen de tipos de interacci√≥n\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        df_interaccion = pd.DataFrame(resultados_interaccion).sort_values('Razon', ascending=False)\n",
    "        \n",
    "        colors_tipo = ['red' if x == 'Multiplicativo' else 'green' for x in df_interaccion['Tipo']]\n",
    "        bars = ax.barh(df_interaccion['Modelo'], df_interaccion['Razon'],\n",
    "                      color=colors_tipo, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axvline(1.5, color='black', linestyle='--', linewidth=2, label='Umbral Multiplicativo (1.5x)')\n",
    "        ax.axvline(1.0, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "        ax.set_xlabel('Raz√≥n de Efectos (No-Est / Est)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Clasificaci√≥n del Tipo de Interacci√≥n por Modelo\\n(Rojo = Multiplicativo, Verde = Aditivo)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, (bar, val, tipo) in enumerate(zip(bars, df_interaccion['Razon'], df_interaccion['Tipo'])):\n",
    "            ax.text(val + 0.05, i, f'{val:.2f}x ({tipo})', \n",
    "                   va='center', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P4_2_clasificacion_interaccion.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        modelos_multiplicativos = df_interaccion[df_interaccion['Tipo'] == 'Multiplicativo']['Modelo'].tolist()\n",
    "        print(f\"   ‚úì Modelos con efecto multiplicativo: {modelos_multiplicativos}\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 5: FRONTERA DE COLAPSO DEEP LEARNING\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_5_frontera_dl(self):\n",
    "        \"\"\"\n",
    "        ¬øExiste una 'frontera de colapso' para DeepAR y EnCQR-LSTM donde su \n",
    "        rendimiento cae por debajo de m√©todos estad√≠sticos simples?\n",
    "        \"\"\"\n",
    "        \n",
    "        modelos_dl = ['DeepAR', 'EnCQR-LSTM']\n",
    "        modelos_estadisticos = ['Block Bootstrapping', 'Sieve Bootstrap', 'LSPM']\n",
    "        \n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "        \n",
    "        # FIGURA 5.1: Evoluci√≥n comparativa con varianza\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        for idx, modelo_dl in enumerate(modelos_dl):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Calcular promedios por varianza\n",
    "            dl_vals = [self.df[self.df['Varianza error'] == v][modelo_dl].mean() for v in varianzas]\n",
    "            \n",
    "            # Promedio de modelos estad√≠sticos\n",
    "            est_vals = []\n",
    "            for v in varianzas:\n",
    "                df_v = self.df[self.df['Varianza error'] == v]\n",
    "                est_vals.append(df_v[modelos_estadisticos].mean().mean())\n",
    "            \n",
    "            # Plotear\n",
    "            ax.plot(varianzas, dl_vals, 'o-', label=f'{modelo_dl} (DL)',\n",
    "                   color=COLORES_MODELOS[modelo_dl], linewidth=3, markersize=10)\n",
    "            ax.plot(varianzas, est_vals, 's--', label='Promedio Estad√≠sticos',\n",
    "                   color='green', linewidth=2.5, markersize=8, alpha=0.7)\n",
    "            \n",
    "            # Identificar punto de cruce\n",
    "            diferencias = np.array(dl_vals) - np.array(est_vals)\n",
    "            if np.any(diferencias > 0):\n",
    "                idx_cruce_candidatos = np.where(diferencias > 0)[0]\n",
    "                if len(idx_cruce_candidatos) > 0:\n",
    "                    idx_cruce = idx_cruce_candidatos[0]\n",
    "                    var_cruce = varianzas[idx_cruce]\n",
    "                    ax.axvline(var_cruce, color='red', linestyle=':', linewidth=2, alpha=0.7)\n",
    "                    ax.annotate(f'Frontera de colapso\\nVarianza ‚âà {var_cruce:.3f}',\n",
    "                               xy=(var_cruce, dl_vals[idx_cruce]),\n",
    "                               xytext=(20, -30), textcoords='offset points',\n",
    "                               bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8),\n",
    "                               arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                               fontsize=10, fontweight='bold')\n",
    "            \n",
    "            ax.set_xlabel('Varianza del Error', fontweight='bold', fontsize=12)\n",
    "            ax.set_ylabel('ECRPS Promedio', fontweight='bold', fontsize=12)\n",
    "            ax.set_title(f'Frontera de Colapso: {modelo_dl}',\n",
    "                        fontweight='bold', fontsize=13)\n",
    "            ax.legend(fontsize=11)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Identificaci√≥n de Frontera donde Deep Learning < M√©todos Estad√≠sticos',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P5_1_frontera_colapso_dl.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 5.2: Brecha de rendimiento por escenario\n",
    "        # >>> INICIO DE MODIFICACI√ìN 3 <<<\n",
    "        fig, ax = plt.subplots(figsize=(16, 9)) # Ajustar tama√±o para mejor visualizaci√≥n\n",
    "        \n",
    "        escenarios_unicos = self.df['Escenario'].unique()\n",
    "        \n",
    "        brechas = []\n",
    "        for esc in escenarios_unicos:\n",
    "            df_esc = self.df[self.df['Escenario'] == esc]\n",
    "            \n",
    "            for modelo_dl in modelos_dl:\n",
    "                dl_mean = df_esc[modelo_dl].mean()\n",
    "                est_mean = df_esc[modelos_estadisticos].mean().mean()\n",
    "                \n",
    "                brecha_pct = ((dl_mean - est_mean) / est_mean) * 100\n",
    "                \n",
    "                brechas.append({\n",
    "                    'Escenario': esc.replace(\"_\", \" \"), # Nombres m√°s legibles\n",
    "                    'Modelo_DL': modelo_dl,\n",
    "                    'Brecha_pct': brecha_pct\n",
    "                })\n",
    "        \n",
    "        df_brechas = pd.DataFrame(brechas)\n",
    "        \n",
    "        # Pivot para visualizaci√≥n\n",
    "        pivot_brechas = df_brechas.pivot(index='Escenario', columns='Modelo_DL', values='Brecha_pct')\n",
    "        pivot_brechas = pivot_brechas.sort_values(by='DeepAR', ascending=False)\n",
    "        \n",
    "        x = np.arange(len(pivot_brechas))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Se cambia barh por bar para un gr√°fico vertical\n",
    "        ax.bar(x - width/2, pivot_brechas['DeepAR'], width, \n",
    "               label='DeepAR', color=COLORES_MODELOS['DeepAR'], alpha=0.8, edgecolor='black')\n",
    "        ax.bar(x + width/2, pivot_brechas['EnCQR-LSTM'], width,\n",
    "               label='EnCQR-LSTM', color=COLORES_MODELOS['EnCQR-LSTM'], alpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Se cambia axvline por axhline\n",
    "        ax.axhline(0, color='black', linestyle='-', linewidth=2)\n",
    "        ax.axhline(50, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Umbral Cr√≠tico (+50%)')\n",
    "        \n",
    "        # Se configuran los ejes para el gr√°fico vertical\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(pivot_brechas.index, rotation=45, ha='right', fontsize=9)\n",
    "        ax.set_ylabel('Brecha vs M√©todos Estad√≠sticos (%)', fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel('Escenario', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Escenarios donde Deep Learning Colapsa\\n(>0% = DL peor que Estad√≠sticos)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=11, loc='upper right')\n",
    "        ax.grid(True, alpha=0.3, axis='y') # Grid en el eje Y\n",
    "        # >>> FIN DE MODIFICACI√ìN 3 <<<\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P5_2_brecha_dl_por_escenario.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úì Escenarios cr√≠ticos identificados: {len(pivot_brechas[pivot_brechas['DeepAR'] > 50])}\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 6: CONSISTENCIA \"MEJOR MODELO\"\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_6_consistencia_mejor_modelo(self):\n",
    "        \"\"\"\n",
    "        ¬øCon qu√© frecuencia el modelo marcado como 'Mejor Modelo' coincide con el \n",
    "        que REALMENTE tiene el menor error num√©rico?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Verificar si existe la columna \"Mejor Modelo\"\n",
    "        if 'Mejor Modelo' not in self.df.columns:\n",
    "            print(\"   ‚ö†Ô∏è  Columna 'Mejor Modelo' no encontrada. Saltando an√°lisis.\\n\")\n",
    "            return\n",
    "        \n",
    "        # Identificar el modelo con menor error en cada fila\n",
    "        self.df['Modelo_Min_Real'] = self.df[self.modelos].idxmin(axis=1)\n",
    "        self.df['Coincide'] = self.df['Mejor Modelo'] == self.df['Modelo_Min_Real']\n",
    "        \n",
    "        # FIGURA 6.1: Tasa de coincidencia global\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        tasa_coincidencia = self.df['Coincide'].mean() * 100\n",
    "        \n",
    "        labels = ['Coincide', 'No Coincide']\n",
    "        sizes = [tasa_coincidencia, 100 - tasa_coincidencia]\n",
    "        colors = ['#4CAF50', '#F44336']\n",
    "        explode = (0.1, 0)\n",
    "        \n",
    "        ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "               autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "               textprops={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        ax.set_title('Consistencia de \"Mejor Modelo\" con Menor Error Real\\n',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P6_1_consistencia_mejor_modelo_global.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 6.2: Coincidencia por escenario y distribuci√≥n\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # Por Escenario\n",
    "        ax1 = axes[0]\n",
    "        coincidencia_esc = self.df.groupby('Escenario')['Coincide'].mean() * 100\n",
    "        coincidencia_esc = coincidencia_esc.sort_values(ascending=True)\n",
    "        \n",
    "        colors_esc = ['green' if x > 70 else 'orange' if x > 50 else 'red' \n",
    "                      for x in coincidencia_esc.values]\n",
    "        bars1 = ax1.barh(range(len(coincidencia_esc)), coincidencia_esc.values,\n",
    "                        color=colors_esc, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax1.set_yticks(range(len(coincidencia_esc)))\n",
    "        ax1.set_yticklabels([x[:30] + '...' if len(x) > 30 else x for x in coincidencia_esc.index], \n",
    "                           fontsize=8)\n",
    "        ax1.set_xlabel('% de Coincidencia', fontweight='bold', fontsize=11)\n",
    "        ax1.set_title('Consistencia por Escenario', fontweight='bold', fontsize=12)\n",
    "        ax1.axvline(70, color='green', linestyle='--', linewidth=1.5, alpha=0.5, label='Umbral Bueno (70%)')\n",
    "        ax1.axvline(50, color='orange', linestyle='--', linewidth=1.5, alpha=0.5, label='Umbral Aceptable (50%)')\n",
    "        ax1.legend(fontsize=9)\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Por Distribuci√≥n\n",
    "        ax2 = axes[1]\n",
    "        coincidencia_dist = self.df.groupby('Distribuci√≥n')['Coincide'].mean() * 100\n",
    "        coincidencia_dist = coincidencia_dist.sort_values(ascending=False)\n",
    "        \n",
    "        colors_dist = ['green' if x > 70 else 'orange' if x > 50 else 'red' \n",
    "                       for x in coincidencia_dist.values]\n",
    "        bars2 = ax2.bar(range(len(coincidencia_dist)), coincidencia_dist.values,\n",
    "                       color=colors_dist, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax2.set_xticks(range(len(coincidencia_dist)))\n",
    "        ax2.set_xticklabels(coincidencia_dist.index, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('% de Coincidencia', fontweight='bold', fontsize=11)\n",
    "        ax2.set_title('Consistencia por Distribuci√≥n', fontweight='bold', fontsize=12)\n",
    "        ax2.axhline(70, color='green', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "        ax2.axhline(50, color='orange', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for bar, val in zip(bars2, coincidencia_dist.values):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{val:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.suptitle('An√°lisis de Consistencia de \"Mejor Modelo\" por Condiciones',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P6_2_consistencia_por_condiciones.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"   ‚úì Tasa de coincidencia global: {tasa_coincidencia:.1f}%\")\n",
    "        print(f\"   ‚úì Distribuci√≥n con mayor consistencia: {coincidencia_dist.idxmax()} ({coincidencia_dist.max():.1f}%)\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 7: AN√ÅLISIS DE SEGUNDA DERIVADA\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_7_segunda_derivada(self):\n",
    "        \"\"\"\n",
    "        ¬øQu√© modelos muestran 'deterioro acelerado' (segunda derivada positiva) \n",
    "        vs 'deterioro constante' al aumentar Pasos?\n",
    "        \"\"\"\n",
    "        \n",
    "        pasos = sorted(self.df['Paso'].unique())\n",
    "        \n",
    "        if len(pasos) < 3:\n",
    "            print(\"   ‚ö†Ô∏è  Insuficientes pasos para an√°lisis de segunda derivada. Saltando.\\n\")\n",
    "            return\n",
    "        \n",
    "        # FIGURA 7.1: Aceleraci√≥n del deterioro\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        aceleraciones = []\n",
    "        \n",
    "        for modelo in self.modelos:\n",
    "            valores = [self.df[self.df['Paso'] == p][modelo].mean() for p in pasos]\n",
    "            \n",
    "            # Primera derivada (velocidad de cambio)\n",
    "            primera_deriv = np.diff(valores)\n",
    "            \n",
    "            # Segunda derivada (aceleraci√≥n)\n",
    "            if len(primera_deriv) > 1:\n",
    "                segunda_deriv = np.diff(primera_deriv)\n",
    "                aceleracion_media = np.mean(segunda_deriv)\n",
    "                \n",
    "                # Clasificaci√≥n\n",
    "                if aceleracion_media > 0.001:\n",
    "                    tipo = 'Acelerado'\n",
    "                elif aceleracion_media < -0.001:\n",
    "                    tipo = 'Desacelerado'\n",
    "                else:\n",
    "                    tipo = 'Constante'\n",
    "                \n",
    "                aceleraciones.append({\n",
    "                    'Modelo': modelo,\n",
    "                    'Aceleracion': aceleracion_media,\n",
    "                    'Tipo': tipo\n",
    "                })\n",
    "        \n",
    "        df_aceleraciones = pd.DataFrame(aceleraciones).sort_values('Aceleracion', ascending=False)\n",
    "        \n",
    "        # Colores seg√∫n tipo\n",
    "        color_map = {'Acelerado': 'red', 'Constante': 'orange', 'Desacelerado': 'green'}\n",
    "        colors = [color_map[t] for t in df_aceleraciones['Tipo']]\n",
    "        \n",
    "        bars = ax.barh(df_aceleraciones['Modelo'], df_aceleraciones['Aceleracion'],\n",
    "                      color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "        ax.set_xlabel('Aceleraci√≥n del Deterioro (Segunda Derivada)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Clasificaci√≥n de Modelos por Aceleraci√≥n del Deterioro\\n(Rojo=Acelerado, Verde=Desacelerado, Naranja=Constante)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, (bar, val, tipo) in enumerate(zip(bars, df_aceleraciones['Aceleracion'], \n",
    "                                                  df_aceleraciones['Tipo'])):\n",
    "            ax.text(val + (0.0001 if val > 0 else -0.0001), i, f'{val:.4f} ({tipo})',\n",
    "                   va='center', ha='left' if val > 0 else 'right',\n",
    "                   fontweight='bold', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P7_1_segunda_derivada_aceleracion.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 7.2: Comparaci√≥n de primeras y segundas derivadas\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "        \n",
    "        # >>> INICIO DE MODIFICACI√ìN 4 <<<\n",
    "        # Mapear cada modelo a su tipo de aceleraci√≥n para el estilo de l√≠nea\n",
    "        modelo_a_tipo = df_aceleraciones.set_index('Modelo')['Tipo'].to_dict()\n",
    "        tipo_a_estilo = {'Acelerado': '-', 'Desacelerado': '--', 'Constante': ':'}\n",
    "\n",
    "        # Primera derivada\n",
    "        ax1 = axes[0]\n",
    "        for modelo in self.modelos:\n",
    "            valores = [self.df[self.df['Paso'] == p][modelo].mean() for p in pasos]\n",
    "            primera_deriv = np.diff(valores)\n",
    "            pasos_deriv = pasos[1:]\n",
    "            \n",
    "            tipo_modelo = modelo_a_tipo.get(modelo, 'Constante')\n",
    "            linestyle = tipo_a_estilo[tipo_modelo]\n",
    "            \n",
    "            ax1.plot(pasos_deriv, primera_deriv, 'o-', label=modelo,\n",
    "                    linewidth=2.5, markersize=8, color=COLORES_MODELOS[modelo],\n",
    "                    linestyle=linestyle)\n",
    "        \n",
    "        ax1.axhline(0, color='black', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "        ax1.set_xlabel('Paso', fontweight='bold', fontsize=11)\n",
    "        ax1.set_ylabel('Primera Derivada (Velocidad)', fontweight='bold', fontsize=11)\n",
    "        ax1.set_title('Velocidad de Deterioro por Paso\\n(Acelerado:‚Äî, Desacelerado:--, Constante:..)',\n",
    "                     fontweight='bold', fontsize=12)\n",
    "        ax1.legend(fontsize=9, ncol=3) # Aumentar columnas para que quepan todos\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Segunda derivada\n",
    "        ax2 = axes[1]\n",
    "        for modelo in self.modelos:\n",
    "            valores = [self.df[self.df['Paso'] == p][modelo].mean() for p in pasos]\n",
    "            primera_deriv = np.diff(valores)\n",
    "            \n",
    "            if len(primera_deriv) > 1:\n",
    "                segunda_deriv = np.diff(primera_deriv)\n",
    "                pasos_deriv2 = pasos[2:]\n",
    "                \n",
    "                tipo_modelo = modelo_a_tipo.get(modelo, 'Constante')\n",
    "                linestyle = tipo_a_estilo[tipo_modelo]\n",
    "                \n",
    "                ax2.plot(pasos_deriv2, segunda_deriv, 's-', label=modelo,\n",
    "                        linewidth=2.5, markersize=8, color=COLORES_MODELOS[modelo],\n",
    "                        linestyle=linestyle)\n",
    "        \n",
    "        ax2.axhline(0, color='black', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "        ax2.set_xlabel('Paso', fontweight='bold', fontsize=11)\n",
    "        ax2.set_ylabel('Segunda Derivada (Aceleraci√≥n)', fontweight='bold', fontsize=11)\n",
    "        ax2.set_title('Aceleraci√≥n del Deterioro por Paso\\n(Valores positivos = Deterioro acelerado)',\n",
    "                     fontweight='bold', fontsize=12)\n",
    "        ax2.legend(fontsize=9, ncol=3) # Aumentar columnas\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        # >>> FIN DE MODIFICACI√ìN 4 <<<\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P7_2_comparacion_derivadas.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        modelos_vulnerables = df_aceleraciones[df_aceleraciones['Tipo'] == 'Acelerado']['Modelo'].tolist()\n",
    "        modelos_robustos = df_aceleraciones[df_aceleraciones['Tipo'] == 'Desacelerado']['Modelo'].tolist()\n",
    "        print(f\"   ‚úì Modelos con deterioro acelerado: {len(modelos_vulnerables)}\")\n",
    "        print(f\"   ‚úì Modelos con deterioro desacelerado: {len(modelos_robustos)}\")\n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 8: INTERACCI√ìN NO LINEALIDAD √ó VARIANZA\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_8_interaccion_nolineal_varianza(self):\n",
    "        \"\"\"\n",
    "        ¬øLa ventaja de LSPM/LSPMW en escenarios no lineales DESAPARECE cuando \n",
    "        la varianza del error supera cierto umbral?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Filtrar escenarios no lineales\n",
    "        df_nolineal = self.df[self.df['Lineal'] == 'No Lineal'].copy()\n",
    "        \n",
    "        if df_nolineal.empty:\n",
    "            print(\"   ‚ö†Ô∏è  No hay datos para escenarios no lineales. Saltando an√°lisis.\\n\")\n",
    "            return\n",
    "            \n",
    "        varianzas = sorted(df_nolineal['Varianza error'].unique())\n",
    "        \n",
    "        # >>> INICIO DE MODIFICACI√ìN 5 <<<\n",
    "        # Se redefine el c√°lculo para hacer una comparaci√≥n m√°s clara entre los extremos\n",
    "        # Se usa el primer cuartil (Q1) para \"Varianza Baja\" y el tercer cuartil (Q3) para \"Varianza Alta\"\n",
    "        if len(varianzas) > 3:\n",
    "            q1, q3 = np.percentile(varianzas, [25, 75])\n",
    "            df_baja = df_nolineal[df_nolineal['Varianza error'] <= q1]\n",
    "            df_alta = df_nolineal[df_nolineal['Varianza error'] >= q3]\n",
    "        else: # Fallback para pocos datos\n",
    "            q2 = np.percentile(varianzas, 50)\n",
    "            df_baja = df_nolineal[df_nolineal['Varianza error'] <= q2]\n",
    "            df_alta = df_nolineal[df_nolineal['Varianza error'] >= q2]\n",
    "\n",
    "        grupos_dfs = {'Varianza Baja': df_baja, 'Varianza Alta': df_alta}\n",
    "        # >>> FIN DE MODIFICACI√ìN 5 <<<\n",
    "\n",
    "        # FIGURA 8.1: Ranking por grupo de varianza\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        for idx, (grupo, df_grupo) in enumerate(grupos_dfs.items()):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            if df_grupo.empty:\n",
    "                ax.text(0.5, 0.5, 'Sin datos', ha='center', va='center', fontsize=12)\n",
    "                ax.set_title(f'Ranking en Escenarios No Lineales\\n{grupo}', fontweight='bold', fontsize=12)\n",
    "                continue\n",
    "\n",
    "            # Calcular medias y ranking\n",
    "            medias = df_grupo[self.modelos].mean().sort_values()\n",
    "            \n",
    "            colors = ['gold' if m in ['LSPM', 'LSPMW'] else 'steelblue' for m in medias.index]\n",
    "            bars = ax.barh(range(len(medias)), medias.values, color=colors,\n",
    "                          alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "            \n",
    "            ax.set_yticks(range(len(medias)))\n",
    "            ax.set_yticklabels(medias.index, fontsize=10)\n",
    "            ax.set_xlabel('ECRPS Promedio', fontweight='bold', fontsize=11)\n",
    "            ax.set_title(f'Ranking en Escenarios No Lineales\\n{grupo}',\n",
    "                        fontweight='bold', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Destacar posici√≥n de LSPM/LSPMW\n",
    "            pos_lspm = list(medias.index).index('LSPM') + 1\n",
    "            pos_lspmw = list(medias.index).index('LSPMW') + 1\n",
    "            \n",
    "            ax.text(0.02, 0.98, f'LSPM: Rank {pos_lspm}\\nLSPMW: Rank {pos_lspmw}',\n",
    "                   transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
    "                   va='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "        \n",
    "        plt.suptitle('¬øLSPM/LSPMW Pierden Ventaja con Alta Varianza?',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P8_1_interaccion_nolineal_varianza_ranking.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 8.2: Cambio de posici√≥n en ranking\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        cambios_ranking = []\n",
    "        \n",
    "        # >>> INICIO DE MODIFICACI√ìN 5 (cont.) <<<\n",
    "        # Se usan los dataframes df_baja y df_alta definidos previamente\n",
    "        for modelo in self.modelos:\n",
    "            if df_baja.empty or df_alta.empty: continue\n",
    "            \n",
    "            ranking_baja = df_baja[self.modelos].mean().rank().loc[modelo]\n",
    "            ranking_alta = df_alta[self.modelos].mean().rank().loc[modelo]\n",
    "            \n",
    "            cambio = ranking_alta - ranking_baja  # Positivo = empeor√≥ posici√≥n\n",
    "            \n",
    "            cambios_ranking.append({\n",
    "                'Modelo': modelo,\n",
    "                'Cambio': cambio,\n",
    "                'Empeora': cambio > 0\n",
    "            })\n",
    "        # >>> FIN DE MODIFICACI√ìN 5 (cont.) <<<\n",
    "\n",
    "        if not cambios_ranking:\n",
    "            print(\"   ‚ö†Ô∏è  No se pudo generar el gr√°fico de cambio de ranking.\\n\")\n",
    "            return\n",
    "\n",
    "        df_cambios = pd.DataFrame(cambios_ranking).sort_values('Cambio', ascending=True)\n",
    "        \n",
    "        colors_cambio = ['red' if x > 2 else 'orange' if x > 0 else 'green' \n",
    "                        for x in df_cambios['Cambio']]\n",
    "        bars = ax.barh(df_cambios['Modelo'], df_cambios['Cambio'],\n",
    "                      color=colors_cambio, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "        ax.axvline(3, color='red', linestyle='--', linewidth=1.5, alpha=0.5, \n",
    "                  label='Cambio Cr√≠tico (>3 posiciones)')\n",
    "        ax.set_xlabel('Cambio en Ranking (Varianza Alta - Varianza Baja)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Deterioro de Posici√≥n con Alta Varianza en Escenarios No Lineales\\n(Positivo = Pierde posiciones, Negativo = Gana posiciones)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, (bar, val, modelo) in enumerate(zip(bars, df_cambios['Cambio'], df_cambios['Modelo'])):\n",
    "            label_text = f'{val:+.1f}'\n",
    "            if modelo in ['LSPM', 'LSPMW']:\n",
    "                label_text += ' ‚≠ê'\n",
    "            ax.text(val + (0.1 if val > 0 else -0.1), i, label_text,\n",
    "                   va='center', ha='left' if val > 0 else 'right',\n",
    "                   fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P8_2_cambio_ranking_lspm.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # An√°lisis espec√≠fico de LSPM/LSPMW\n",
    "        cambio_lspm = df_cambios[df_cambios['Modelo'] == 'LSPM']['Cambio'].values[0]\n",
    "        cambio_lspmw = df_cambios[df_cambios['Modelo'] == 'LSPMW']['Cambio'].values[0]\n",
    "        \n",
    "        print(f\"   ‚úì Cambio de ranking LSPM: {cambio_lspm:+.1f} posiciones\")\n",
    "        print(f\"   ‚úì Cambio de ranking LSPMW: {cambio_lspmw:+.1f} posiciones\")\n",
    "        \n",
    "        if cambio_lspm > 3 or cambio_lspmw > 3:\n",
    "            print(\"   ‚ö†Ô∏è  ADVERTENCIA: LSPM/LSPMW pierden >3 posiciones con alta varianza\")\n",
    "        else:\n",
    "            print(\"   ‚úì LSPM/LSPMW mantienen ventaja incluso con alta varianza\")\n",
    "        \n",
    "        print(\"   ‚úì 2 figuras generadas\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREGUNTA 9: MAPA DE DECISI√ìN OPERACIONAL\n",
    "    # ========================================================================\n",
    "\n",
    "    def _pregunta_9_mapa_decision(self):\n",
    "        \"\"\"\n",
    "        ¬øPodemos construir una 'regla de decisi√≥n' simple para elegir modelo \n",
    "        basada en 3 variables: Estacionariedad, Distribuci√≥n y Varianza?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Crear categor√≠as de varianza\n",
    "        varianzas = sorted(self.df['Varianza error'].unique())\n",
    "        q33, q67 = np.percentile(varianzas, [33, 67])\n",
    "        \n",
    "        self.df['Nivel_Varianza'] = pd.cut(\n",
    "            self.df['Varianza error'],\n",
    "            bins=[-np.inf, q33, q67, np.inf],\n",
    "            labels=['Baja', 'Media', 'Alta']\n",
    "        )\n",
    "        \n",
    "        # FIGURA 9.1: √Årbol de decisi√≥n visual (heatmap 3D colapsado)\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        niveles_var = ['Baja', 'Media', 'Alta']\n",
    "        estacionariedades = ['Estacionario', 'No Estacionario']\n",
    "        \n",
    "        for idx_est, est in enumerate(estacionariedades):\n",
    "            for idx_var,niv_var in enumerate(niveles_var):\n",
    "                ax = axes[idx_est, idx_var]\n",
    "                \n",
    "                # Filtrar datos\n",
    "                df_filtrado = self.df[\n",
    "                    (self.df['Estacionario'] == est) & \n",
    "                    (self.df['Nivel_Varianza'] == niv_var)\n",
    "                ]\n",
    "                \n",
    "                if len(df_filtrado) == 0:\n",
    "                    ax.text(0.5, 0.5, 'Sin datos', ha='center', va='center',\n",
    "                           transform=ax.transAxes, fontsize=12)\n",
    "                    ax.set_title(f'{est} + Varianza {niv_var}', fontweight='bold', fontsize=11)\n",
    "                    continue\n",
    "                \n",
    "                # Calcular mejor modelo por distribuci√≥n\n",
    "                mejores_por_dist = {}\n",
    "                for dist in df_filtrado['Distribuci√≥n'].unique():\n",
    "                    df_dist = df_filtrado[df_filtrado['Distribuci√≥n'] == dist]\n",
    "                    medias = df_dist[self.modelos].mean()\n",
    "                    mejor = medias.idxmin()\n",
    "                    mejores_por_dist[dist] = mejor\n",
    "                \n",
    "                # Crear matriz para heatmap\n",
    "                distribuciones = sorted(df_filtrado['Distribuci√≥n'].unique())\n",
    "                matriz_decision = pd.DataFrame(index=distribuciones, columns=['Mejor Modelo'])\n",
    "                \n",
    "                for dist in distribuciones:\n",
    "                    matriz_decision.loc[dist, 'Mejor Modelo'] = mejores_por_dist.get(dist, 'N/A')\n",
    "                \n",
    "                # Asignar colores por modelo\n",
    "                modelo_a_color = {modelo: idx for idx, modelo in enumerate(self.modelos)}\n",
    "                matriz_numerica = matriz_decision['Mejor Modelo'].map(\n",
    "                    lambda x: modelo_a_color.get(x, -1)\n",
    "                ).values.reshape(-1, 1)\n",
    "                \n",
    "                im = ax.imshow(matriz_numerica, cmap='tab10', aspect='auto', vmin=0, vmax=len(self.modelos)-1)\n",
    "                \n",
    "                # Configurar ejes\n",
    "                ax.set_yticks(range(len(distribuciones)))\n",
    "                ax.set_yticklabels(distribuciones, fontsize=9)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_title(f'{est}\\nVarianza {niv_var}', fontweight='bold', fontsize=11)\n",
    "                \n",
    "                # A√±adir texto con nombre del modelo\n",
    "                for i, dist in enumerate(distribuciones):\n",
    "                    modelo = mejores_por_dist.get(dist, 'N/A')\n",
    "                    ax.text(0, i, modelo, ha='center', va='center',\n",
    "                           fontweight='bold', fontsize=9, color='white',\n",
    "                           bbox=dict(boxstyle='round', facecolor='black', alpha=0.6))\n",
    "        \n",
    "        plt.suptitle('Mapa de Decisi√≥n Operacional: Mejor Modelo por Condiciones\\n(Cada celda muestra el modelo √≥ptimo)',\n",
    "                    fontweight='bold', fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P9_1_mapa_decision_operacional.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 9.2: Reglas de decisi√≥n simplificadas\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        \n",
    "        # Calcular frecuencia de \"mejor modelo\" en cada combinaci√≥n\n",
    "        reglas = []\n",
    "        \n",
    "        for est in estacionariedades:\n",
    "            for niv_var in niveles_var:\n",
    "                df_comb = self.df[\n",
    "                    (self.df['Estacionario'] == est) & \n",
    "                    (self.df['Nivel_Varianza'] == niv_var)\n",
    "                ]\n",
    "                \n",
    "                if len(df_comb) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Encontrar modelo con menor error promedio\n",
    "                medias = df_comb[self.modelos].mean()\n",
    "                mejor_modelo = medias.idxmin()\n",
    "                mejor_ecrps = medias.min()\n",
    "                \n",
    "                # Calcular % de veces que es el mejor\n",
    "                cuenta_mejor = 0\n",
    "                for idx, row in df_comb.iterrows():\n",
    "                    if row[self.modelos].idxmin() == mejor_modelo:\n",
    "                        cuenta_mejor += 1\n",
    "                \n",
    "                frecuencia = (cuenta_mejor / len(df_comb)) * 100 if len(df_comb) > 0 else 0\n",
    "                \n",
    "                reglas.append({\n",
    "                    'Condicion': f'{est[:3]}+Var_{niv_var}',\n",
    "                    'Completo': f'{est} + Varianza {niv_var}',\n",
    "                    'Mejor_Modelo': mejor_modelo,\n",
    "                    'ECRPS': mejor_ecrps,\n",
    "                    'Frecuencia': frecuencia\n",
    "                })\n",
    "        \n",
    "        df_reglas = pd.DataFrame(reglas).sort_values('Frecuencia', ascending=True)\n",
    "        \n",
    "        # Visualizaci√≥n\n",
    "        y_pos = np.arange(len(df_reglas))\n",
    "        colors_reglas = [COLORES_MODELOS.get(m, 'gray') for m in df_reglas['Mejor_Modelo']]\n",
    "        \n",
    "        bars = ax.barh(y_pos, df_reglas['Frecuencia'], color=colors_reglas,\n",
    "                      alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(df_reglas['Condicion'], fontsize=10)\n",
    "        ax.set_xlabel('Frecuencia de Optimalidad (%)', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Confiabilidad de Reglas de Decisi√≥n\\n(% de casos donde el modelo recomendado es √≥ptimo)',\n",
    "                    fontweight='bold', fontsize=14, pad=20)\n",
    "        ax.axvline(70, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Umbral Alta Confianza (70%)')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # A√±adir etiquetas con modelo recomendado\n",
    "        for i, (bar, modelo, freq) in enumerate(zip(bars, df_reglas['Mejor_Modelo'], \n",
    "                                                     df_reglas['Frecuencia'])):\n",
    "            ax.text(freq + 2, i, f'{modelo} ({freq:.0f}%)',\n",
    "                   va='center', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P9_2_confiabilidad_reglas.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # FIGURA 9.3: √Årbol de decisi√≥n textual\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Generar reglas textuales\n",
    "        texto_reglas = \"√ÅRBOL DE DECISI√ìN OPERACIONAL\\n\" + \"=\"*60 + \"\\n\\n\"\n",
    "        \n",
    "        for idx, row in df_reglas.iterrows():\n",
    "            confianza = \"ALTA\" if row['Frecuencia'] > 70 else \"MEDIA\" if row['Frecuencia'] > 50 else \"BAJA\"\n",
    "            texto_reglas += f\"üìå SI: {row['Completo']}\\n\"\n",
    "            texto_reglas += f\"   ‚Üí USAR: {row['Mejor_Modelo']}\\n\"\n",
    "            texto_reglas += f\"   ‚Üí Confianza: {confianza} ({row['Frecuencia']:.1f}%)\\n\"\n",
    "            texto_reglas += f\"   ‚Üí ECRPS esperado: {row['ECRPS']:.4f}\\n\\n\"\n",
    "        \n",
    "        # A√±adir reglas generales\n",
    "        texto_reglas += \"\\n\" + \"=\"*60 + \"\\n\"\n",
    "        texto_reglas += \"REGLAS GENERALES SIMPLIFICADAS:\\n\\n\"\n",
    "        \n",
    "        # Mejor modelo global\n",
    "        mejor_global = df_reglas.loc[df_reglas['ECRPS'].idxmin(), 'Mejor_Modelo']\n",
    "        texto_reglas += f\"üèÜ Modelo m√°s robusto (recomendaci√≥n por defecto): {mejor_global}\\n\\n\"\n",
    "        \n",
    "        # Modelos por caracter√≠sticas\n",
    "        mejor_no_est = df_reglas[df_reglas['Completo'].str.contains('No Estacionario')].iloc[0]['Mejor_Modelo'] if len(df_reglas[df_reglas['Completo'].str.contains('No Estacionario')]) > 0 else 'N/A'\n",
    "        mejor_alta_var = df_reglas[df_reglas['Completo'].str.contains('Alta')].iloc[0]['Mejor_Modelo'] if len(df_reglas[df_reglas['Completo'].str.contains('Alta')]) > 0 else 'N/A'\n",
    "        \n",
    "        texto_reglas += f\"üî¥ Para datos NO estacionarios: {mejor_no_est}\\n\"\n",
    "        texto_reglas += f\"üî¥ Para varianza ALTA: {mejor_alta_var}\\n\"\n",
    "        \n",
    "        ax.text(0.05, 0.95, texto_reglas, transform=ax.transAxes,\n",
    "               fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.dir_salida / 'P9_3_arbol_decision_textual.png',\n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Guardar reglas en CSV\n",
    "        df_reglas.to_csv(self.dir_salida / 'reglas_decision.csv', index=False)\n",
    "        \n",
    "        print(f\"   ‚úì Modelo m√°s robusto: {mejor_global}\")\n",
    "        print(f\"   ‚úì Reglas de alta confianza (>70%): {len(df_reglas[df_reglas['Frecuencia'] > 70])}\")\n",
    "        print(f\"   ‚úì Archivo CSV generado: reglas_decision.csv\")\n",
    "        print(\"   ‚úì 3 figuras generadas\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCI√ìN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal de ejecuci√≥n\"\"\"\n",
    "    print(\"\\n\" + \"‚ñà\" * 80)\n",
    "    print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "    print(\"‚ñà\" + \" \" * 15 + \"AN√ÅLISIS DE PREGUNTAS DE PROFUNDIZACI√ìN\" + \" \" * 23 + \"‚ñà\")\n",
    "    print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "    print(\"‚ñà\" * 80 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        # Crear instancia del analizador\n",
    "        analizador = AnalizadorPreguntasProfundizacion(RUTA_DATOS)\n",
    "\n",
    "        # Ejecutar an√°lisis completo\n",
    "        analizador.ejecutar_analisis_completo()\n",
    "\n",
    "        print(\"\\n\" + \"‚ñà\" * 80)\n",
    "        print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "        print(\"‚ñà\" + \" \" * 20 + \"‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE\" + \" \" * 23 + \"‚ñà\")\n",
    "        print(\"‚ñà\" + \" \" * 78 + \"‚ñà\")\n",
    "        print(\"‚ñà\" * 80 + \"\\n\")\n",
    "\n",
    "        print(\"üìä RESUMEN DE FIGURAS GENERADAS POR PREGUNTA:\\n\")\n",
    "        print(\"   Pregunta 1 (Punto de quiebre AREPD): 2 figuras\")\n",
    "        print(\"   Pregunta 2 (Zona de dominio BB): 2 figuras\")\n",
    "        print(\"   Pregunta 3 (Deterioro AV-MCPS): 2 figuras\")\n",
    "        print(\"   Pregunta 4 (Penalizaci√≥n Normal): 2 figuras\")\n",
    "        print(\"   Pregunta 5 (Frontera DL): 2 figuras\")\n",
    "        print(\"   Pregunta 6 (Consistencia Mejor Modelo): 2 figuras\")\n",
    "        print(\"   Pregunta 7 (Segunda derivada): 2 figuras\")\n",
    "        print(\"   Pregunta 8 (Interacci√≥n No-Lineal √ó Varianza): 2 figuras\")\n",
    "        print(\"   Pregunta 9 (Mapa de decisi√≥n): 3 figuras\")\n",
    "        print(\"\\n   üìÅ TOTAL: 19 figuras PNG + 1 archivo CSV (reglas_decision.csv)\")\n",
    "        \n",
    "        print(\"\\nüìÅ ESTRUCTURA DE RESULTADOS:\")\n",
    "        print(f\"   {DIR_SALIDA}/\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P1_1: Punto de quiebre AREPD - Comparativo\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P1_2: Tasa de deterioro AREPD\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P2_1: Zona de dominio BB - Heatmap\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P2_2: Frecuencia de dominio BB\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P3_1: Deterioro AV-MCPS - Curvas\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P3_2: Tipo de deterioro AV-MCPS\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P4_1: Penalizaci√≥n Normal - Interacci√≥n\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P4_2: Clasificaci√≥n de interacci√≥n\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P5_1: Frontera de colapso DL\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P5_2: Brecha DL por escenario\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P6_1: Consistencia Mejor Modelo - Global\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P6_2: Consistencia por condiciones\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P7_1: Segunda derivada - Aceleraci√≥n\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P7_2: Comparaci√≥n de derivadas\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P8_1: Interacci√≥n No-Lineal √ó Varianza - Ranking\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P8_2: Cambio de ranking LSPM\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P9_1: Mapa de decisi√≥n operacional\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P9_2: Confiabilidad de reglas\")\n",
    "        print(\"   ‚îú‚îÄ‚îÄ P9_3: √Årbol de decisi√≥n textual\")\n",
    "        print(\"   ‚îî‚îÄ‚îÄ reglas_decision.csv\")\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå ERROR: No se encontr√≥ el archivo {RUTA_DATOS}\")\n",
    "        print(\"   Por favor, verifica que el archivo existe y la ruta es correcta.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR INESPERADO: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
