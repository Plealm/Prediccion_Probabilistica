% !TeX root = ../main.tex
\chapter{Simulación}
\label{cap:diseño_simulacion}

Este capítulo describe el diseño experimental desarrollado para evaluar el desempeño de los métodos de pronóstico probabilístico en series temporales. Se presenta la justificación de los escenarios de evaluación, la metodología de simulación empleada y las características específicas de los procesos generadores de datos utilizados.

\section{Introducción}
\label{sec:intro_simulacion}

La evaluación rigurosa de metodologías de pronóstico probabilístico requiere un marco experimental controlado que permita comparar el desempeño de diferentes técnicas bajo condiciones conocidas. A diferencia de los estudios con datos reales, donde la distribución verdadera es desconocida y la evaluación se limita a métricas indirectas, los estudios de simulación ofrecen la ventaja fundamental de conocer exactamente el proceso generador de datos (DGP, por sus siglas en inglés) \parencite{Hyndman2021FPP3}.

Este conocimiento del DGP permite evaluar directamente la calidad de las distribuciones predictivas mediante su comparación con la verdadera distribución teórica. En particular, el uso del ECRPS (Expected Continuous Ranked Probability Score) como métrica principal de evaluación se justifica porque permite cuantificar simultáneamente la calibración y la nitidez de los pronósticos probabilísticos, comparando las muestras generadas por cada método con muestras de la distribución teórica verdadera \parencite{Gneiting2014}.

El diseño experimental desarrollado considera tres dimensiones fundamentales de variación: (1) la estructura temporal del proceso (estacionariedad y linealidad), (2) la distribución del término de error, y (3) la magnitud de la varianza del ruido. Esta combinación genera un espacio de escenarios suficientemente amplio para evaluar la robustez y adaptabilidad de los métodos bajo diferentes condiciones operativas.

\section{Diseño de la Simulación}
\label{sec:diseño_experimental}

\subsection{Selección de Escenarios de Evaluación}
\label{subsec:escenarios}

El presente estudio considera tres escenarios fundamentales que caracterizan diferentes clases de comportamiento en series temporales. La selección de estos escenarios se fundamenta en la clasificación teórica de procesos estocásticos y en consideraciones de relevancia práctica.

\subsubsection{Escenario 1: Lineal Estacionario (ARMA)}

El primer escenario considera procesos autorregresivos de media móvil (ARMA), que representan la clase fundamental de modelos lineales estacionarios. Un proceso ARMA$(p,q)$ se caracteriza por su capacidad de capturar tanto la persistencia temporal (componente AR) como la dependencia de shocks pasados (componente MA), manteniendo propiedades estadísticas constantes en el tiempo \parencite{Arrieta2017}.

La estacionariedad de estos procesos garantiza que la media, varianza y estructura de autocorrelación permanezcan invariantes bajo traslaciones temporales, lo que facilita la modelación y el pronóstico \parencite{Hyndman2021FPP3}. Este escenario permite evaluar el desempeño de los métodos en condiciones ideales, donde los supuestos fundamentales de muchas técnicas estadísticas se cumplen.

\subsubsection{Escenario 2: Lineal No Estacionario (ARIMA)}

El segundo escenario aborda procesos autorregresivos integrados de media móvil (ARIMA), que extienden la clase ARMA para series con tendencias estocásticas. La presencia de raíces unitarias en el polinomio autorregresivo genera comportamientos de paseo aleatorio que son comunes en series económicas y financieras \parencite{Hyndman2021FPP3}.

La no estacionariedad introduce desafíos adicionales para el pronóstico probabilístico, ya que la incertidumbre crece sin límite conforme aumenta el horizonte de predicción. Este escenario permite evaluar la capacidad de los métodos para adaptarse a estructuras no estacionarias mediante diferenciación o técnicas adaptativas.

\subsubsection{Escenario 3: No Lineal Estacionario (SETAR)}

El tercer escenario considera modelos autorregresivos de umbral auto-excitados (SETAR), que permiten cambios estructurales endógenos en la dinámica del proceso. Estos modelos capturan no linealidades mediante el cambio de régimen determinado por valores pasados de la propia serie \parencite{Chen2023}.

La estacionariedad global de un proceso SETAR requiere condiciones específicas sobre los parámetros autorregresivos en cada régimen y la frecuencia de transición entre regímenes. Estas condiciones se discuten en detalle en la Sección~\ref{subsec:setar_stationarity}. Este escenario es particularmente relevante para evaluar la capacidad de los métodos conformales de capturar dinámicas asimétricas y dependientes del estado del sistema.

\subsubsection{Ausencia del Escenario No Lineal No Estacionario}

La combinación de no linealidad y no estacionariedad, aunque teóricamente posible, presenta desafíos metodológicos sustanciales que la excluyen del alcance de este estudio. Los modelos que combinan ambas características (por ejemplo, SETAR con raíces unitarias condicionales o modelos de cambio de régimen con deriva) requieren condiciones de estabilidad extremadamente restrictivas y su caracterización teórica es un área de investigación activa \parencite{Chen2023}.

Más fundamentalmente, la validez teórica de muchos métodos de predicción conformal, incluyendo aquellos basados en el enfoque de Barber et al. \parencite{Barber2023}, asume que el proceso subyacente es al menos localmente estacionario o que las desviaciones de la estacionariedad son graduales y pueden ser capturadas mediante esquemas de ponderación adaptativos. La presencia simultánea de cambios estructurales abruptos (no linealidad) y tendencias estocásticas persistentes (no estacionariedad) violaría estos supuestos fundamentales, invalidando las garantías teóricas de cobertura.

Por estas razones, el presente estudio se enfoca en los tres escenarios anteriores, que permiten una evaluación rigurosa y teóricamente fundamentada del desempeño de los métodos.

\subsection{Estructura del Diseño Factorial}
\label{subsec:diseño_factorial}

El diseño experimental implementa un esquema factorial completo que combina sistemáticamente tres dimensiones de variación para cada uno de los tres escenarios considerados. Esta estructura genera un total de 420 configuraciones únicas de simulación, distribuidas equitativamente entre los escenarios.

\subsubsection{Dimensión 1: Configuraciones Paramétricas del Proceso}

Para cada clase de modelo (ARMA, ARIMA, SETAR), se consideran 7 configuraciones paramétricas distintas que representan diferentes grados de complejidad y características dinámicas. Las especificaciones detalladas de estas configuraciones se presentan en la Sección~\ref{sec:procesos_generadores}. Esta diversidad paramétrica permite evaluar la sensibilidad de los métodos a diferentes estructuras de dependencia temporal.

\subsubsection{Dimensión 2: Distribuciones del Término de Error}

Se consideran cinco familias de distribuciones para el término de innovación $\varepsilon_t$, seleccionadas para representar diferentes características de forma, simetría y comportamiento en las colas:

\begin{enumerate}
\item \textbf{Normal:} $\varepsilon_t \sim N(0, \sigma^2)$. Representa el caso base con colas ligeras y simetría perfecta.

\item \textbf{T-Student:} $\varepsilon_t \sim \sigma \cdot \frac{t_{18}}{\sqrt{18/16}}$, donde $t_{18}$ denota una distribución t de Student con 18 grados de libertad. Esta parametrización garantiza varianza unitaria y genera colas más pesadas que la normal, capturando eventos extremos más frecuentes.

\item \textbf{Exponencial:} $\varepsilon_t \sim \sigma(Y - 1)$, donde $Y \sim \text{Exp}(1)$. Produce asimetría positiva y es relevante para series que modelan variables intrínsecamente positivas o con shocks unidireccionales.

\item \textbf{Uniforme:} $\varepsilon_t \sim U(-\sqrt{3}\sigma, \sqrt{3}\sigma)$. Genera soporte acotado y ausencia de colas, representando un caso extremo de curtosis negativa.

\item \textbf{Mixtura de Normales:} $\varepsilon_t \sim 0.75 \cdot N(-\sigma/4, \sigma^2/16) + 0.25 \cdot N(3\sigma/4, \sigma^2/16)$. Produce bimodalidad y permite evaluar el desempeño bajo distribuciones predictivas complejas con múltiples modas.
\end{enumerate}

Esta selección permite evaluar la robustez de los métodos ante desviaciones del supuesto de normalidad que frecuentemente se asume en la literatura de pronóstico \parencite{Arrieta2017}.

\subsubsection{Dimensión 3: Niveles de Varianza del Error}

Se consideran cuatro niveles de varianza $\sigma^2 \in \{0.2, 0.5, 1.0, 3.0\}$ que representan diferentes razones señal-ruido. El nivel base $\sigma^2 = 1.0$ corresponde a la parametrización estándar, mientras que $\sigma^2 = 0.2$ representa un escenario de alta predictibilidad y $\sigma^2 = 3.0$ captura situaciones de alta volatilidad donde la incertidumbre inherente domina la dinámica del sistema.

\subsubsection{Combinatoria Total}

La combinación factorial de estas tres dimensiones genera:
\begin{equation}
N_{\text{config}} = 7 \text{ modelos} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones por escenario}
\end{equation}

Con tres escenarios (ARMA, ARIMA, SETAR), el espacio experimental completo comprende:
\begin{equation}
N_{\text{total}} = 140 \times 3 = 420 \text{ configuraciones únicas}
\end{equation}

Adicionalmente, considerando que cada configuración se evalúa en un horizonte de predicción de 12 pasos usando ventana rodante para que se realice predicción a un paso adelante, el número total de combinaciones configuración-horizonte es de $420 \times 12 = 5040$.

\subsection{Protocolo de Simulación y Partición de Datos}
\label{subsec:protocolo_simulacion}

Para cada una de las 420 configuraciones, se implementa el siguiente protocolo de simulación:

\begin{enumerate}
\item \textbf{Generación de la Serie:} Se simulan $n_{\text{total}} = 302$ observaciones del proceso especificado, precedidas por un período de burn-in de 50 observaciones que se descartan para eliminar el efecto de las condiciones iniciales. Esto resulta en una serie efectiva de longitud $n = 252$.

\item \textbf{Partición Tripartita:} La serie se divide en tres conjuntos disjuntos:
\begin{itemize}
\item \textbf{Conjunto de Entrenamiento:} $n_{\text{train}} = 200$ observaciones iniciales utilizadas para la estimación inicial de parámetros y el ajuste de hiperparámetros.
\item \textbf{Conjunto de Calibración:} $n_{\text{cal}} = 40$ observaciones subsecuentes utilizadas para la calibración de intervalos de predicción y la construcción de distribuciones conformales.
\item \textbf{Conjunto de Prueba:} $n_{\text{test}} = 12$ observaciones finales utilizadas para la evaluación del desempeño predictivo.
\end{itemize}

\item \textbf{Esquema de Ventana Rodante:} La evaluación se realiza mediante una ventana rodante (rolling window) donde:
\begin{itemize}
\item Para el primer paso de predicción, se utilizan las primeras 200 observaciones para entrenamiento y las siguientes 40 para calibración.
\item Para cada paso $h = 1, \ldots, 12$, la ventana de entrenamiento se extiende para incluir las observaciones anteriores, manteniendo fijo el conjunto de calibración de tamaño 40 inmediatamente anterior al punto de predicción.
\item Este esquema emula una situación operativa donde el analista actualiza periódicamente los modelos conforme nueva información se hace disponible.
\end{itemize}

\item \textbf{Generación de Distribuciones Predictivas:} Para cada método y cada paso de predicción $h$, se generan muestras de la distribución predictiva. Estas muestras se comparan con muestras de la distribución teórica verdadera del proceso (conocida por construcción del DGP) mediante el cálculo del ECRPS para ese paso específico.


\end{enumerate}

Este protocolo garantiza que la evaluación sea tanto rigurosa (mediante la comparación con la distribución verdadera) como realista (mediante el esquema de ventana rodante que refleja la práctica operativa).

\section{Procesos Generadores de Datos}
\label{sec:procesos_generadores}

Esta sección describe formalmente los modelos utilizados como procesos generadores de datos en cada escenario, junto con las configuraciones paramétricas específicas consideradas. Para cada clase de modelo, se presentan las ecuaciones fundamentales, las condiciones de estacionariedad (cuando corresponda) y las parametrizaciones concretas evaluadas.
\subsection{Procesos ARMA: Escenario Lineal Estacionario}
\label{subsec:arma}

\subsubsection{Definición y Representación}

Un proceso autorregresivo de media móvil de órdenes $p$ y $q$, denotado ARMA$(p,q)$, se define mediante la ecuación en diferencias estocástica:
\begin{equation}
Y_t = c + \sum_{i=1}^p \phi_i Y_{t-i} + \varepsilon_t + \sum_{j=1}^q \theta_j \varepsilon_{t-j}
\label{eq:arma_general}
\end{equation}
donde $c$ es un término constante, $\{\phi_i\}_{i=1}^p$ son los coeficientes autorregresivos, $\{\theta_j\}_{j=1}^q$ son los coeficientes de media móvil, y $\{\varepsilon_t\}$ es un proceso de ruido blanco con media cero y varianza $\sigma^2$.

Utilizando el operador de rezagos $L$ definido por $L^k Y_t = Y_{t-k}$, el proceso puede expresarse en forma compacta:
\begin{equation}
\Phi(L) Y_t = c + \Theta(L) \varepsilon_t
\label{eq:arma_lag_operator}
\end{equation}
donde $\Phi(L) = 1 - \sum_{i=1}^p \phi_i L^i$ es el polinomio autorregresivo y $\Theta(L) = 1 + \sum_{j=1}^q \theta_j L^j$ es el polinomio de media móvil.

\subsubsection{Condiciones de Estacionariedad e Invertibilidad}

La estacionariedad y la invertibilidad de un proceso ARMA están determinadas por las raíces de sus polinomios característicos \parencite{Hyndman2021FPP3}:

\begin{itemize}
\item \textbf{Estacionariedad:} El proceso es estacionario en covarianza si y solo si todas las raíces del polinomio autorregresivo $\Phi(z) = 0$ se encuentran estrictamente fuera del círculo unitario complejo. Equivalentemente, las raíces del polinomio $\Phi(L)$ deben satisfacer $|z_i| > 1$ para todo $i$.

\item \textbf{Invertibilidad:} El proceso es invertible si y solo si todas las raíces del polinomio de media móvil $\Theta(z) = 0$ se encuentran estrictamente fuera del círculo unitario complejo.
\end{itemize}

Estas condiciones garantizan que el proceso admite representaciones de Wold (MA$(\infty)$) y autorregresiva (AR$(\infty)$) convergentes, lo que es fundamental para la teoría de pronóstico \parencite{Arrieta2017}.

\subsubsection{Distribución Predictiva Verdadera}

Para un proceso ARMA estacionario e invertible, la distribución del siguiente valor $Y_{n+1}$ condicionada a la historia observada $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$ tiene una forma analítica explícita. Dado que el modelo es lineal, la distribución condicional está completamente caracterizada por su media y varianza condicionales.

La media condicional se obtiene de la ecuación estructural del modelo:
\begin{equation}
\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n] = c + \sum_{i=1}^p \phi_i Y_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j}
\label{eq:arma_conditional_mean}
\end{equation}

donde todos los términos del lado derecho son conocidos. La varianza condicional es constante e igual a la varianza del ruido:
\begin{equation}
\text{Var}[Y_{n+1} \mid \mathcal{F}_n] = \sigma^2
\label{eq:arma_conditional_variance}
\end{equation}

Por lo tanto, si el ruido $\varepsilon_t$ sigue una distribución $F$ con media cero y varianza $\sigma^2$, la distribución predictiva verdadera es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F\left(\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\label{eq:arma_predictive_distribution}
\end{equation}

Esta distribución puede evaluarse numéricamente generando una muestra grande de errores futuros $\varepsilon_{n+1}^{(b)} \sim F(0, \sigma^2)$ y calculando:
\begin{equation}
Y_{n+1}^{(b)} = c + \sum_{i=1}^p \phi_i Y_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j} + \varepsilon_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:arma_monte_carlo_samples}
\end{equation}

donde $B$ es un número suficientemente grande (en esta investigación, $B = 1000$). Esta muestra empírica aproxima la distribución predictiva verdadera y sirve como referencia para el cálculo del ECRPS.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:arma_configs} presenta las siete configuraciones ARMA consideradas en este estudio. La selección incluye modelos puramente autorregresivos [AR(1), AR(2)], puramente de media móvil [MA(1), MA(2)], y mixtos [ARMA(1,1), ARMA(2,2), ARMA(2,1)], con diferentes grados de persistencia temporal y complejidad estructural.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos ARMA.}
\label{tab:arma_configs}
\begin{tabular}{lcccc}
\toprule
\textbf{Nombre} & \textbf{$p$} & \textbf{$q$} & \textbf{$\boldsymbol{\phi}$} & \textbf{$\boldsymbol{\theta}$} \\
\midrule
AR(1)     & 1 & 0 & $[0.9]$              & $[]$           \\
AR(2)     & 2 & 0 & $[0.5, -0.3]$        & $[]$           \\
MA(1)     & 0 & 1 & $[]$                 & $[0.7]$        \\
MA(2)     & 0 & 2 & $[]$                 & $[0.4, 0.2]$   \\
ARMA(1,1) & 1 & 1 & $[0.6]$              & $[0.3]$        \\
ARMA(2,2) & 2 & 2 & $[0.4, -0.2]$        & $[0.5, 0.1]$   \\
ARMA(2,1) & 2 & 1 & $[0.7, 0.2]$         & $[0.5]$        \\
\bottomrule
\end{tabular}
\end{table}

Todas las configuraciones fueron verificadas para satisfacer las condiciones de estacionariedad e invertibilidad mediante el cálculo numérico de las raíces de los polinomios característicos correspondientes.

\subsection{Procesos ARIMA: Escenario Lineal No Estacionario}
\label{subsec:arima}

\subsubsection{Definición y Operador de Diferenciación}

Un proceso autorregresivo integrado de media móvil de órdenes $(p,d,q)$, denotado ARIMA$(p,d,q)$, se construye aplicando el operador de diferenciación $\Delta = 1 - L$ un total de $d$ veces a una serie $Y_t$ y modelando la serie diferenciada resultante $W_t = \Delta^d Y_t$ mediante un proceso ARMA$(p,q)$ estacionario:
\begin{equation}
\Phi(L) W_t = c + \Theta(L) \varepsilon_t
\label{eq:arima_general}
\end{equation}
donde $W_t = (1-L)^d Y_t$.

Equivalentemente, en términos de la serie original:
\begin{equation}
\Phi(L)(1-L)^d Y_t = c + \Theta(L) \varepsilon_t
\label{eq:arima_original_scale}
\end{equation}

El orden de integración $d$ representa el número de raíces unitarias en el polinomio autorregresivo ampliado. En la gran mayoría de aplicaciones prácticas, $d \in \{0, 1, 2\}$, siendo $d=1$ el caso más frecuente \parencite{Hyndman2021FPP3}.

\subsubsection{Propiedades de Estacionariedad}

Un proceso ARIMA$(p,d,q)$ es no estacionario por construcción cuando $d > 0$, debido a la presencia de raíces unitarias. Sin embargo, la serie diferenciada $W_t = \Delta^d Y_t$ es estacionaria si el componente ARMA$(p,q)$ subyacente satisface las condiciones de estacionariedad e invertibilidad descritas en la Sección~\ref{subsec:arma}.

Esta propiedad de \textit{estacionariedad en diferencias} es fundamental para el pronóstico, ya que permite aplicar toda la teoría desarrollada para procesos estacionarios a la serie transformada $W_t$, recuperando posteriormente los pronósticos en la escala original mediante integración sucesiva \parencite{Hyndman2021FPP3}.

\subsubsection{Distribución Predictiva Verdadera}

Para un proceso ARIMA$(p,d,q)$, la distribución del siguiente valor $Y_{n+1}$ condicionada a la historia observada se obtiene mediante un procedimiento de dos etapas que explota la estructura de diferenciación del modelo.

Primero, se predice el siguiente valor de la serie diferenciada $W_{n+1} = \Delta^d Y_{n+1}$ usando la distribución ARMA subyacente. Para el caso más común $d=1$, la serie diferenciada es:
\begin{equation}
W_t = Y_t - Y_{t-1}
\end{equation}

y su predicción un paso adelante, condicionada a la historia $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$, sigue la distribución ARMA:
\begin{equation}
W_{n+1} \mid \mathcal{F}_n \sim F\left(\mathbb{E}[W_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\end{equation}

donde:
\begin{equation}
\mathbb{E}[W_{n+1} \mid \mathcal{F}_n] = c + \sum_{i=1}^p \phi_i W_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j}
\label{eq:arima_diff_conditional_mean}
\end{equation}

Segundo, se recupera la predicción en la escala original mediante la relación de integración:
\begin{equation}
Y_{n+1} = Y_n + W_{n+1}
\label{eq:arima_integration}
\end{equation}

Por lo tanto, la distribución predictiva verdadera para $Y_{n+1}$ es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F\left(Y_n + \mathbb{E}[W_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\label{eq:arima_predictive_distribution}
\end{equation}

Esta distribución puede evaluarse numéricamente generando muestras del incremento futuro:
\begin{equation}
W_{n+1}^{(b)} = c + \sum_{i=1}^p \phi_i W_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j} + \varepsilon_{n+1}^{(b)}
\label{eq:arima_diff_samples}
\end{equation}

y aplicando la transformación:
\begin{equation}
Y_{n+1}^{(b)} = Y_n + W_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:arima_level_samples}
\end{equation}

donde $\varepsilon_{n+1}^{(b)} \sim F(0, \sigma^2)$ son errores futuros independientes. Esta muestra empírica representa la distribución predictiva verdadera que sirve como referencia para el ECRPS.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:arima_configs} presenta las siete configuraciones ARIMA$(p,1,q)$ consideradas en este estudio. Todas las configuraciones utilizan $d=1$, reflejando el caso más común en aplicaciones económicas y financieras. La selección incluye desde el paseo aleatorio puro [ARIMA(0,1,0)] hasta modelos con estructura autorregresiva y de media móvil en la serie diferenciada.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos ARIMA.}
\label{tab:arima_configs}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccc}
\toprule
\textbf{Nombre} & \boldmath$p$ & \boldmath$d$ & \boldmath$q$ & \boldmath$\phi$ & \boldmath$\theta$ \\
\midrule
ARIMA(0,1,0) & 0 & 1 & 0 & $[]$              & $[]$              \\
ARIMA(1,1,0) & 1 & 1 & 0 & $[0.6]$           & $[]$              \\
ARIMA(2,1,0) & 2 & 1 & 0 & $[0.5, -0.2]$     & $[]$              \\
ARIMA(0,1,1) & 0 & 1 & 1 & $[]$              & $[0.5]$           \\
ARIMA(0,1,2) & 0 & 1 & 2 & $[]$              & $[0.4, 0.25]$     \\
ARIMA(1,1,1) & 1 & 1 & 1 & $[0.7]$           & $[-0.3]$          \\
ARIMA(2,1,2) & 2 & 1 & 2 & $[0.6, 0.2]$      & $[0.4, -0.1]$     \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Procesos SETAR: Escenario No Lineal Estacionario}
\label{subsec:setar}

\subsubsection{Definición y Mecanismo de Cambio de Régimen}

Un modelo autorregresivo de umbral auto-excitado con dos regímenes, denotado SETAR(2; $p_1$, $p_2$), se define mediante una estructura de cambio de régimen determinado por valores pasados de la propia serie \parencite{Chen2023}:

\begin{equation}
Y_t = 
\begin{cases}
\phi_0^{(1)} + \sum_{i=1}^{p_1} \phi_i^{(1)} Y_{t-i} + \varepsilon_t^{(1)} & \text{si } Y_{t-d} \leq r \\
\phi_0^{(2)} + \sum_{i=1}^{p_2} \phi_i^{(2)} Y_{t-i} + \varepsilon_t^{(2)} & \text{si } Y_{t-d} > r
\end{cases}
\label{eq:setar_general}
\end{equation}

donde:
\begin{itemize}
\item $r$ es el \textit{valor umbral} (threshold value) que determina el cambio de régimen
\item $d$ es el \textit{rezago de umbral} (threshold delay) que especifica qué valor pasado de la serie se utiliza para determinar el régimen activo
\item $\phi_0^{(j)}$ y $\{\phi_i^{(j)}\}_{i=1}^{p_j}$ son los parámetros específicos del régimen $j$
\item $\varepsilon_t^{(j)} \sim WN(0, \sigma_j^2)$ son procesos de ruido blanco que pueden tener varianzas diferentes en cada régimen
\end{itemize}

La notación SETAR(2; $d$, $p$) denota un modelo de dos regímenes con rezago de umbral $d$ y orden autorregresivo común $p$ en ambos regímenes (aunque en general $p_1$ y $p_2$ pueden diferir).

\subsubsection{Estacionariedad en Procesos SETAR}
\label{subsec:setar_stationarity}

La estacionariedad de procesos SETAR es sustancialmente más compleja que en modelos lineales, ya que la dinámica cambia endógenamente según el estado del sistema. Las condiciones suficientes para la estacionariedad han sido objeto de extensa investigación \parencite{Chen2023}.

\paragraph{Caso SETAR(2; 1, 1):} Para el caso más simple de dos regímenes con orden autorregresivo 1, \cite{petruccelli1984consistent} demostraron que el proceso es ergódico si y solo si:
\begin{equation}
|\phi_1^{(1)}| < 1, \quad |\phi_1^{(2)}| < 1, \quad \text{y} \quad |\phi_1^{(1)} \phi_1^{(2)}| < 1
\label{eq:setar11_stationarity}
\end{equation}

Esta condición requiere que cada régimen sea individualmente estable y que el producto de los coeficientes autorregresivos sea menor que uno en valor absoluto. Esta última condición captura el efecto de la interacción entre regímenes.

\paragraph{Caso General SETAR(2; $p_1$, $p_2$):} Para órdenes autorregresivos mayores,\cite{chan1985testing} proporcionaron una condición suficiente basada en el radio espectral de las matrices compañeras:
\begin{equation}
\max_j \sum_{i=1}^{p_j} |\phi_i^{(j)}| < 1
\label{eq:setar_sufficient_condition}
\end{equation}

Sin embargo, esta condición es bastante conservadora. Un criterio más general y menos restrictivo se basa en el concepto de \textit{radio espectral conjunto} (joint spectral radius) de las matrices compañeras de ambos regímenes \parencite{Chen2023}. Sea $\boldsymbol{\Phi}^{(j)}$ la matriz compañera del régimen $j$:

\begin{equation}
\boldsymbol{\Phi}^{(j)} = 
\begin{pmatrix}
\phi_1^{(j)} & \phi_2^{(j)} & \cdots & \phi_{p-1}^{(j)} & \phi_p^{(j)} \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & 0
\end{pmatrix}
\end{equation}

El radio espectral conjunto se define como:
\begin{equation}
\rho(\{\boldsymbol{\Phi}^{(1)}, \boldsymbol{\Phi}^{(2)}\}) = \lim_{k \to \infty} \max \|\boldsymbol{\Phi}^{(i_1)} \cdots \boldsymbol{\Phi}^{(i_k)}\|^{1/k}
\end{equation}
donde el máximo se toma sobre todas las secuencias posibles de $k$ matrices.

El proceso SETAR es estacionario si $\rho(\{\boldsymbol{\Phi}^{(1)}, \boldsymbol{\Phi}^{(2)}\}) < 1$. Este criterio es menos restrictivo que \eqref{eq:setar_sufficient_condition} y permite que algunos regímenes individuales sean incluso explosivos, siempre que la dinámica global del sistema sea estabilizadora \parencite{Chen2023}.

\subsubsection{Distribución Predictiva Verdadera}

La distribución del siguiente valor $Y_{n+1}$ en un proceso SETAR condicionada a la historia observada $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$ depende críticamente del régimen que será activado en el tiempo $n+1$. A diferencia de los modelos lineales, la predicción requiere determinar primero qué régimen gobernará la dinámica futura.

El régimen activo en el tiempo $n+1$ se determina comparando el valor retardado $Y_{n+1-d}$ con el umbral $r$:
\begin{equation}
\text{Régimen}_{n+1} = 
\begin{cases}
1 & \text{si } Y_{n+1-d} \leq r \\
2 & \text{si } Y_{n+1-d} > r
\end{cases}
\label{eq:setar_regime_determination}
\end{equation}

Dado que $Y_{n+1-d}$ ya es conocido en el tiempo $n$ (pues $n+1-d \leq n$ para $d \geq 1$), el régimen futuro es determinístico y no hay incertidumbre sobre cuál dinámica aplicar. Una vez identificado el régimen $j \in \{1, 2\}$, la media condicional se calcula mediante:
\begin{equation}
\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j] = \phi_0^{(j)} + \sum_{i=1}^{p_j} \phi_i^{(j)} Y_{n+1-i}
\label{eq:setar_conditional_mean}
\end{equation}

donde todos los valores $Y_{n+1-i}$ en el lado derecho son observados. La varianza condicional es constante dentro de cada régimen:
\begin{equation}
\text{Var}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j] = \sigma_j^2
\label{eq:setar_conditional_variance}
\end{equation}

Por lo tanto, la distribución predictiva verdadera es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F_j\left(\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j], \sigma_j^2\right)
\label{eq:setar_predictive_distribution}
\end{equation}

donde $F_j$ es la distribución del ruido en el régimen $j$ y el subíndice $j$ se determina mediante \eqref{eq:setar_regime_determination}.

Esta distribución puede evaluarse numéricamente generando una muestra grande de errores futuros específicos del régimen activo:
\begin{equation}
Y_{n+1}^{(b)} = \phi_0^{(j)} + \sum_{i=1}^{p_j} \phi_i^{(j)} Y_{n+1-i} + \varepsilon_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:setar_monte_carlo_samples}
\end{equation}

donde $\varepsilon_{n+1}^{(b)} \sim F_j(0, \sigma_j^2)$ son errores independientes del régimen determinado. A diferencia de los modelos ARMA, aquí no existe incertidumbre sobre el régimen en predicciones un paso adelante, lo que simplifica considerablemente la evaluación de la distribución predictiva verdadera.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:setar_configs} presenta las siete configuraciones SETAR consideradas en este estudio. Las configuraciones incluyen diferentes órdenes autorregresivos, rezagos de umbral y valores de umbral, representando una amplia gama de comportamientos no lineales.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos SETAR.}
\label{tab:setar_configs}
\small
\begin{tabular}{lccccl}
\toprule
\textbf{Nombre} & \textbf{$\boldsymbol{\phi}^{(1)}$} & \textbf{$\boldsymbol{\phi}^{(2)}$} & \textbf{$r$} & \textbf{$d$}  \\
\midrule
SETAR-1 & [0.6] & [-0.5] & 0.0 & 1 \\
SETAR-2 & [0.7] & [-0.7] & 0.0 & 2 \\
SETAR-3 & [0.5, -0.2] & [-0.3, 0.1] & 0.5 & 1 \\
SETAR-4 & [0.8, -0.15] & [-0.6, 0.2] & 1.0 & 2 \\
SETAR-5 & [0.4, -0.1, 0.05] & [-0.3, 0.1, -0.05] & 0.0 & 1 \\
SETAR-6 & [0.5, -0.3, 0.1] & [-0.4, 0.2, -0.05] & 0.5 & 2 \\
SETAR-7 & [0.3, 0.1] & [-0.2, -0.1] & 0.8 & 3 \\
\bottomrule
\end{tabular}
\end{table}

Finalmente, es importante destacar que todas las configuraciones detalladas en la Tabla~\ref{tab:setar_configs} fueron seleccionadas bajo un estricto criterio de estabilidad. Para garantizar el rigor estadístico de las comparaciones en este escenario, se realizó un análisis de estacionariedad basado en el cálculo numérico del radio espectral conjunto ($\rho$) para cada par de matrices compañeras. Se verificó que en la totalidad de los casos empleados en la simulación se cumple la condición $\rho < 1$, asegurando que los procesos SETAR generados son globalmente estacionarios.

% ===========================================================================
% Modelos predictivos
% ===========================================================================

\section{Modelos predictivos}
\label{sec:modelos_predictivos}
Para evaluar la capacidad de cuantificación de la incertidumbre en diversos entornos estocásticos, esta investigación emplea un conjunto heterogéneo de nueve modelos predictivos. Esta selección abarca desde métodos de remuestreo clásicos y propuestas de predicción conformal, hasta arquitecturas de aprendizaje profundo y modelos híbridos de diseño propio. El uso de esta diversidad de enfoques permite contrastar cómo las garantías teóricas de cada familia de modelos se traducen en un rendimiento práctico bajo la métrica ECRPS, especialmente cuando se enfrentan a la ruptura de los supuestos de intercambiabilidad y linealidad.

\subsection{Circular Block Bootstrap (CBB)}
\label{subsec:CCB}
\subsubsection{Explicación Teórica del Modelo}

El método \textit{Circular Block Bootstrap} (CBB), introducido por \textcite{politis1992circular}, representa una evolución metodológica del remuestreo por bloques que aborda una limitación fundamental de los esquemas no circulares como el Moving Block Bootstrap (MBB). Según \textcite{Lahiri2003}, el problema radica en que las observaciones ubicadas en los extremos de la serie temporal $\{X_1, \dots, X_n\}$ aparecen con menor frecuencia en los bloques remuestreados, generando una infra-representación sistemática de los bordes y sesgo en la estimación de varianza.

\paragraph{Fundamento Teórico}
El CBB resuelve esta asimetría mediante la \textit{circunscripción} de los datos: la serie temporal se conceptualiza como una estructura circular donde $X_n$ es seguido inmediatamente por $X_1$, permitiendo la continuidad periódica. Formalmente, para una serie de longitud $n$ y bloques de tamaño $l$, se definen exactamente $n$ bloques posibles:
\begin{equation}
B_i = \{X_i, X_{i+1 \bmod n}, \dots, X_{i+l-1 \bmod n}\}, \quad i = 1,\dots,n
\end{equation}

donde el operador módulo ($\bmod$) implementa la extensión circular. Esta construcción garantiza que cada observación histórica tiene probabilidad idéntica $1/n$ de ser seleccionada como punto de inicio de un bloque, eliminando el sesgo de borde.

\paragraph{Algoritmo de Remuestreo}
El procedimiento de generación de muestras bootstrap opera en dos etapas:

\begin{enumerate}
    \item \textbf{Muestreo de puntos de inicio}: Se seleccionan $B$ índices $\{i_1, \dots, i_B\}$ uniformemente de $\{1,\dots,n\}$, donde $B$ es el número de réplicas bootstrap deseadas.
    \item \textbf{Construcción de bloques circulares}: Cada réplica $X^*_b$ se forma extrayendo el bloque circular iniciado en $i_b$:
    \begin{equation}
    X^*_b = X_{(i_b + r) \bmod n}, \quad r \in \{0, 1, \dots, l-1\}
    \end{equation}
\end{enumerate}

Para pronóstico un paso adelante, la distribución predictiva se aproxima mediante el conjunto de valores finales de cada bloque: $\{\hat{y}_{t+1}^{(1)}, \dots, \hat{y}_{t+1}^{(B)}\}$, donde $\hat{y}_{t+1}^{(b)} = X^*_b$.

\paragraph{Propiedades Estadísticas}
\textcite{Lahiri2003} establece que bajo condiciones de mixing (dependencia que decae con el tiempo), el estimador CBB de la varianza es consistente cuando $l \to \infty$ y $l/n \to 0$ conforme $n \to \infty$. La equiprobabilidad de selección garantiza distribuciones predictivas mejor calibradas en contextos de dependencia temporal, haciendo al CBB apropiado para series financieras y económicas donde la estructura de autocorrelación es relevante.

\paragraph{Clasificación del Modelo}
El CBB es un método \textbf{no paramétrico} puro: no asume ninguna forma funcional para la distribución subyacente de los datos ni estima parámetros poblacionales. La distribución predictiva emerge directamente del remuestreo empírico de la historia observada, preservando las características distribucionales y de dependencia presentes en la muestra sin imponer supuestos estructurales.

\subsubsection{De la Teoría a la Práctica}

La implementación desarrollada introduce tres adaptaciones principales respecto a la formulación teórica estándar:

\paragraph{Adaptación 1: Simplificación del Esquema de Remuestreo}
La teoría clásica del CBB \parencite{politis1992circular} genera bloques completos de longitud $l$ que luego se concatenan para formar series bootstrap de longitud $n$. En contraste, la implementación para pronóstico un paso adelante simplifica el proceso: dado que solo se requiere predecir $\hat{y}_{t+1}$, se muestrea directamente el valor en la posición $(i_b + r) \bmod n$ donde $r = n \bmod l$ representa la posición relativa dentro del último bloque histórico. Esta simplificación reduce la complejidad computacional de $O(Bl)$ a $O(B)$ operaciones de indexación.

\paragraph{Adaptación 2: Selección Automática de $l$}
Mientras que la teoría requiere especificación manual del tamaño de bloque basado en análisis del proceso estocástico subyacente, la implementación incorpora la heurística automática de \textcite{politis2004automatic}: $l \approx 1.5 \times n^{1/3}$, ademas de otros valores de refencia para balancear eficiencia y captura de dependencia.

\paragraph{Adaptación 3: Congelamiento Post-Optimización}
A diferencia de implementaciones estándar que podrían recalcular $l$ en cada paso, el diseño experimental congela el hiperparámetro tras la fase de validación, este congelamiento es crítico para prevenir \textit{data leakage}: re-optimizar en ventanas rolling introduciría información futura en la selección del modelo, violando la evaluación predictiva rigurosa.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del CBB}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría Clásica} & \textbf{Implementación} \\
\midrule
Remuestreo & Bloques completos concatenados & Valor directo $(i + n \bmod l) \bmod n$ \\
Longitud de bloque & Manual / dependiente del contexto & Optimización\\
Actualización de $l$ & No especificada & Congelada post-optimización \\
Complejidad & $O(Bl)$ para serie completa & $O(B)$ para un pronóstico \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}
\paragraph{Hiperparámetro Principal: \texttt{block\_length} ($l$)}
Controla la cantidad de dependencia temporal preservada en las réplicas bootstrap. Su configuración óptima se determina mediante una estrategia de optimización que emplea una búsqueda en grilla. La grilla de valores candidatos para $l$ está definida por las siguientes cuatro opciones, que representan diferentes escalas de dependencia temporal en función del tamaño de muestra $n$:

\begin{enumerate}
    \item $l = 5$, para modelar estructuras de dependencia de corto plazo.
    \item $l = \lfloor 1.5 \cdot n^{1/3} \rfloor$, una aproximación heurística común en métodos de \textit{block bootstrap}.
    \item $l = \lfloor \sqrt{n} \rfloor$, que ofrece un balance entre corto y mediano plazo.
    \item $l = \lfloor n / 5 \rfloor$, diseñada para capturar posibles estructuras de dependencia de largo plazo.
\end{enumerate}

La selección del valor óptimo de $l$ de entre estas cuatro opciones se realiza minimizando el \textbf{CRPS promedio} (ECRPS) sobre un conjunto de validación, conforme a la métrica detallada en la subsección \ref{subsec:ecrps}.

\paragraph{Protocolo de Congelamiento}
Una vez identificado el valor óptimo mediante validación, este se congela y se utiliza de manera fija durante la fase de prueba. Esta lógica garantiza que no haya contaminación de información y que el modelo evaluado sea idéntico al seleccionado durante la validación.


\subsection{Sieve Bootstrap (SB)}
\label{subsec:SB}
\subsubsection{Explicación Teórica del Modelo}

El \textit{Sieve Bootstrap}, introducido por \textcite{Buhlmann1997} y analizado en profundidad por \textcite{Lahiri2003}, representa un enfoque alternativo al remuestreo por bloques para series temporales dependientes. En lugar de preservar la dependencia mediante partición física de la serie, el método emplea una aproximación paramétrica para filtrar la estructura de autocorrelación.

\paragraph{Fundamento Teórico}
El Sieve Bootstrap se fundamenta en el teorema de Wold, que establece que cualquier proceso estocástico estacionario linealmente regular admite una representación autorregresiva de orden infinito, AR($\infty$). Formalmente, para una serie temporal $\{X_t\}$ estacionaria con media $\mu$, existe una representación:
\begin{equation}
X_t - \mu = \sum_{j=1}^{\infty} \phi_j (X_{t-j} - \mu) + \epsilon_t
\end{equation}
donde $\{\epsilon_t\}$ es un proceso de innovaciones i.i.d. con media cero y varianza $\sigma^2$.

En la práctica, esta representación infinita se aproxima mediante un modelo autorregresivo finito AR($p$) donde $p$ crece con el tamaño muestral $n$:
\begin{equation}
X_t = \phi_0 + \sum_{j=1}^{p} \phi_j X_{t-j} + \epsilon_t
\end{equation}

\paragraph{Algoritmo de Remuestreo}
El procedimiento del Sieve Bootstrap opera en tres etapas secuenciales:

\begin{enumerate}
    \item \textbf{Ajuste del tamiz autorregresivo}: Se estima el modelo AR($p$) mediante mínimos cuadrados ordinarios sobre la serie histórica, obteniendo coeficientes $\hat{\phi} = (\hat{\phi}_0, \hat{\phi}_1, \dots, \hat{\phi}_p)$.
    
    \item \textbf{Extracción de residuos}: Se calculan los residuos del modelo ajustado:
    \begin{equation}
    \hat{\epsilon}_t = X_t - \hat{\phi}_0 - \sum_{j=1}^{p} \hat{\phi}_j X_{t-j}, \quad t = p+1, \dots, n
    \end{equation}
    que idealmente deben comportarse como realizaciones i.i.d. Estos residuos se centran: $\tilde{\epsilon}_t = \hat{\epsilon}_t - \bar{\epsilon}$.
    
    \item \textbf{Generación de muestras bootstrap}: Para cada réplica $b = 1, \dots, B$:
    \begin{itemize}
        \item Se remuestrean con reemplazo los residuos centrados: $\epsilon^*_b \sim \{\tilde{\epsilon}_{p+1}, \dots, \tilde{\epsilon}_n\}$
        \item Se genera la predicción un paso adelante:
        \begin{equation}
        \hat{X}_{n+1}^{(b)} = \hat{\phi}_0 + \sum_{j=1}^{p} \hat{\phi}_j X_{n+1-j} + \epsilon^*_b
        \end{equation}
    \end{itemize}
\end{enumerate}

La distribución predictiva empírica está dada por el conjunto $\{\hat{X}_{n+1}^{(1)}, \dots, \hat{X}_{n+1}^{(B)}\}$.

\paragraph{Propiedades de Consistencia}
\textcite{Buhlmann1997} demuestra que bajo condiciones de regularidad (estacionaridad, ergodicidad, y $p = p_n \to \infty$ con $p_n^3/n \to 0$), el Sieve Bootstrap aproxima consistentemente la distribución del estimador de interés. La clave es que el orden $p$ debe crecer suficientemente para capturar la dependencia, pero no tan rápido como para introducir varianza excesiva por sobreparametrización.

\paragraph{Clasificación del Modelo}
El Sieve Bootstrap es un método \textbf{semiparamétrico}: utiliza una estructura paramétrica (el modelo AR) para filtrar la dependencia temporal, pero trata la distribución de los residuos de forma no paramétrica mediante bootstrap empírico. No asume una forma distribucional específica para las innovaciones, solo que sean aproximadamente i.i.d. después del filtrado AR.

\subsubsection{De la Teoría a la Práctica}

La implementación desarrollada incorpora tres adaptaciones clave para el contexto de pronóstico rolling:

\paragraph{Adaptación 1: Selección de Orden Basada en Validación}
Mientras la teoría asintótica sugiere $p \to \infty$ con $n$, la implementación emplea una grilla discreta de órdenes candidatos $p \in \{5, 10, 20\}$ evaluados mediante ECRPS en validación. Esta discretización responde a dos consideraciones prácticas: (i) evitar sobreparametrización en muestras finitas ($n = 200$), y (ii) reducir tiempo computacional frente a búsquedas exhaustivas.

\paragraph{Adaptación 2: Congelamiento de Parámetros AR}
La implementación introduce un mecanismo de congelamiento crítico: durante la fase de calibración, se ajusta el modelo AR($p^*$) con orden óptimo $p^*$ sobre los datos de entrenamiento+calibración combinados, almacenando permanentemente:
\begin{itemize}
    \item Coeficientes autorregresivos: $\hat{\phi}^* = (\hat{\phi}_0^*, \dots, \hat{\phi}_{p^*}^*)$
    \item Residuos centrados: $\tilde{\epsilon}^* = \{\tilde{\epsilon}_{p^*+1}, \dots, \tilde{\epsilon}_{n_{\text{calib}}}\}$
\end{itemize}

En cada ventana rolling subsecuente, se reutilizan $\hat{\phi}^*$ y $\tilde{\epsilon}^*$ sin re-estimación, aplicando solo los últimos $p^*$ valores observados para generar la predicción. Esta estrategia previene data leakage y reduce variabilidad numérica.

\paragraph{Adaptación 3: Predicción Secuencial Eficiente}
En lugar de generar series bootstrap completas de longitud $n$ (complejidad $O(Bn)$), la implementación genera directamente predicciones un paso adelante (complejidad $O(B)$): dado el vector de historia reciente $\mathbf{X}_{n-p^*:n} = (X_{n-p^*+1}, \dots, X_n)$, cada predicción se calcula como:
\begin{equation}
\hat{X}_{n+1}^{(b)} = \hat{\phi}_0^* + \sum_{j=1}^{p^*} \hat{\phi}_j^* X_{n+1-j} + \epsilon^*_b
\end{equation}
donde $\epsilon^*_b$ se muestrea de $\tilde{\epsilon}^*$ con reemplazo.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del Sieve Bootstrap}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría Clásica} & \textbf{Implementación} \\
\midrule
Orden AR & $p \to \infty$ con $n$ & Grilla discreta $\{5, 10, 20\}$\\
Selección de $p$ & Criterios asintóticos & Validación cruzada (ECRPS) \\
Parámetros $\hat{\phi}$ & Re-estimados en cada muestra & Congelados post-calibración \\
Residuos & Recalculados dinámicamente & Pool fijo $\tilde{\epsilon}^*$ \\
Complejidad & $O(Bn)$ & $O(B)$ \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Hiperparámetro Principal: \texttt{order} ($p$)}
Define la profundidad del tamiz autorregresivo, controlando cuánta memoria del proceso se captura. Durante la fase de validación se evalúan tres configuraciones sobre $n_{\text{train}} = 200$:
\begin{itemize}
    \item $p = 5$ (dependencias de corto plazo, hasta una semana)
    \item $p = 10$ (memoria intermedia, aproximadamente dos semanas)
    \item $p = 20$ (dependencias extendidas, un mes)
\end{itemize}

\paragraph{Métrica de Optimización}
La selección del orden óptimo $p^*$ se realiza minimizando el \textbf{ECRPS} (véase \ref{subsec:ecrps}) sobre el conjunto de validación. El orden seleccionado es aquel que produce las distribuciones predictivas más calibradas durante la fase de validación.

\paragraph{Protocolo de Congelamiento}
Una vez identificado $p^*$ mediante validación, el método ajusta el modelo AR($p^*$) sobre los datos de entrenamiento+calibración y almacena permanentemente los coeficientes $\hat{\phi}^*$ y residuos centrados $\tilde{\epsilon}^*$. Estos parámetros se reutilizan sin re-estimación en toda la fase de prueba rolling, previniendo data leakage y garantizando evaluación rigurosa.

\paragraph{Parámetros Operacionales}
\begin{itemize}
    \item \texttt{n\_boot} ($B$): Número de réplicas bootstrap. Por defecto: $B = 1000$.
    \item \texttt{random\_state}: Semilla para reproducibilidad del generador aleatorio.
    \item \texttt{verbose}: Control de mensajes diagnósticos durante congelamiento.
\end{itemize}


\subsection{Least Squares Prediction Machine (LSPM)}
\label{subsec:LSPM}
\subsubsection{Explicación Teórica del Modelo}

El \textit{Least Squares Prediction Machine} (LSPM), introducido por \textcite{Vovk2022}, representa una evolución de la predicción conformal que trasciende la generación de intervalos de confianza para construir distribuciones predictivas completas. A diferencia de los predictores conformales estándar, el LSPM se define como un Sistema Predictivo Conformal (CPS), cuya salida es una Función de Distribución Predictiva Conformal (CPD).

\paragraph{Fundamento Teórico: Predicción Conformal}
La predicción conformal se fundamenta en el principio de intercambiabilidad: dada una secuencia de pares $(x_1, y_1), \dots, (x_{n-1}, y_{n-1})$ y un nuevo objeto $x_n$, se asume que para cualquier etiqueta candidata $y$, la secuencia aumentada $(x_1, y_1), \dots, (x_{n-1}, y_{n-1}), (x_n, y)$ es intercambiable. Bajo este supuesto, se puede construir una distribución predictiva válida sin asumir una forma paramétrica específica para los errores.

El LSPM utiliza mínimos cuadrados ordinarios (OLS) como algoritmo subyacente. La versión más robusta es el \textbf{LSPM Studentizado}, que emplea los elementos diagonales de la matriz de proyección (matriz hat) $\bar{H}$ para normalizar los residuos. Según \textcite{Vovk2022}, esta normalización garantiza que la distribución predictiva sea monótonamente creciente, incluso cuando el nuevo objeto posee alto apalancamiento (\textit{leverage}).

\paragraph{Construcción de la Distribución Predictiva}
Para un conjunto de datos aumentado que incluye $n-1$ ejemplos de entrenamiento $(x_1, y_1), \dots, (x_{n-1}, y_{n-1})$ y un nuevo objeto $x_n$ con etiqueta hipotética $y$, se construye la matriz de diseño aumentada:
\begin{equation}
\bar{X} = \begin{pmatrix}
1 & x_1^T \\
\vdots & \vdots \\
1 & x_{n-1}^T \\
1 & x_n^T
\end{pmatrix}
\end{equation}

La matriz hat se define como:
\begin{equation}
\bar{H} = \bar{X}(\bar{X}^T\bar{X})^{-1}\bar{X}^T
\end{equation}

donde $h_{i,j}$ denota el elemento en la fila $i$ y columna $j$ de $\bar{H}$.

\paragraph{Valores Críticos Studentizados}
La distribución predictiva se construye mediante valores críticos $C_i$ que actúan como puntos de salto de una función escalonada. Para la versión studentizada, según las ecuaciones 7.15 y 7.16 de \textcite{Vovk2022}:

\begin{equation}
C_i = \frac{A_i}{B_i}, \quad i = 1, \dots, n-1
\end{equation}

donde:
\begin{equation}
B_i = \sqrt{1 - h_{n,n}} + \frac{h_{i,n}}{\sqrt{1 - h_{i,i}}}
\end{equation}

\begin{equation}
A_i = \frac{\sum_{j=1}^{n-1} h_{j,n} y_j}{\sqrt{1 - h_{n,n}}} + \frac{y_i - \sum_{j=1}^{n-1} h_{i,j} y_j}{\sqrt{1 - h_{i,i}}}
\end{equation}

El término $A_i$ combina dos componentes: (i) la predicción OLS estándar sobre el punto nuevo, normalizada por su leverage, y (ii) el residuo studentizado del punto $i$ en un ajuste leave-one-out. El término $B_i$ normaliza esta combinación considerando el leverage tanto del punto nuevo ($h_{n,n}$) como del punto histórico ($h_{i,i}$) y su covarianza ($h_{i,n}$).

\paragraph{Propiedades Estadísticas}
\textcite{Vovk2022} demuestra que bajo intercambiabilidad, la función de distribución construida mediante estos valores críticos es estadísticamente válida: para cualquier nivel de confianza $\alpha$, el intervalo de predicción conformal tiene cobertura exacta $1-\alpha$ en expectativa sobre la secuencia intercambiable. La studentización es crucial para mantener esta validez incluso cuando $h_{n,n} \to 1$ (leverage extremo del punto de prueba).

\paragraph{Clasificación del Modelo}
El LSPM es un método \textbf{no paramétrico} basado en distribución-libre (\textit{distribution-free}). Aunque utiliza OLS como algoritmo subyacente, no asume ninguna forma distribucional para los errores $\epsilon_i$. La validez estadística proviene únicamente del supuesto de intercambiabilidad, no de supuestos gaussianos o paramétricos sobre los residuos.

\subsubsection{De la Teoría a la Práctica}

La implementación desarrollada adapta el marco teórico de Vovk al contexto específico de pronóstico en series temporales:

\paragraph{Adaptación 1: Construcción Autorregresiva}
La teoría original asume vectores de características $x_i$ independientes. En series temporales, se construyen objetos dinámicamente mediante retardos: dado $p$ lags, cada observación se transforma en un vector autorregresivo:
\begin{equation}
x_t = (y_{t-1}, y_{t-2}, \dots, y_{t-p})^T
\end{equation}

Esto convierte al LSPM en un predictor conformal autorregresivo AR($p$), donde la matriz de diseño se construye dinámicamente en cada ventana rolling usando los últimos $n$ valores disponibles.

\paragraph{Adaptación 2: Estabilidad Numérica}
Para calcular $\bar{H} = \bar{X}(\bar{X}^T\bar{X})^{-1}\bar{X}^T$, la implementación emplea la pseudoinversa de Moore-Penrose en lugar de la inversa estándar. Esta decisión es crítica: series temporales con alta autocorrelación generan matrices $\bar{X}^T\bar{X}$ casi singulares (número de condición alto), causando inestabilidad numérica en la inversión directa. La pseudoinversa provee una solución regularizada que previene errores computacionales.

\paragraph{Adaptación 3: Filtrado de Singularidades}
La teoría requiere que $h_{i,i} < 1$ y $h_{n,n} < 1$ para que los denominadores en $A_i$ y $B_i$ sean no nulos. La implementación incorpora dos mecanismos de seguridad:
\begin{itemize}
    \item Umbral de tolerancia: Se filtran observaciones con $|1 - h_{i,i}| < 10^{-10}$ o $|1 - h_{n,n}| < 10^{-10}$.
    \item Filtrado de divisores: Se descartan valores críticos donde $|B_i| < 10^{-10}$.
\end{itemize}

Estos filtros previenen divisiones por cero cuando un punto es tan influyente que el modelo lo ajusta sin residuo.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del LSPM}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría Clásica} & \textbf{Implementación} \\
\midrule
Vectores $x_i$ & Características independientes & Retardos AR: $(y_{t-1}, \dots, y_{t-p})$ \\
Cálculo de $\bar{H}$ & Inversa $(\bar{X}^T\bar{X})^{-1}$ & Pseudoinversa (Moore-Penrose) \\
Singularidades & Supuesto $h_{i,i}, h_{n,n} < 1$ & Filtrado explícito ($10^{-10}$) \\
Valores críticos & Todos los $n-1$ puntos & Solo puntos con $B_i$ válido\\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Parámetro Principal: \texttt{n\_lags} ($p$)}
Define el orden autorregresivo del modelo, controlando cuántos retardos se usan para construir la matriz de diseño. La implementación emplea la heurística:
\begin{equation}
p = \max\left(1, \lfloor n^{1/3} \rfloor\right)
\end{equation}

Esta regla está motivada por consideraciones teóricas de métodos de tamiz (sieve methods) en estadística no paramétrica, donde el número de parámetros $p$ debe crecer con el tamaño muestral $n$ pero a una tasa controlada que preserve consistencia. La tasa $n^{1/3}$ es estándar en la literatura de bootstrap para series temporales \parencite{Buhlmann1997, politis2004automatic}, garantizando que $p \to \infty$ conforme $n \to \infty$, pero manteniendo $p^3/n \to 0$ para evitar sobreajuste. Esta misma tasa aparece en la heurística de longitud de bloque del CBB, reflejando un principio unificado: el número de parámetros debe escalar subcuadráticamente con la muestra. Para $n = 200$, resulta $p \approx 5$ lags, suficiente para capturar autocorrelaciones de corto plazo sin saturar los grados de libertad del modelo OLS subyacente.

\paragraph{Métrica de Optimización}
Dado que el LSPM es un método conformal teóricamente válido sin hiperparámetros libres (la versión studentizada es la única apropiada según \textcite{Vovk2022}), la optimización se limita a seleccionar $p$ mediante la heurística anterior.

\paragraph{Protocolo de Congelamiento}
El método ejecuta una operación crítica: congela $p$ basado en el tamaño del conjunto de entrenamiento+calibración. Este valor congelado $p^*$ se almacena  y se reutiliza en todas las ventanas rolling subsecuentes, garantizando que la dimensionalidad de la matriz de diseño permanezca constante y previniendo data leakage.

\paragraph{Parámetros Operacionales}
\begin{itemize}
    \item \texttt{version}: Fijado permanentemente como \texttt{'studentized'}. Es la única versión que garantiza validez conformal sin restricciones adicionales sobre leverage.
    \item \texttt{random\_state}: Semilla para reproducibilidad del generador aleatorio (usado en muestreo posterior de la CPD si se requiere).
    \item \texttt{verbose}: Control de mensajes diagnósticos durante congelamiento.
\end{itemize}

```latex

\subsection{Least Squares Prediction Machine with Weighted Residuals (LSPMW)}
\label{subsec:LSPMW}
\subsubsection{Explicación Teórica del Modelo}

El \textit{Least Squares Prediction Machine with Weighted Residuals} (LSPMW) constituye una extensión del LSPM diseñada para contextos donde el supuesto de intercambiabilidad se viola por deriva distributiva (\textit{distribution drift}) o no estacionaridad. Esta variante se fundamenta en los desarrollos de \textcite{Barber2023} sobre predicción conformal no intercambiable.

\paragraph{Fundamento Teórico: Cuantiles Ponderados}
\textcite{Barber2023} demuestran que cuando la intercambiabilidad falla, la pérdida de cobertura (\textit{coverage gap}) de un predictor conformal puede acotarse mediante la distancia de variación total entre distribuciones. Para mitigar este problema en presencia de deriva temporal, proponen sustituir la distribución empírica uniforme por una distribución empírica ponderada.

Formalmente, dado un conjunto de valores críticos (residuos conformales) $\{C_1, \dots, C_{n-1}\}$ ordenados cronológicamente, se asignan pesos temporales no uniformes $w_i$ que priorizan observaciones recientes. La función de distribución empírica ponderada se define como:
\begin{equation}
\hat{F}_n(y) = \sum_{i=1}^{n-1} \tilde{w}_i \cdot \mathbbm{1}(C_i \leq y)
\end{equation}

donde $\tilde{w}_i = w_i / \sum_{j=1}^{n-1} w_j$ son los pesos normalizados.

\paragraph{Esquema de Decaimiento Geométrico}
Para deriva gradual, \textcite{Barber2023} recomiendan un esquema de decaimiento geométrico:
\begin{equation}
w_i = \rho^{n-1-i}, \quad \rho \in (0, 1)
\end{equation}

donde $i$ indexa los residuos en orden cronológico (de más antiguo a más reciente). El hiperparámetro $\rho$ controla la tasa de olvido:
\begin{itemize}
    \item $\rho \to 1$: Convergencia al LSPM uniforme (memoria larga)
    \item $\rho \to 0$: Concentración extrema en el pasado inmediato (memoria corta)
\end{itemize}

El peso efectivo del $i$-ésimo residuo decrece exponencialmente conforme retrocedemos en el tiempo, otorgando a la observación más reciente ($i = n-1$) peso $\rho^0 = 1$, y a la más antigua ($i = 1$) peso $\rho^{n-2}$.

\paragraph{Cuantiles Ponderados}
Para un nivel de cobertura $\alpha$, el cuantil $(1-\alpha)$ de la distribución ponderada se obtiene mediante:
\begin{equation}
q_{1-\alpha} = \inf\left\{y : \sum_{i: C_i \leq y} \tilde{w}_i \geq 1-\alpha\right\}
\end{equation}

Este cuantil ponderado adapta la región de predicción conforme la distribución subyacente cambia en el tiempo.

\paragraph{Propiedades de Cobertura}
\textcite{Barber2023} establecen que bajo deriva Lipschitz-continua con constante $L$, el error de cobertura del predictor ponderado satisface:
\begin{equation}
\left|\mathbb{P}(Y_{n+1} \in \hat{C}_{1-\alpha}) - (1-\alpha)\right| \leq O\left(\frac{L}{\rho} + \frac{1}{\sqrt{n_{\text{eff}}}}\right)
\end{equation}

donde $n_{\text{eff}} = (\sum_i \tilde{w}_i^2)^{-1}$ es el tamaño de muestra efectivo. Esta cota revela el trade-off: $\rho$ pequeño reduce sesgo por deriva ($L/\rho$ disminuye) pero aumenta varianza ($n_{\text{eff}}$ disminuye).

\paragraph{Clasificación del Modelo}
El LSPMW es un método \textbf{no paramétrico adaptativo}. Mantiene la propiedad distribution-free del LSPM (no asume forma distribucional de errores) pero introduce un mecanismo de ponderación que adapta la distribución predictiva a cambios temporales sin modelar explícitamente la deriva.

\subsubsection{De la Teoría a la Práctica}

La implementación desarrollada traduce la teoría de \textcite{Barber2023} mediante un esquema de \textbf{muestreo ponderado adaptativo} que recalcula dinámicamente los residuos conformales:

\paragraph{Adaptación 1: Muestreo Ponderado vs. Expansión por Replicación}
A diferencia de métodos que pre-computan una distribución expandida, la implementación emplea \textit{muestreo estratificado} en tiempo real. Para generar una distribución predictiva de tamaño $N = 1000$, se ejecuta:
\begin{equation}
\tilde{R}_j \sim \text{Categorical}(\{C_1, \dots, C_n\}, \{\tilde{w}_1, \dots, \tilde{w}_n\}), \quad j = 1, \dots, N
\end{equation}

donde cada muestra $\tilde{R}_j$ se extrae de los valores críticos $\{C_i\}$ con probabilidades proporcionales a los pesos normalizados $\tilde{w}_i = w_i / \sum_k w_k$. Este enfoque es matemáticamente equivalente a muestrear de la distribución empírica ponderada $\hat{F}_n(y)$.

\paragraph{Adaptación 2: Recálculo Dinámico de Residuos}
Contrario a esquemas de congelamiento completo, la implementación mantiene únicamente los hiperparámetros ($\rho^*$, $p^*$) fijos tras calibración. En cada paso del rolling forecast:
\begin{enumerate}
    \item Se recalculan valores críticos $\{C_1, \dots, C_m\}$ usando la ventana temporal actual.
    \item Se actualizan pesos $w_i = (\rho^*)^{m-1-i}$ para $i = 1, \dots, m$ (ajustados al tamaño $m$ de la ventana).
    \item Se normalizan: $\tilde{w}_i = w_i / \sum_j w_j$.
    \item Se genera la distribución predictiva mediante muestreo con reemplazo usando estos pesos actualizados.
\end{enumerate}

Este diseño permite que el modelo se adapte continuamente a derivas distributivas, pues los residuos más recientes (calculados con datos actuales) reciben mayor ponderación automáticamente.

\paragraph{Adaptación 3: Protocolo de Congelamiento Parcial}
El método \texttt{freeze\_hyperparameters()} almacena únicamente:
\begin{itemize}
    \item \texttt{\_frozen\_rho}: Valor óptimo $\rho^*$ seleccionado por validación
    \item \texttt{\_frozen\_n\_lags}: Orden autorregresivo $p^*$ heredado de LSPM
\end{itemize}

\textit{No} se congelan los residuos conformales ni sus pesos asociados, permitiendo que la distribución predictiva refleje el estado más reciente de la serie temporal. Esta filosofía difiere del LSPM estándar, donde congelar la distribución completa es apropiado bajo intercambiabilidad, pero resulta inadecuado bajo deriva continua.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del LSPMW}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría (Barber et al.)} & \textbf{Implementación} \\
\midrule
Salida & Cuantiles ponderados específicos & Distribución completa (1000 muestras) \\
Método de generación & Definición abstracta $\hat{F}_n(y)$ & Muestreo ponderado con reemplazo \\
Actualización & Marco teórico general & Recálculo dinámico de residuos \\
Residuos & Valores críticos conceptuales & LSPM studentizados recalculados \\
Optimización & $\rho$ teórico o fijo & Búsqueda por validación (ECRPS) \\
Congelamiento & No especificado & Solo hiperparámetros ($\rho^*$, $p^*$) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Hiperparámetro Principal: \texttt{rho} ($\rho$)}
Controla la tasa de decaimiento temporal de los pesos. Durante validación se evalúan valores en el rango $[0.90, 0.99]$:
\begin{itemize}
    \item $\rho = 0.90$: Adaptación rápida, memoria efectiva $\approx 10$ observaciones
    \item $\rho = 0.95$: Balance intermedio, memoria efectiva $\approx 20$ observaciones  
    \item $\rho = 0.99$: Adaptación lenta, cercano al LSPM uniforme
\end{itemize}

El tamaño efectivo de muestra bajo decaimiento geométrico es:
\begin{equation}
n_{\text{eff}} = \frac{1}{\sum_{i=1}^{n-1} \tilde{w}_i^2} \approx \frac{1-\rho}{1-\rho^n}
\end{equation}

\paragraph{Métrica de Optimización}
La selección de $\rho^*$ se realiza minimizando el \textbf{ECRPS} (véase \ref{subsec:ecrps}) sobre el conjunto de validación. Para cada candidato $\rho$, se genera la distribución ponderada en cada tiempo de validación mediante muestreo estratificado y se evalúa la calibración predictiva.

\paragraph{Protocolo de Congelamiento Parcial}
El método \texttt{freeze\_hyperparameters()} ejecuta:
\begin{enumerate}
    \item Congela $p^*$ (número de lags) heredado de la clase base LSPM.
    \item Identifica y almacena $\rho^*$ óptimo del proceso de validación.
    \item Activa bandera \texttt{\_is\_frozen} para uso de hiperparámetros congelados.
    \item \textit{No congela} valores críticos ni pesos, preservando adaptabilidad temporal.
\end{enumerate}

Durante la fase de prueba rolling, el método \texttt{fit\_predict()}:
\begin{enumerate}
    \item Recalcula residuos conformales $\{C_1, \dots, C_m\}$ usando $p^*$ sobre la ventana actual.
    \item Computa pesos actualizados: $w_i = (\rho^*)^{m-1-i}$ para $i = 1, \dots, m$.
    \item Normaliza: $\tilde{w}_i = w_i / \sum_j w_j$.
    \item Genera 1000 muestras mediante \texttt{np.random.choice} con probabilidades $\{\tilde{w}_i\}$.
\end{enumerate}

\paragraph{Parámetros Operacionales}
\begin{itemize}
    \item \texttt{n\_samples\_target}: Tamaño de la distribución sintética. Fijado en 1000 para resolución suficiente en cálculo de ECRPS.
    \item \texttt{n\_lags} ($p$): Heredado de LSPM, determina orden autorregresivo. Congelado como $p = \lfloor n^{1/3} \rfloor$.
    \item \texttt{random\_state}: Semilla para muestreo estocástico reproducible.
    \item \texttt{verbose}: Control de mensajes diagnósticos.
\end{itemize}

\paragraph{Justificación del Diseño Adaptativo}
El recálculo dinámico de residuos responde a la naturaleza del problema: bajo deriva continua, residuos históricos pierden relevancia predictiva progresivamente. Mientras que congelar $\rho^*$ captura la estructura óptima de decaimiento temporal identificada en validación, mantener los residuos actualizados asegura que la información más reciente sobre el proceso generador de datos se incorpore a cada predicción. Este balance entre estabilidad estructural (hiperparámetros fijos) y adaptabilidad táctica (residuos dinámicos) es la contribución clave de la implementación al marco teórico de \textcite{Barber2023}.

\subsection{Mondrian Conformal Predictive System (MCPS)}
\label{subsec:MCPS}
\subsubsection{Explicación Teórica del Modelo}

El \textit{Mondrian Conformal Predictive System} (MCPS), formalizado por \textcite{Bostrom2021}, constituye una extensión localmente adaptativa del Sistema Predictivo Conformal estándar (SCPS). A diferencia del SCPS, que asume homogeneidad en la distribución de errores sobre todo el espacio de entrada, el MCPS reconoce que la incertidumbre predictiva varía significativamente según el régimen operativo del modelo. Este enfoque resulta especialmente relevante en aplicaciones reales donde, por ejemplo, predicciones en rangos bajos del modelo pueden exhibir patrones de error distintos a predicciones en rangos altos.

\paragraph{Fundamento: Particionamiento Mondrian}

La innovación central radica en la estrategia de \textbf{particionamiento Mondrian} \parencite{Vovk2005}, que divide el conjunto de calibración $\mathcal{D}_c$ en subconjuntos disjuntos basándose en características compartidas de las predicciones. Sea $h: \mathcal{X} \rightarrow \mathbb{R}$ un modelo de regresión entrenado, y $B \in \mathbb{N}$ el número de bins. Se define una partición:

\begin{equation}
\mathcal{D}_c = \bigcup_{\kappa=1}^{B} \mathcal{D}_c^{\kappa}, \quad \mathcal{D}_c^{\kappa} \cap \mathcal{D}_c^{\kappa'} = \emptyset \text{ para } \kappa \neq \kappa'
\end{equation}

Cada subconjunto $\mathcal{D}_c^{\kappa}$ agrupa instancias $(x_j, y_j)$ cuyas predicciones $h(x_j)$ pertenecen al mismo rango, construido mediante cuantiles empíricos:

\begin{equation}
\mathcal{D}_c^{\kappa} = \left\{ (x_j, y_j) \in \mathcal{D}_c \,:\, q_{\frac{\kappa-1}{B}} \leq h(x_j) < q_{\frac{\kappa}{B}} \right\}
\end{equation}

donde $q_p$ denota el cuantil $p$ de las predicciones $\{h(x_j)\}_{j=1}^{N_c}$. Esta estrategia captura automáticamente heterogeneidad: instancias con predicciones similares comparten probablemente patrones de error similares.

\paragraph{Cálculo Localizado de Scores Conformales}

Para una instancia de prueba $x$ con predicción $h(x)$, se determina su bin correspondiente $\kappa^*$:

\begin{equation}
\kappa^* = \arg\min_{\kappa} \left\{ \kappa \,:\, h(x) < q_{\frac{\kappa}{B}} \right\}
\end{equation}

Los \textbf{calibration scores} se calculan únicamente sobre el subconjunto local $\mathcal{D}_c^{\kappa^*}$:

\begin{equation}
C_j^{\kappa^*} = h(x) + (y_j - h(x_j)), \quad \forall (x_j, y_j) \in \mathcal{D}_c^{\kappa^*}
\end{equation}

Esta formulación es algebraicamente idéntica al SCPS, pero la diferencia crítica reside en que los residuos históricos provienen exclusivamente de casos con comportamiento predictivo similar. Matemáticamente, se estima la distribución condicional $P(\epsilon \mid h(x) \in \text{Bin}_{\kappa})$ localmente, en lugar de globalmente como en SCPS donde se asume $P(\epsilon \mid h(x)) = P(\epsilon)$.

\paragraph{Construcción de la Distribución Predictiva}

Según \textcite{Vovk2022}, el conjunto de scores ordenados $C_{(1)}^{\kappa^*} < C_{(2)}^{\kappa^*} < \cdots < C_{(N_c^{\kappa^*})}^{\kappa^*}$ define una distribución empírica que aproxima la distribución predictiva verdadera. Para cualquier función medible $g$:

\begin{equation}
\mathbb{E}[g(Y) \mid x \in \text{Bin}_{\kappa^*}] \approx \frac{1}{N_c^{\kappa^*}} \sum_{j=1}^{N_c^{\kappa^*}} g(C_j^{\kappa^*})
\end{equation}

con error que converge a cero cuando $N_c^{\kappa^*} \to \infty$. Esto implica que cualquier estadístico (media, varianza, cuantiles) puede calcularse directamente sobre los scores sin reconstruir una CDF completa.

\paragraph{Garantías de Cobertura Local}

\textcite{Bostrom2021} demuestran que, bajo intercambiabilidad dentro de cada bin, se logra \textbf{cobertura condicional válida} en cada estrato. Para cualquier nivel $\alpha$:

\begin{equation}
\mathbb{P}\left(Y \in \left[\hat{F}_{\kappa}^{-1}(\alpha/2 \mid x), \hat{F}_{\kappa}^{-1}(1-\alpha/2 \mid x)\right] \,\big|\, x \in \text{Bin}_{\kappa}\right) \geq 1 - \alpha
\end{equation}

Las bandas de predicción se \textbf{ajustan automáticamente}: regiones donde el modelo es confiable producen intervalos estrechos; regiones con alta variabilidad generan intervalos amplios. Si denotamos $W_{\kappa}(x)$ como el ancho del intervalo:

\begin{equation}
W_{\kappa_1}(x_1) \neq W_{\kappa_2}(x_2) \quad \text{si} \quad \text{Var}(\epsilon \mid x \in \text{Bin}_{\kappa_1}) \neq \text{Var}(\epsilon \mid x \in \text{Bin}_{\kappa_2})
\end{equation}

Esta propiedad contrasta con el SCPS, donde $W(x) \approx \text{constante}$ para todo $x$, resultando en sobre-cobertura en regiones de baja incertidumbre o sub-cobertura en regiones de alta incertidumbre.

\paragraph{Naturaleza No Paramétrica}

El MCPS es un modelo \textbf{no paramétrico}. No asume forma funcional específica para la distribución de errores ni para la relación entre predictores y respuesta. La distribución predictiva se construye enteramente a partir de datos empíricos mediante el conjunto de scores conformales, sin parámetros poblacionales a estimar. El único modelo subyacente es el regressor base $h(x)$ (que puede ser paramétrico o no), pero la construcción de intervalos conformales es completamente libre de distribución.

\subsubsection{De la Teoría a la Práctica}

La implementación del MCPS para series temporales autorregresivas representa una \textbf{contribución metodológica novedosa}, traduciendo el framework teórico—originalmente diseñado para datos independientes en logística \parencite{Ye2025}—hacia contextos con dependencias temporales.

\paragraph{Adaptaciones Principales}

\textbf{(1) Construcción Dinámica de Features:} Mientras que \textcite{Ye2025} asumen features pre-existentes $x_i$ observables (ubicación, peso, transportista), en series temporales los ``objetos'' se construyen dinámicamente como ventanas deslizantes de $p$ rezagos:

\begin{equation}
x_t = [y_{t-p}, y_{t-p+1}, \ldots, y_{t-1}] \in \mathbb{R}^p
\end{equation}

Esta transformación convierte la serie univariada en una matriz autoregresiva, introduciendo dependencias inherentes entre filas. La validez se preserva bajo condiciones de \textit{mixing débil} \parencite{Yu1994}, donde observaciones suficientemente separadas son ``casi independientes''.

\textbf{(2) Binning Adaptativo:} En series con baja variabilidad, las predicciones pueden concentrarse en rangos estrechos. Se emplea binning tolerante a duplicados, donde el número efectivo de bins es:

\begin{equation}
B_{\text{efectivo}} = \left|\left\{q_{\frac{\kappa}{B}} : \kappa = 1, \ldots, B-1\right\}\right| \leq B
\end{equation}

Si el binning falla completamente (predicciones idénticas), el sistema degrada a SCPS global automáticamente.

\textbf{(3) Representación Discreta:} En lugar de generar una CDF continua con suavizado $\tau$ (como en \texttt{Crepes} \parencite{Bostrom2022}), se retornan directamente los scores $\{C_j^{\kappa^*}\}$. Esta representación es computacionalmente eficiente para calcular ECRPS:


\textbf{(4) Fallback Jerárquico:} Si el bin localizado $\kappa^*$ contiene menos de 5 observaciones, se usa el conjunto completo:

\begin{equation}
\mathcal{D}_c^{\text{efectivo}} = 
\begin{cases}
\mathcal{D}_c^{\kappa^*} & \text{si } N_c^{\kappa^*} \geq 5 \\
\mathcal{D}_c & \text{si } N_c^{\kappa^*} < 5
\end{cases}
\end{equation}

Esta heurística, no especificada en \textcite{Bostrom2021}, previene intervalos erráticamente anchos por tamaño de muestra insuficiente.

\begin{table}[htbp]
\centering
\caption{Comparación entre MCPS Teórico y su Implementación para Series Temporales}
\label{tab:mcps_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5.5cm}}
\toprule
\textbf{Aspecto} & \textbf{Teoría (Ye et al., 2025)} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Dominio & Logística (órdenes independientes) & Series temporales autorregresivas \\
Features & Pre-existentes observables & Dinámicas (ventanas deslizantes) \\
Librería conformal & \texttt{Crepes} \parencite{Bostrom2022} & Implementación directa de ecuaciones \\
Representación CPD & CDF continua con suavizado $\tau$ & Distribución empírica discreta exacta \\
Binning robusto & Cuantiles fijos & Fusión automática de bins colapsados \\
Fallback a SCPS & No mencionado & Automático si $N_c^{\kappa} < 5$ \\
Horizonte & Batch (predicciones simultáneas) & Secuencial (rolling forecast) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Hiperparámetros Primarios}

\textbf{n\_lags ($p$):} Define el orden del modelo autorregresivo. Controla cuánta memoria temporal incorpora el predictor.
\begin{itemize}
    \item \textit{Rango:} $\{5, 10, 15, 20\}$
\end{itemize}

\textbf{n\_bins ($B$):} Número de particiones Mondrian. Controla la adaptabilidad local versus varianza por tamaño de muestra.
\begin{itemize}
    \item \textit{Rango:} $\{5, 10, 15\}$ según \textcite{Bostrom2021}
    \item \textit{Trade-off:} $B$ pequeño ($\leq 3$) degrada a SCPS; $B$ excesivo ($\geq 20$) fragmenta calibración
    \item \textit{Tamaño esperado por bin:} $\mathbb{E}[N_c^{\kappa}] = N_c / B$
\end{itemize}


\paragraph{Optimización de Hiperparámetros}

\textbf{Estrategia de congelamiento:} El método definido entrena el modelo base XGBoost \textbf{una sola vez} sobre los datos de entrenamiento completos, calculando los artefactos necesarios:

\begin{itemize}
    \item Predicciones de calibración $\{h(x_j)\}_{j \in \mathcal{D}_c}$
    \item Valores observados de calibración $\{y_j\}_{j \in \mathcal{D}_c}$
    \item Bordes de bins $\{q_{\kappa/B}\}_{\kappa=0}^{B}$
\end{itemize}

Estos artefactos se reutilizan en todas las predicciones posteriores sin reentrenamiento, garantizando eficiencia computacional en rolling forecasts.

\textbf{Métrica de optimización:} Los hiperparámetros $(p, B, \text{test\_size})$ se optimizan minimizando el \textbf{Expected Continuous Ranked Probability Score (ECRPS)} sobre un conjunto de validación:

\begin{equation}
(p^*, B^*) = \arg\min_{(p,B,s)} \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{CRPS}(\mathcal{C}_i^{\kappa^*}, y_i)
\end{equation}

Esta optimización ocurre en la fase inicial; una vez congelados, los hiperparámetros permanecen fijos durante todo el horizonte de predicción.

\paragraph{Parámetros del Modelo Base}

El regressor autorregresivo $h: \mathbb{R}^p \rightarrow \mathbb{R}$ se implementa mediante XGBoost \parencite{Chen2016}:

\begin{itemize}
    \item \textbf{n\_estimators:} 50 (número de árboles)
    \item \textbf{max\_depth:} 3 (previene sobreajuste, actúa como GAM)
    \item \textbf{learning\_rate:} 0.1 (convergencia estable)
    \item \textbf{objective:} \texttt{reg:squarederror} (regresión por mínimos cuadrados)
\end{itemize}

Esta parametrización conservadora previene captura de ruido, crítico cuando $p$ es grande relativo a $N_{\text{train}}$.

\paragraph{Balance Fundamental}

La selección conjunta define un trade-off:

\begin{equation}
\text{Calidad de } h(x) \propto (1 - \text{test\_size}) \cdot f(p)
\end{equation}

\begin{equation}
\text{Precisión de CPD} \propto \text{test\_size} \cdot \frac{N_c}{B}
\end{equation}

Para series típicas con $T \approx 1000$, la configuración $(p=10, B=10, \text{test\_size}=0.25)$ resulta en $\sim$25 scores por bin, suficiente para estimación robusta según \textcite{Hyndman1996}.


\subsection{Adaptive Volatility Mondrian Conformal Predictive System (AV-MCPS)}
\label{subsec:AV-MCPS}
\subsubsection{Explicación Teórica del Modelo}

El \textit{Adaptive Volatility Mondrian Conformal Predictive System} (AV-MCPS) constituye una extensión metodológica del MCPS estándar desarrollada específicamente para esta investigación, representando una de las contribuciones originales más significativas de la tesis. Mientras que el MCPS de \textcite{Ye2025} particiona el espacio de calibración únicamente según predicciones puntuales $h(x)$, el AV-MCPS introduce una \textbf{estratificación bidimensional} que incorpora explícitamente la volatilidad local como segunda dimensión de heterogeneidad.

\paragraph{Motivación: Límites del Particionamiento Unidimensional}

El MCPS estándar asume implícitamente homogeneidad de varianza dentro de cada bin de predicción. Formalmente, si $\mathcal{D}_c^{\kappa}$ denota el subconjunto con predicciones en $[q_{\kappa/B}, q_{(\kappa+1)/B})$:

\begin{equation}
\text{Var}(\epsilon_i \mid h(x_i) \in \mathcal{D}_c^{\kappa}) \approx \text{constante} \quad \forall i \in \mathcal{D}_c^{\kappa}
\end{equation}

Esta suposición se viola frecuentemente en series temporales con \textbf{heterocedasticidad condicional}, donde la volatilidad de errores varía sistemáticamente: $\text{Var}(\epsilon_t) = \sigma_t^2 \neq \text{constante}$. Dos observaciones con predicciones similares $h(x_i) \approx h(x_j)$ pueden experimentar errores de magnitudes radicalmente diferentes si provienen de regímenes de volatilidad distintos.

\paragraph{Particionamiento Bidimensional}

El AV-MCPS propone una partición conjunta basada en dos características:

\begin{enumerate}
\item \textbf{Predicción puntual} $h(x_i)$: Captura el nivel esperado de la variable objetivo
\item \textbf{Volatilidad local} $\sigma_i$: Mide la variabilidad reciente mediante ventana rodante de longitud $w$:
\begin{equation}
\sigma_i = \sqrt{\frac{1}{w-1} \sum_{k=i-w}^{i-1} (y_k - \bar{y}_{i,w})^2}
\end{equation}
\end{enumerate}

El conjunto de calibración se particiona en una grilla bidimensional:

\begin{equation}
\mathcal{D}_c = \bigcup_{\kappa=1}^{B_{\text{pred}}} \bigcup_{\lambda=1}^{B_{\text{vol}}} \mathcal{D}_c^{(\kappa,\lambda)}
\end{equation}

donde cada celda agrupa observaciones que satisfacen simultáneamente:

\begin{equation}
\mathcal{D}_c^{(\kappa,\lambda)} = \left\{ (x_i, y_i) \in \mathcal{D}_c : 
\begin{aligned}
&q_{\text{pred}}^{\kappa-1} \leq h(x_i) < q_{\text{pred}}^{\kappa} \\
&\text{y } q_{\text{vol}}^{\lambda-1} \leq \sigma_i < q_{\text{vol}}^{\lambda}
\end{aligned}
\right\}
\end{equation}

Esta estratificación genera $B_{\text{pred}} \times B_{\text{vol}}$ celdas, cada una representando un régimen específico de (nivel, volatilidad).

\paragraph{Cálculo Localizado de Scores}

Para un punto de prueba $x_{\text{test}}$, el procedimiento es:

\begin{enumerate}
\item Calcular predicción $h(x_{\text{test}})$ y volatilidad local $\sigma_{\text{test}}$
\item Determinar la celda correspondiente $(\kappa^*, \lambda^*)$:
\begin{equation}
\kappa^* = \arg\min_{\kappa} \{ \kappa : h(x_{\text{test}}) < q_{\text{pred}}^{\kappa} \}
\end{equation}
\begin{equation}
\lambda^* = \arg\min_{\lambda} \{ \lambda : \sigma_{\text{test}} < q_{\text{vol}}^{\lambda} \}
\end{equation}
\item Calcular scores localizados sobre $\mathcal{D}_c^{(\kappa^*,\lambda^*)}$:
\begin{equation}
C_i^{(\kappa^*,\lambda^*)} = h(x_{\text{test}}) + (y_i - h(x_i)), \quad \forall (x_i, y_i) \in \mathcal{D}_c^{(\kappa^*,\lambda^*)}
\end{equation}
\end{enumerate}

\paragraph{Garantías de Cobertura Bidimensional}

Bajo intercambiabilidad condicional dentro de cada celda, se preservan las garantías conformales localmente:

\begin{equation}
\mathbb{P}\left(Y \in \hat{C}_{\alpha}(x) \,\big|\, x \in \text{Bin}_{\kappa}^{\text{pred}}, \sigma(x) \in \text{Bin}_{\lambda}^{\text{vol}}\right) \geq 1 - \alpha
\end{equation}

Esta garantía es más fuerte que la del MCPS, ya que condiciona sobre una partición más fina. Al controlar simultáneamente por nivel y volatilidad, el AV-MCPS logra \textbf{adaptabilidad condicional mejorada}.

\paragraph{Trade-off: Resolución vs. Tamaño de Muestra}

El número esperado de observaciones por celda es:

\begin{equation}
\mathbb{E}\left[N_c^{(\kappa,\lambda)}\right] = \frac{N_c}{B_{\text{pred}} \times B_{\text{vol}}}
\end{equation}

Aumentar resolución mejora localización pero reduce tamaño de muestra por celda. El AV-MCPS implementa \textbf{fallback jerárquico}:

\begin{equation}
\mathcal{D}_c^{\text{efectivo}} = 
\begin{cases}
\mathcal{D}_c^{(\kappa^*,\lambda^*)} & \text{si } N_c^{(\kappa^*,\lambda^*)} \geq 5 \\
\mathcal{D}_c^{(\kappa^*, \cdot)} & \text{si } N_c^{(\kappa^*,\lambda^*)} < 5 \text{ y } N_c^{(\kappa^*, \cdot)} \geq 5 \\
\mathcal{D}_c & \text{en otro caso}
\end{cases}
\end{equation}

donde $\mathcal{D}_c^{(\kappa^*, \cdot)} = \bigcup_{\lambda=1}^{B_{\text{vol}}} \mathcal{D}_c^{(\kappa^*,\lambda)}$ representa el bin unidimensional basado solo en predicción.

\paragraph{Naturaleza No Paramétrica}

El AV-MCPS es un modelo \textbf{no paramétrico}. No asume forma funcional para la distribución de errores ni para la relación entre predictores y respuesta. La estratificación bidimensional es completamente libre de distribución, construyéndose enteramente a partir de cuantiles empíricos. La distribución predictiva se genera directamente desde scores conformales sin parámetros poblacionales.

\subsubsection{De la Teoría a la Práctica}

La implementación del AV-MCPS para series temporales traduce el marco teórico bidimensional en un sistema operativo mediante decisiones de diseño específicas.

\paragraph{Adaptaciones Principales}

\textbf{(1) Estimación Rolling de Volatilidad:} Se emplea desviación estándar rolling con ventana fija:

\begin{equation}
\hat{\sigma}_t = \sqrt{\frac{1}{w-1} \sum_{k=t-w}^{t-1} (y_k - \bar{y}_t)^2}
\end{equation}

Justificación: simplicidad computacional, robustez a shocks transitorios, e interpretabilidad directa. Para las primeras $w-1$ observaciones se aplica \textit{backfilling}:

\begin{equation}
\hat{\sigma}_t = \sqrt{\frac{1}{t-2} \sum_{k=1}^{t-1} (y_k - \bar{y}_{1:t-1})^2} \quad \text{para } t < w
\end{equation}

\textbf{(2) Binning Adaptativo Robusto:} Las series de volatilidad exhiben distribuciones asimétricas con colas pesadas. Se emplea \texttt{pd.qcut} con \texttt{duplicates='drop'}, fusionando automáticamente bins con fronteras colapsadas:

\begin{equation}
B_{\text{vol}}^{\text{efectivo}} = \left|\left\{q_{\text{vol}}^{\lambda} : \lambda = 1, \ldots, B_{\text{vol}}-1\right\}\right| \leq B_{\text{vol}}
\end{equation}

Si el binning falla en alguna dimensión, el sistema degrada a MCPS unidimensional o SCPS global.

\textbf{(3) Representación Discreta de Scores:} Consistente con LSPM y MCPS, se retornan directamente los scores sin transformación:

\begin{equation}
\mathcal{C}^{(\kappa^*,\lambda^*)} = \left\{C_i^{(\kappa^*,\lambda^*)}\right\}_{i=1}^{N_c^{(\kappa^*,\lambda^*)}}
\end{equation}

Esta representación empírica permite cálculo eficiente de CRPS sin reconstruir CDF continua.

\textbf{(4) Protocolo de Congelamiento Bidimensional:} El método \texttt{freeze\_hyperparameters()} fija simultáneamente:

\begin{itemize}
\item Parámetros del regressor XGBoost (entrenado una sola vez)
\item Bordes de bins de predicción $\{q_{\text{pred}}^{\kappa}\}_{\kappa=0}^{B_{\text{pred}}}$
\item Bordes de bins de volatilidad $\{q_{\text{vol}}^{\lambda}\}_{\lambda=0}^{B_{\text{vol}}}$
\item Artefactos de calibración: $\{h(x_i)\}$, $\{y_i\}$, $\{\sigma_i\}$
\end{itemize}

Durante evaluación rolling, para cada punto $t$ se extraen rezagos, se calcula $h(x_t)$ con modelo congelado, se estima $\sigma_t$ con ventana actual, y se asigna a celda usando bordes congelados. Este protocolo previene data leakage y garantiza validez estadística.

\begin{table}[htbp]
\centering
\caption{Comparación entre MCPS Estándar y AV-MCPS}
\label{tab:avmcps_vs_mcps}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{MCPS} & \textbf{AV-MCPS} \\
\midrule
Dimensiones de partición & Unidimensional (predicción) & Bidimensional (predicción + volatilidad) \\
Número de bins & $B$ & $B_{\text{pred}} \times B_{\text{vol}}$ \\
Tamaño esperado de celda & $N_c / B$ & $N_c / (B_{\text{pred}} \times B_{\text{vol}})$ \\
Captura de heterocedasticidad & Indirecta (via niveles) & Explícita (via volatilidad local) \\
Complejidad computacional & $O(\log B)$ & $O(\log B_{\text{pred}} + \log B_{\text{vol}})$ \\
Estrategia de fallback & Degradar a SCPS si $N_c^{\kappa} < 5$ & Jerárquica: 2D $\to$ 1D $\to$ SCPS \\
Hiperparámetros adicionales & Ninguno & \texttt{volatility\_window}, $B_{\text{vol}}$ \\
Casos de uso óptimos & Heterogeneidad por nivel & Series con volatilidad cambiante \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Innovación Metodológica}

La contribución fundamental es reconocer que \textbf{la volatilidad local predice el error independientemente del nivel}. Si $e_i = |y_i - h(x_i)|$, la hipótesis subyacente es:

\begin{equation}
\mathbb{E}[e_i \mid h(x_i), \sigma_i] \neq \mathbb{E}[e_i \mid h(x_i)]
\end{equation}

El AV-MCPS explota esta estructura mediante estratificación bidimensional, logrando distribuciones predictivas que se adaptan simultáneamente al \textit{nivel} y al \textit{régimen de incertidumbre}. Esta adaptabilidad dual representa una ventaja teórica significativa, especialmente en series con volatilidad time-varying como procesos financieros, climáticos o epidemiológicos.

\subsubsection{Optimización, Parámetros e Hiperparámetros}
\paragraph{Hiperparámetros Primarios}

\textbf{n\_lags ($p$):} Orden del modelo autorregresivo.


\textbf{n\_pred\_bins ($B_{\text{pred}}$):} Resolución en dimensión de predicción.


\textbf{n\_vol\_bins ($B_{\text{vol}}$):} Resolución en dimensión de volatilidad.


\textbf{volatility\_window ($w$):} Longitud de ventana rolling para estimación de volatilidad.

\textbf{test\_size:} Proporción del conjunto de calibración.


\paragraph{Configuraciones Evaluadas}

El espacio de hiperparámetros se limita a dos configuraciones estratégicamente diseñadas:

\textbf{Configuración Conservadora:} $(p=10, B_{\text{pred}}=8, B_{\text{vol}}=3)$
\begin{itemize}
    \item \textit{Filosofía:} Prioriza robustez y tamaño de muestra suficiente por celda
    \item \textit{Número de celdas:} $8 \times 3 = 24$ particiones
    \end{itemize}

\textbf{Configuración Agresiva:} $(p=15, B_{\text{pred}}=10, B_{\text{vol}}=5)$
\begin{itemize}
    \item \textit{Filosofía:} Maximiza adaptabilidad local mediante particionamiento fino
    \item \textit{Número de celdas:} $10 \times 5 = 50$ particiones
\end{itemize}

\paragraph{Justificación del Espacio de Búsqueda Restringido}

A diferencia de MCPS y LSPM, el AV-MCPS no explora grillas exhaustivas. Esta decisión se fundamenta en:

\begin{enumerate}
    \item \textbf{Complejidad combinatoria:} Con 3 hiperparámetros interdependientes, una búsqueda exhaustiva sobre $\{10,15,20\} \times \{6,8,10\} \times \{3,4,5\}$ implicaría 27 evaluaciones por serie. Dado el costo computacional del rolling forecast conformal, esto resulta prohibitivo para el benchmark de 100 series
    
    \item \textbf{Trade-off fundamental:} Existe una tensión inherente entre $B_{\text{pred}} \times B_{\text{vol}}$ (que determina localización) y $N_c$ (fijo por serie). Las dos configuraciones representan los extremos del espectro viable: conservadora (celdas grandes, menor varianza) vs agresiva (celdas pequeñas, mayor adaptabilidad)
    
    \item \textbf{Evidencia empírica preliminar:} Experimentos piloto sobre un subconjunto de 10 series indicaron que configuraciones intermedias (e.g., $B_{\text{pred}}=8, B_{\text{vol}}=4$) rara vez superaban a los extremos, sugiriendo un comportamiento bimodal del desempeño
\end{enumerate}

\paragraph{Optimización de Hiperparámetros}

La selección entre ambas configuraciones se realiza minimizando \textbf{ECRPS} sobre un conjunto de validación temporal. Formalmente:

\begin{equation}
(p^*, B_{\text{pred}}^*, B_{\text{vol}}^*) = \arg\min_{(p, B_p, B_v) \in \mathcal{H}} \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{CRPS}(\mathcal{C}_i^{(\kappa^*,\lambda^*)}, y_i)
\end{equation}

donde $\mathcal{H} = \{(10,8,3), (15,10,5)\}$ es el conjunto de configuraciones candidatas. Una vez optimizada, la configuración seleccionada se congela mediante \texttt{freeze\_hyperparameters()}, fijando simultáneamente el modelo base XGBoost, los bordes de bins bidimensionales, y los artefactos de calibración.

\paragraph{Parámetros del Modelo Base}

Idénticos a MCPS: XGBoost con 50 árboles, profundidad 3, learning rate 0.1, objetivo \texttt{reg:squarederror}. Esta parametrización conservadora es particularmente crítica en AV-MCPS, donde el conjunto de entrenamiento puede ser más pequeño debido a la fragmentación bidimensional del conjunto de calibración.


\subsection{DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks}
\label{subsec:DeepAR}
\subsubsection{Explicación Teórica del Modelo}

DeepAR, introducido por \textcite{Salinas2020}, representa un cambio de paradigma en el pronóstico probabilístico de series temporales al trasladar el enfoque desde el modelado individual de cada serie hacia el \textbf{aprendizaje de un modelo global} a partir de múltiples series relacionadas mediante una arquitectura de red neuronal recurrente autorregresiva.

\paragraph{Fundamento Arquitectónico}

El modelo emplea redes LSTM (Long Short-Term Memory) para procesar secuencias temporales de forma autorregresiva. Para una serie temporal $i$ con valores $z_{i,t}$, DeepAR modela la distribución condicional del futuro dado el pasado:

\begin{equation}
P(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T})
\end{equation}

donde $t_0$ denota el punto de inicio del horizonte de predicción, $z_{i,1:t_0-1}$ representa el rango de condicionamiento, $z_{i,t_0:T}$ los valores futuros, y $x_{i,1:T}$ son covariables conocidas.

La arquitectura factoriza esta distribución mediante el producto de verosimilitudes condicionales:

\begin{equation}
Q_{\Theta}(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T}) = \prod_{t=t_0}^{T} Q_{\Theta}(z_{i,t} \mid z_{i,1:t-1}, x_{i,1:T})
\end{equation}

donde cada factor está parametrizado por la salida de la red recurrente:

\begin{equation}
Q_{\Theta}(z_{i,t} \mid z_{i,1:t-1}, x_{i,1:T}) = \ell(z_{i,t} \mid \theta(h_{i,t}, \Theta))
\end{equation}

El estado oculto $h_{i,t}$ se actualiza recursivamente mediante:

\begin{equation}
h_{i,t} = h(h_{i,t-1}, z_{i,t-1}, x_{i,t}, \Theta)
\end{equation}

donde $h(\cdot)$ es una función implementada por una red LSTM multicapa con parámetros $\Theta$.

\paragraph{Naturaleza Autorregresiva}

En cada paso temporal $t$, la red consume como entrada el valor observado del paso anterior $z_{i,t-1}$ junto con las covariables $x_{i,t}$ y el estado oculto previo $h_{i,t-1}$. Durante el entrenamiento, todos los valores $z_{i,t}$ en el rango de predicción son conocidos. Durante la predicción, para $t \geq t_0$, los valores futuros se reemplazan por muestras $\tilde{z}_{i,t} \sim \ell(\cdot \mid \theta(h_{i,t}, \Theta))$ generadas por el propio modelo, que se retroalimentan para calcular el siguiente estado oculto mediante \textit{muestreo ancestral}.

\paragraph{Modelado Probabilístico}

DeepAR no predice directamente valores futuros, sino los \textbf{parámetros de una distribución de probabilidad} $\theta(h_{i,t})$ sobre valores posibles. Para datos de valores reales, se emplea \textbf{verosimilitud Gaussiana}:

\begin{equation}
\ell_G(z \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)
\end{equation}

donde la media $\mu(h_{i,t})$ y desviación estándar $\sigma(h_{i,t})$ se obtienen mediante transformaciones de la salida de la red:

\begin{equation}
\mu(h_{i,t}) = w_{\mu}^T h_{i,t} + b_{\mu}, \quad \sigma(h_{i,t}) = \log(1 + \exp(w_{\sigma}^T h_{i,t} + b_{\sigma}))
\end{equation}

Los parámetros $\Theta$ del modelo se aprenden maximizando la log-verosimilitud:

\begin{equation}
\mathcal{L}(\Theta) = \sum_{i=1}^{N} \sum_{t=1}^{T} \log \ell(z_{i,t} \mid \theta(h_{i,t}, \Theta))
\end{equation}

Una ventaja fundamental es que el modelo es completamente observable durante el entrenamiento, no requiriendo técnicas de inferencia variacional o métodos de Monte Carlo para aproximar la función objetivo.

\paragraph{Manejo de Escalas Heterogéneas}

DeepAR introduce un mecanismo de escalamiento que normaliza las entradas y salidas autorregresivas por un factor de escala específico de cada serie $\nu_i$:

\begin{equation}
\tilde{z}_{i,t} = \frac{z_{i,t}}{\nu_i}, \quad \nu_i = 1 + \frac{1}{t_0} \sum_{t=1}^{t_0} z_{i,t}
\end{equation}

Los parámetros de la verosimilitud se ajustan correspondientemente:

\begin{equation}
\mu_{\text{escalado}} = \nu_i \cdot \mu(h_{i,t}), \quad \sigma_{\text{escalado}} = \nu_i \cdot \sigma(h_{i,t})
\end{equation}

\paragraph{Generación de Pronósticos Probabilísticos}

La generación de pronósticos se realiza mediante muestreo ancestral, produciendo $B$ trayectorias completas $\{\tilde{z}_{i,t_0:T}^{(b)}\}_{b=1}^{B}$. El conjunto de trayectorias representa una muestra empírica de la distribución predictiva conjunta, preservando las correlaciones temporales aprendidas por el modelo.

\paragraph{Clasificación del Modelo}

DeepAR es un \textbf{modelo semi-paramétrico}. La componente paramétrica reside en la arquitectura LSTM con parámetros $\Theta$ que deben estimarse, y en la elección de la familia de distribuciones de verosimilitud (Gaussiana, Binomial Negativa). La componente no paramétrica emerge del muestreo ancestral, que genera distribuciones predictivas empíricas sin asumir formas funcionales rígidas para la distribución conjunta multi-paso.

\subsubsection{De la Teoría a la Práctica}

La implementación de DeepAR para series temporales univariadas desarrollada en esta investigación traduce el marco teórico autorregresivo a un sistema predictivo concreto mediante adaptaciones específicas al contexto de este estudio.

\paragraph{Adaptaciones Principales}

\textbf{(1) Simplificación de Covariables:} A diferencia del trabajo original de \textcite{Salinas2020} que asume múltiples covariables $x_{i,t}$, esta implementación \textbf{no utiliza covariables externas}. La arquitectura se reduce a su forma puramente autorregresiva:

\begin{equation}
h_{i,t} = h(h_{i,t-1}, z_{i,t-1}, \Theta)
\end{equation}

Esta simplificación elimina la dependencia de información auxiliar, centrando la comparación en la capacidad de extraer patrones de la historia temporal intrínseca.

\textbf{(2) Construcción de Ventanas:} Dado que cada configuración genera una única serie de longitud $n=252$, se emplea \textit{windowing} deslizante para generar múltiples instancias de entrenamiento. Se construyen pares $(X^{(k)}, y^{(k)})$ mediante:

\begin{equation}
X^{(k)} = [y_k, \ldots, y_{k+p-1}], \quad y^{(k)} = y_{k+p}
\end{equation}

para $k = 1, \ldots, n_{\text{train}} - p$, generando aproximadamente $n_{\text{train}} - p$ instancias de entrenamiento.

\textbf{(3) Normalización Z-Score:} En lugar del escalamiento por $\nu_i$ del original, se emplea normalización Z-score completa:

\begin{equation}
\tilde{z}_t = \frac{z_t - \mu_{\text{train}}}{\sigma_{\text{train}} + \epsilon}
\end{equation}

garantizando que las entradas a la LSTM tengan media cero y varianza unitaria. Las predicciones se des-normalizan mediante la transformación inversa.

\textbf{(4) Early Stopping:} Se reserva el 20\% final de las instancias generadas como conjunto de validación interno. El entrenamiento se detiene si la pérdida de validación no mejora durante un número de épocas de paciencia (típicamente 5), reteniendo los parámetros correspondientes a la época con menor pérdida.

\textbf{(5) Protocolo de Congelamiento:} Para evitar \textit{data leakage} en la evaluación rolling window, el modelo se entrena una única vez sobre $n_{\text{train}} = 200$ observaciones. Los parámetros de normalización $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$ y los pesos de la red $\Theta_{\text{frozen}}$ se congelan completamente. Durante la fase de evaluación rolling, para cada paso $t = 1, \ldots, 12$, se normalizan los últimos $p$ valores observados usando parámetros congelados, se calcula la predicción con $\Theta_{\text{frozen}}$ sin re-entrenamiento, y se des-normalizan las muestras predictivas. Este protocolo previene contaminación de información futura y garantiza validez estadística.

La Tabla \ref{tab:deepar_comparison} resume las diferencias metodológicas entre la implementación original y la adaptación desarrollada.

\begin{table}[htbp]
\centering
\caption{Comparación entre DeepAR Original y DeepAR Adaptado}
\label{tab:deepar_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{DeepAR Original} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Contexto de aplicación & Miles de series relacionadas & Serie temporal única (windowing local) \\
Covariables & Múltiples features temporales & Sin covariables externas \\
Verosimilitud & Gaussiana y Binomial Negativa & Gaussiana únicamente \\
Normalización & Escalamiento por $\nu_i$ & Z-score completo \\
Muestreo de entrenamiento & Ponderado por velocidad & Uniforme sobre ventanas \\
Regularización & No especificada & Early stopping con validación interna \\
Protocolo de evaluación & Modelo único para todas las series & Congelamiento total para rolling forecast \\
Framework & MXNet & PyTorch \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Hiperparámetros Arquitectónicos}

\textbf{hidden\_size ($h$):} Dimensionalidad del estado oculto de cada celda LSTM. Controla la capacidad representacional del modelo.

\textbf{n\_lags ($p$):} Número de valores pasados utilizados como entrada autorregresiva. Similar al orden en modelos AR$(p)$.

\textbf{num\_layers ($L$):} Número de capas LSTM apiladas. Valores mayores incrementan el número de parámetros: $\Theta \propto L \times h^2$.

\textbf{epochs ($E$):} Número máximo de pasadas completas sobre el conjunto de entrenamiento. El early stopping típicamente detiene antes de alcanzar este máximo.

\textbf{lr (learning rate):} Controla el tamaño del paso en la actualización de parámetros mediante el optimizador Adam \parencite{Kingma2015}.

\paragraph{Espacio de Búsqueda}

La optimización de hiperparámetros explora dos configuraciones estratégicamente diseñadas:

\textbf{Configuración Ligera:}
\begin{itemize}
    \item \texttt{hidden\_size}=20, \texttt{n\_lags}=10, \texttt{num\_layers}=1, \texttt{epochs}=25, \texttt{lr}=0.01
    \item \textit{Filosofía:} Arquitectura compacta que reduce riesgo de sobreajuste
    \item \textit{Parámetros totales:} Aproximadamente 2,200 parámetros
\end{itemize}

\textbf{Configuración Profunda:}
\begin{itemize}
    \item \texttt{hidden\_size}=32, \texttt{n\_lags}=15, \texttt{num\_layers}=2, \texttt{epochs}=30, \texttt{lr}=0.005
    \item \textit{Filosofía:} Mayor capacidad representacional con regularización implícita vía learning rate reducido
    \item \textit{Parámetros totales:} Aproximadamente 8,500 parámetros
\end{itemize}

\paragraph{Protocolo de Optimización}

La selección entre ambas configuraciones se realiza minimizando \textbf{ECRPS (Expected Continuous Ranked Probability Score)} sobre un conjunto de calibración temporal de 40 observaciones posteriores al entrenamiento. Formalmente:

\begin{equation}
(h^*, p^*, L^*) = \arg\min_{(h, p, L) \in \mathcal{H}} \frac{1}{N_{\text{cal}}} \sum_{i=1}^{N_{\text{cal}}} \text{CRPS}(\hat{F}_i, y_i)
\end{equation}

donde $\mathcal{H}$ es el conjunto de configuraciones candidatas y $\hat{F}_i$ es la distribución predictiva empírica generada por muestreo ancestral.

\paragraph{Congelamiento de Hiperparámetros}

Una vez optimizada, la configuración seleccionada se congela mediante el método \texttt{freeze\_hyperparameters()}. Este método:

\begin{enumerate}
    \item Estima los parámetros de normalización: $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$
    \item Entrena la red LSTM hasta convergencia (early stopping)
    \item Almacena los pesos de la red: $\Theta_{\text{frozen}}$
    \item Establece el flag \texttt{\_is\_frozen = True}
\end{enumerate}

Durante toda la evaluación rolling window posterior, el método \texttt{fit\_predict} verifica este flag: si es \texttt{True}, omite completamente el entrenamiento y procede directamente a generar predicciones con el modelo existente. Este protocolo garantiza que no haya fuga de información futura y que las métricas de desempeño reflejen la capacidad predictiva genuina del modelo.


\subsection{Autoregressive Exponentially-weighted Polynomial Distribution (AREPD)}
\label{subsec:AREPD}
\subsubsection{Explicación Teórica del Modelo}

El modelo \textit{Autoregressive Exponentially-weighted Polynomial Distribution} (AREPD) representa una contribución metodológica original de esta investigación, desarrollada como una extensión híbrida que combina predicción ponderada temporalmente con expansión polinomial de características autorregresivas. A diferencia de métodos puramente conformales como LSPM que utilizan regresión lineal, AREPD introduce \textbf{transformaciones no lineales polinomiales} de las entradas autorregresivas.

\paragraph{Fundamento Matemático}

Para una serie temporal $\{Y_t\}_{t=1}^{n}$ con $p$ rezagos y grado polinomial $d$, AREPD construye una matriz de diseño expandida $\mathbf{X} \in \mathbb{R}^{(n-p) \times (1+pd)}$:

\begin{equation}
\mathbf{X}_i = \left[1, Y_{i}, Y_{i}^2, \ldots, Y_{i}^d, Y_{i+1}, Y_{i+1}^2, \ldots, Y_{i+1}^d, \ldots, Y_{i+p-1}^d\right]
\end{equation}

para $i = 1, \ldots, n-p$. Esta expansión permite capturar relaciones cuadráticas, cúbicas o de orden superior entre valores pasados y futuros sin recurrir a arquitecturas de aprendizaje profundo.

\paragraph{Ponderación Exponencial Temporal}

AREPD asigna pesos exponencialmente decrecientes a observaciones históricas:

\begin{equation}
w_i = \rho^{n-p-i}, \quad i = 1, \ldots, n-p
\end{equation}

donde $\rho \in (0, 1)$ es el parámetro de decaimiento. Los pesos se normalizan: $\tilde{w}_i = w_i / \sum_{j=1}^{n-p} w_j$. La observación más reciente recibe peso máximo, mientras que observaciones antiguas reciben pesos progresivamente menores. La vida media efectiva es:

\begin{equation}
\tau_{1/2} = \frac{\log(2)}{\log(1/\rho)}
\end{equation}

Por ejemplo, con $\rho = 0.95$, $\tau_{1/2} \approx 13.5$ observaciones.

\paragraph{Estimación mediante Regresión Ridge}

Los coeficientes se estiman resolviendo el problema de regresión Ridge ponderada:

\begin{equation}
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \left\{\sum_{i=1}^{n-p} \tilde{w}_i \left(y_i - \mathbf{X}_i^T \boldsymbol{\beta}\right)^2 + \lambda \|\boldsymbol{\beta}\|_2^2\right\}
\end{equation}

donde $\lambda > 0$ es el parámetro de regularización Ridge. La penalización Ridge es crítica por dos razones:

\begin{enumerate}
\item \textbf{Estabilidad numérica:} La expansión polinomial genera matrices casi singulares debido a alta correlación entre términos como $Y_{t-1}$ y $Y_{t-1}^2$
\item \textbf{Prevención de sobreajuste:} Con $p \cdot d$ características, el modelo tiene alta capacidad expresiva que requiere regularización
\end{enumerate}

La solución tiene forma cerrada:

\begin{equation}
\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^T\mathbf{W}\mathbf{X} + \lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{W}\mathbf{y}
\end{equation}

donde $\mathbf{W} = \text{diag}(\tilde{w}_1, \ldots, \tilde{w}_{n-p})$ es la matriz diagonal de pesos.

\paragraph{Generación de Distribuciones Predictivas}

AREPD genera distribuciones predictivas mediante un enfoque \textbf{histórico-empírico}. Para predecir $Y_{n+1}$:

\begin{enumerate}
\item Construye vectores de características expandidos para todos los puntos históricos
\item Calcula predicciones puntuales históricas: $\hat{Y}_{\text{hist}} = \mathbf{X}_{\text{hist}} \hat{\boldsymbol{\beta}}$
\item Transforma a escala original: $\hat{Y}_{\text{hist}}^{\text{original}} = \hat{Y}_{\text{hist}} \cdot \sigma_{\text{train}} + \mu_{\text{train}}$
\item Utiliza $\{\hat{Y}_{\text{hist}}^{\text{original}}\}$ como muestra empírica de la distribución predictiva
\end{enumerate}

Este enfoque difiere fundamentalmente de la predicción conformal estándar. En lugar de ajustar mediante residuos de calibración, AREPD construye una distribución empírica directamente desde predicciones históricas del modelo ajustado, asumiendo que bajo estacionariedad local, estas predicciones son representativas del futuro.

\paragraph{Garantías de Cobertura}

A diferencia de métodos conformales con garantías formales, AREPD \textbf{no posee garantías teóricas de cobertura bajo intercambiabilidad}. La distribución predictiva se construye asumiendo:

\begin{equation}
\hat{Y}_t \mid \mathcal{F}_{t-1} \overset{d}{\approx} \hat{Y}_s \mid \mathcal{F}_{s-1} \quad \forall t, s \in \{p+1, \ldots, n\}
\end{equation}

Esta suposición puede violarse en presencia de no estacionariedad fuerte, heterocedasticidad condicional, o eventos raros no observados durante el entrenamiento.

\paragraph{Clasificación del Modelo}

AREPD es un \textbf{modelo semi-paramétrico}. La componente paramétrica reside en la estructura de regresión Ridge con coeficientes $\hat{\boldsymbol{\beta}}$ que deben estimarse. La componente no paramétrica emerge del uso de la distribución empírica de predicciones históricas sin asumir formas funcionales rígidas para la distribución predictiva.

\begin{table}[htbp]
\centering
\caption{Comparación Conceptual de AREPD con Métodos Relacionados}
\label{tab:arepd_conceptual}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{4cm}}
\toprule
\textbf{Método} & \textbf{Espacio de características} & \textbf{Ponderación temporal} & \textbf{Distribución predictiva} \\
\midrule
LSPM & Lineal (rezagos) & Uniforme & Conformal (scores ajustados) \\
LSPMW & Lineal (rezagos) & Exponencial ($\rho$) & Conformal ponderada \\
AREPD & Polinomial (grado $d$) & Exponencial ($\rho$) & Histórico-empírica \\
DeepAR & No lineal (LSTM) & Uniforme & Muestreo ancestral \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{De la Teoría a la Práctica}

La implementación de AREPD traduce el marco teórico en un sistema predictivo concreto mediante adaptaciones específicas para garantizar estabilidad numérica y validez estadística.

\paragraph{Adaptaciones Principales}

\textbf{(1) Normalización Z-Score Pre-Expansión:} La expansión polinomial es extremadamente sensible a la escala. Si una serie tiene valores en $[100, 200]$, los términos cuadráticos estarán en $[10^4, 4 \times 10^4]$ y los cúbicos en $[10^6, 8 \times 10^6]$, causando inestabilidad numérica. Se aplica normalización Z-score \textbf{antes} de la expansión:

\begin{equation}
\tilde{Y}_t = \frac{Y_t - \mu_{\text{train}}}{\sigma_{\text{train}} + \epsilon}
\end{equation}

donde $\epsilon = 10^{-8}$ previene división por cero. Esto garantiza que todos los términos estén aproximadamente en $[-3, 3]$ bajo normalidad. Las predicciones se des-normalizan:

\begin{equation}
Y_t^{\text{pred}} = \tilde{Y}_t^{\text{pred}} \cdot \sigma_{\text{train}} + \mu_{\text{train}}
\end{equation}

\textbf{(2) Uso de regresión Ridge:} En lugar de implementar manualmente la solución, se utiliza \texttt{Ridge} de scikit-learn con \texttt{sample\_weight}, proporcionando estabilidad numérica mediante SVD optimizado y manejo robusto de casos extremos. El argumento \texttt{fit\_intercept=False} es crucial porque el término constante ya está incluido como primera columna de $\mathbf{X}$.

\textbf{(3) Protocolo de Congelamiento:} Siguiendo la filosofía unificada de todos los modelos, AREPD congela completamente:

\begin{itemize}
\item Parámetros de normalización: $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$
\item Coeficientes del modelo: $\hat{\boldsymbol{\beta}}_{\text{frozen}}$
\item Distribución predictiva base (predicciones históricas)
\end{itemize}

Durante evaluación rolling, se normalizan los últimos $p$ valores usando parámetros congelados, se expande polinomialmente, se aplica $\hat{\boldsymbol{\beta}}_{\text{frozen}}$ sin re-estimación, y se retorna la distribución histórica des-normalizada. Este protocolo garantiza ausencia de data leakage.

\textbf{(4) Manejo de Casos Degenerados:} Si $n < 2p$, el modelo degrada a predictor constante $\hat{Y}_{n+1} = \mu_{\text{frozen}}$. Si la matriz es numéricamente singular (número de condición $> 10^{12}$) después de regularización Ridge, se retorna el mismo fallback.

\begin{table}[htbp]
\centering
\caption{Comparación entre AREPD Teórico y AREPD Implementado}
\label{tab:arepd_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{AREPD Teórico} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Normalización de datos & No especificada & Z-score completo pre-expansión \\
Implementación Ridge & Solución matricial directa & scikit-learn Ridge con SVD \\
Manejo de singularidad & Asume matriz bien condicionada & Fallback a predictor constante \\
Protocolo de evaluación & Re-estimación en cada paso & Congelamiento total para rolling forecast \\
Selección de $\lambda$ & Validación cruzada sugerida & Valor fijo ($\lambda=0.1$) \\
Casos degenerados & No considerados & Predictor constante como fallback \\
Garantías formales & Ninguna (método heurístico) & Sin garantías teóricas \\
Framework & Matemático puro & NumPy + scikit-learn \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Hiperparámetros Estructurales}

\textbf{n\_lags ($p$):} Número de rezagos incluidos en la matriz de diseño. Con grado $d$, la dimensión del espacio de características es $1 + p \cdot d$. Valores mayores permiten capturar dependencias de largo plazo pero reducen el tamaño del conjunto de entrenamiento e incrementan riesgo de sobreajuste.

\textbf{poly\_degree ($d$):} Grado máximo de la expansión polinomial.
\begin{itemize}
\item $d=1$: Modelo puramente lineal (regresión autorregresiva Ridge estándar)
\item $d=2$: Incluye términos cuadráticos, captura efectos de amplificación moderados
\item $d=3$: Incluye términos cúbicos, alta expresividad pero riesgo significativo de sobreajuste
\end{itemize}

Número de parámetros: $1 + p \cdot d$. Para $(p,d)=(5,2)$: 11 parámetros; para $(p,d)=(10,3)$: 31 parámetros.

\textbf{rho ($\rho$):} Parámetro de decaimiento exponencial. Vida media efectiva:
\begin{itemize}
\item $\rho = 0.90$: $\tau_{1/2} \approx 6.6$ observaciones (memoria corta, adaptación rápida)
\item $\rho = 0.95$: $\tau_{1/2} \approx 13.5$ observaciones (balance intermedio)
\item $\rho = 0.98$: $\tau_{1/2} \approx 34.3$ observaciones (memoria larga, estabilidad)
\end{itemize}

\paragraph{Parámetro Fijo}

\textbf{alpha ($\lambda$):} Parámetro de regularización Ridge, fijo en $\lambda=0.1$. Controla el trade-off sesgo-varianza: valores cercanos a cero aproximan mínimos cuadrados no regularizados, valores grandes contraen coeficientes hacia cero. El valor $\lambda=0.1$ proporciona regularización suave sin sesgo excesivo.

\paragraph{Espacio de Búsqueda}

La optimización explora tres configuraciones estratégicamente diseñadas:

\textbf{Configuración 1 - Estándar:}
\begin{itemize}
\item \texttt{n\_lags}=5, \texttt{rho}=0.95, \texttt{poly\_degree}=2
\item \textit{Filosofía:} Balance entre complejidad y robustez
\item \textit{Parámetros:} 11 (1 constante + 5 lineales + 5 cuadráticos)
\end{itemize}

\textbf{Configuración 2 - Memoria Corta:}
\begin{itemize}
\item \texttt{n\_lags}=10, \texttt{rho}=0.90, \texttt{poly\_degree}=2
\item \textit{Filosofía:} Mayor orden autorregresivo con adaptación rápida
\item \textit{Parámetros:} 21 (1 constante + 10 lineales + 10 cuadráticos)
\end{itemize}

\textbf{Configuración 3 - Alta No Linealidad:}
\begin{itemize}
\item \texttt{n\_lags}=5, \texttt{rho}=0.98, \texttt{poly\_degree}=3
\item \textit{Filosofía:} Máxima expresividad no lineal con memoria larga
\item \textit{Parámetros:} 16 (1 constante + 5 lineales + 5 cuadráticos + 5 cúbicos)
\end{itemize}

\paragraph{Protocolo de Optimización}

La selección entre configuraciones minimiza \textbf{ECRPS (Expected Continuous Ranked Probability Score)} sobre el conjunto de calibración de 40 observaciones:

\begin{equation}
(p^*, \rho^*, d^*) = \arg\min_{(p, \rho, d) \in \mathcal{H}} \frac{1}{N_{\text{cal}}} \sum_{i=1}^{N_{\text{cal}}} \text{CRPS}(\hat{F}_i, y_i)
\end{equation}

donde $\mathcal{H} = \{(5,0.95,2), (10,0.90,2), (5,0.98,3)\}$ es el conjunto de configuraciones candidatas y $\hat{F}_i$ es la distribución predictiva empírica generada por el método histórico.

\paragraph{Congelamiento de Hiperparámetros}

Una vez optimizada, la configuración se congela mediante \texttt{freeze\_hyperparameters()}:

\begin{enumerate}
\item Estima $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$ sobre datos de entrenamiento
\item Entrena el modelo Ridge ponderado hasta convergencia
\item Almacena coeficientes: $\hat{\boldsymbol{\beta}}_{\text{frozen}}$
\item Establece flag \texttt{\_is\_frozen = True}
\end{enumerate}

Durante toda la evaluación rolling, \texttt{fit\_predict} verifica este flag: si es \texttt{True}, omite entrenamiento y procede directamente a predecir con el modelo existente, garantizando validez estadística sin fuga de información futura.


\subsection{Ensemble Conformalized Quantile Regression (EnCQR-LSTM)}
\label{subsec:EnCQR-LSTM}
\subsubsection{Explicación Teórica del Modelo}

El \textit{Ensemble Conformalized Quantile Regression} (EnCQR-LSTM) constituye una síntesis metodológica avanzada que integra tres paradigmas complementarios del aprendizaje estadístico: regresión cuantílica (QR), predicción conformal (CP) y aprendizaje en ensamble. Esta arquitectura híbrida busca heredar simultáneamente la \textbf{adaptabilidad heterocedástica} de QR y las \textbf{garantías formales de cobertura} de CP, superando las limitaciones inherentes de cada enfoque por separado.

\paragraph{Motivación y Limitaciones de Métodos Puros}

Los métodos basados únicamente en regresión cuantílica pueden generar intervalos adaptativos cuya amplitud varía localmente con la volatilidad de los datos, pero carecen de garantías formales de cobertura. En la práctica, los intervalos de predicción generados por QR tienden a ser excesivamente confiados (demasiado estrechos), resultando en coberturas empíricas significativamente inferiores al nivel nominal $(1-\alpha)$. \parencite{Jensen2022}

Por otra parte, los métodos de predicción conformal estándar garantizan cobertura marginal válida bajo intercambiabilidad, pero construyen intervalos de amplitud constante o levemente variable. Para series temporales con heterocedasticidad, donde la incertidumbre fluctúa considerablemente, estos intervalos resultan excesivamente conservadores en períodos de baja volatilidad e insuficientes en períodos de alta volatilidad.

EnCQR-LSTM aborda ambas limitaciones mediante una arquitectura de ensamble que combina estimadores LSTM de regresión cuantílica con un mecanismo de conformalización posterior.

\paragraph{Arquitectura de Ensamble y Regresión Cuantílica}

El método presentado por \cite{Jensen2022} construye un ensamble homogéneo de $B$ modelos LSTM, cada uno entrenado sobre subconjuntos \textbf{disjuntos} del conjunto de entrenamiento $\{(x_i, y_i)\}_{i=1}^{T}$. La partición se define como:

\begin{equation}
S_b = \{(x_i, y_i) : i \in [(b-1)T_b + 1, bT_b]\}, \quad b = 1, \ldots, B
\end{equation}

donde $T_b = \lfloor T/B \rfloor$. Esta fragmentación disjunta es fundamental para construir residuos fuera de muestra válidos, garantizando que cada observación está excluida de al menos un modelo del ensamble.

Cada modelo LSTM estima simultáneamente múltiples funciones cuantílicas condicionales mediante la minimización de la pérdida pinball agregada:

\begin{equation}
\mathcal{L}_{\text{pinball}}(\theta_b) = \frac{1}{|S_b| \cdot |\mathcal{T}|} \sum_{(x_i, y_i) \in S_b} \sum_{\tau \in \mathcal{T}} \rho_{\tau}(y_i - \hat{q}_{\tau}^{(b)}(x_i))
\end{equation}

donde $\mathcal{T} = \{0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99\}$ es el conjunto de cuantiles objetivo, y $\rho_{\tau}(u) = \max(\tau \cdot u, (\tau - 1) \cdot u)$ es la función de pérdida pinball que penaliza asimétricamente los errores según el cuantil objetivo.

\paragraph{Predicción Leave-One-Out y Scores de Conformidad}

Una vez entrenados los $B$ modelos, EnCQR construye predicciones leave-one-out (LOO) para cada observación de entrenamiento. Para una observación $i$ en el subconjunto $S_b$, se agregan las predicciones de todos los modelos que no incluyeron esa observación:

\begin{equation}
\hat{q}_{\tau}^{(-i)}(x_i) = \frac{1}{B-1}\sum_{b': i \notin S_{b'}} \hat{q}_{\tau}^{(b')}(x_i)
\end{equation}

Este procedimiento LOO reemplaza el requisito de intercambiabilidad por un esquema de validación cruzada que genera residuos genuinamente fuera de muestra, esencial para series temporales.

EnCQR introduce scores de conformidad \textbf{asimétricos} que cuantifican separadamente el error de cobertura en las colas inferior y superior:

\begin{equation}
\begin{aligned}
E_i^{\text{lo}} &= \hat{q}_{\tau_{\text{lo}}}^{(-i)}(x_i) - y_i \\
E_i^{\text{hi}} &= y_i - \hat{q}_{\tau_{\text{hi}}}^{(-i)}(x_i)
\end{aligned}
\end{equation}

Esta formulación asimétrica permite que las distribuciones de errores para los cuantiles inferior y superior tengan formas diferentes, crucial en presencia de asimetría sistemática.

\paragraph{Conformalización y Distribución Predictiva}

Para una nueva observación $x_{T+1}$, el ensamble completo genera predicciones agregadas que se conformalizan mediante:

\begin{equation}
\hat{C}_{\alpha}(x_{T+1}) = \left[\hat{q}_{\tau_{\text{lo}}}(x_{T+1}) - \omega^{\text{lo}}, \hat{q}_{\tau_{\text{hi}}}(x_{T+1}) + \omega^{\text{hi}}\right]
\end{equation}

donde $\omega^{\text{lo}} = Q_{1-\alpha}(\{E_i^{\text{lo}}\}_{i=1}^{T})$ y $\omega^{\text{hi}} = Q_{1-\alpha}(\{E_i^{\text{hi}}\}_{i=1}^{T})$ son los cuantiles $(1-\alpha)$ empíricos de las distribuciones de scores.

Una innovación clave de la implementación es el ajuste de una distribución \textbf{Skew-Normal} paramétrica a los cuantiles conformalizados. La función de densidad es:

\begin{equation}
f(x; \mu, \sigma, \alpha) = \frac{2}{\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) \Phi\left(\alpha \frac{x - \mu}{\sigma}\right)
\end{equation}

donde $\phi(\cdot)$ y $\Phi(\cdot)$ son la densidad y distribución acumulada Normal estándar. Los parámetros $(\mu, \sigma, \alpha)$ se estiman minimizando:

\begin{equation}
(\hat{\mu}, \hat{\sigma}, \hat{\alpha}) = \arg\min_{(\mu, \sigma, \alpha)} \sum_{i=1}^{|\mathcal{T}|} \left(\hat{q}_{\tau_i}^{\text{conf}} - F^{-1}_{\text{SN}}(\tau_i; \mu, \sigma, \alpha)\right)^2
\end{equation}

Este ajuste garantiza unimodalidad y permite generar $M = 1000$ muestras de la distribución predictiva completa.

\paragraph{Naturaleza del Modelo}

EnCQR-LSTM es un modelo \textbf{semi-paramétrico}. La arquitectura LSTM y la función de pérdida pinball son completamente no paramétricas respecto a la distribución de $Y|X$. Sin embargo, la fase final de ajuste Skew-Normal introduce una componente paramétrica para suavizar la distribución predictiva. No obstante, las garantías de cobertura provienen del mecanismo conformal no paramétrico, no del ajuste distribucional.

\paragraph{Propiedades Teóricas}

Bajo el supuesto de que el proceso de error es estacionario y fuertemente mezclante, EnCQR garantiza \parencite{Jensen2022}:

\begin{enumerate}
\item \textbf{Cobertura marginal válida:} $\mathbb{P}\{Y_{T+1} \in \hat{C}_{\alpha}(X_{T+1})\} \geq 1 - \alpha + O(1/T)$
\item \textbf{Adaptabilidad heterocedástica:} La amplitud varía localmente con $x_{T+1}$ a través de $\hat{q}_{\tau}(x_{T+1})$
\item \textbf{Robustez ante especificación incorrecta:} Incluso con modelo LSTM mal especificado, la conformalización mantiene cobertura válida
\end{enumerate}

\subsubsection{De la Teoría a la Práctica}

La implementación de EnCQR-LSTM para esta investigación introduce adaptaciones específicas respecto al marco teórico original, priorizando factibilidad computacional y robustez empírica.

\paragraph{Adaptaciones Principales}

\textbf{(1) Tamaño de Ensamble Reducido:} Se utiliza $B=3$ modelos en lugar de $B \geq 5$ como sugiere la literatura. Esta reducción responde al trade-off entre diversidad del ensamble y tamaño de subconjuntos de entrenamiento. Para series de longitud $T \approx 200-500$, $B=3$ genera subconjuntos de $65-165$ observaciones, suficientes para entrenar LSTMs efectivos sin fragmentación excesiva.

\textbf{(2) Conjunto de Cuantiles Reducido:} En lugar de estimar 19+ cuantiles, se limita a $\mathcal{T} = \{0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99\}$ (9 cuantiles). Esta reducción disminuye la complejidad de la capa de salida del LSTM y acelera el entrenamiento, manteniendo resolución suficiente para caracterizar la distribución predictiva.

\textbf{(3) Ajuste Skew-Normal como Suavizado:} El paper original no especifica cómo generar distribuciones continuas desde cuantiles discretos. La implementación introduce el ajuste Skew-Normal como mecanismo de suavizado, evitando interpolación lineal que puede generar distribuciones bimodales artificiales. Si la optimización falla, se emplea fallback a distribución Normal estándar.

\textbf{(4) Actualización de Scores Deshabilitada:} El mecanismo de ventana deslizante para actualizar scores conformales ($s=24$) no se implementa en la versión evaluada. Todos los scores se calculan una sola vez durante la fase de calibración y permanecen congelados. Esta simplificación prioriza reproducibilidad y reduce complejidad computacional.

\textbf{(5) Arquitectura LSTM Simplificada:} Se emplea $L=2$ capas LSTM con 32 unidades cada una, sin mecanismos avanzados como attention o skip connections. La regularización se limita a dropout ($p=0.1$) y penalización L2 implícita en Adam.

\begin{table}[htbp]
\centering
\caption{Comparación entre teoría y práctica en EnCQR-LSTM}
\label{tab:encqr_theory_vs_practice}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{4cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{Teoría Original} & \textbf{Implementación} \\
\midrule
Tamaño de ensamble ($B$) & $B \geq 5$ recomendado & $B = 3$ fijo \\
Cuantiles estimados & 19+ cuantiles para alta resolución & 9 cuantiles estratégicos \\
Distribución predictiva & No especificada & Ajuste Skew-Normal paramétrico \\
Actualización de scores & Ventana deslizante cada $s$ observaciones & Deshabilitada (scores congelados) \\
Arquitectura LSTM & No especificada & 2 capas, 32 unidades, dropout 0.1 \\
Protocolo de congelamiento & Scores adaptativos & Congelamiento total (modelos + scores) \\
Complejidad computacional & Alta (múltiples entrenamientos + updates) & Reducida (sin actualizaciones) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimización, Parámetros e Hiperparámetros}

\paragraph{Hiperparámetros del Modelo}

\textbf{n\_lags ($N_x$):} Longitud de ventana temporal de entrada. Define cuántos valores históricos se usan para predecir el siguiente.

\textbf{units ($N_u$):} Número de unidades (dimensión del estado oculto) en cada capa LSTM. Controla la capacidad expresiva del modelo.

\textbf{epochs:} Número de pasadas sobre el conjunto de entrenamiento. Se implementa early stopping con paciencia de 50 épocas para prevenir sobreajuste.

\textbf{Parámetros Fijos:}
\begin{itemize}
\item $B = 3$ (número de modelos en ensamble)
\item $L = 2$ (número de capas LSTM)
\item $\text{lr} = 0.005$ (tasa de aprendizaje Adam)
\item $\text{batch\_size} = 16$
\item $\text{dropout} = 0.1$
\item $\alpha = 0.05$ (nivel de error nominal, cobertura 95\%)
\item $\text{num\_samples} = 1000$ (muestras de distribución Skew-Normal)
\end{itemize}

\paragraph{Espacio de Búsqueda Restringido}

Dada la alta complejidad computacional de entrenar ensambles de LSTMs, el espacio de hiperparámetros se limita estratégicamente a dos configuraciones:

\textbf{Configuración 1 (Conservadora):} $(N_x=10, N_u=24, \text{epochs}=20)$
\begin{itemize}
\item \textit{Filosofía:} Prioriza eficiencia computacional y convergencia rápida
\item \textit{Tiempo de entrenamiento:} Bajo ($\sim$10-15 min por ensamble completo)
\end{itemize}

\textbf{Configuración 2 (Estándar):} $(N_x=20, N_u=32, \text{epochs}=25)$
\begin{itemize}
\item \textit{Filosofía:} Balance entre expresividad y recursos computacionales
\item \textit{Tiempo de entrenamiento:} Medio ($\sim$20-30 min por ensamble completo)
\end{itemize}

\paragraph{Optimización y Congelamiento}

La selección entre ambas configuraciones se realiza minimizando \textbf{ECRPS} sobre un conjunto de validación temporal separado. Formalmente:

\begin{equation}
(N_x^*, N_u^*, \text{epochs}^*) = \arg\min_{(N_x, N_u, e) \in \mathcal{H}} \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{CRPS}(\mathcal{F}_i, y_i)
\end{equation}

donde $\mathcal{H} = \{(10, 24, 20), (20, 32, 25)\}$ y $\mathcal{F}_i$ es la distribución predictiva para la observación $i$.

Una vez optimizada la configuración, se ejecuta el protocolo de congelamiento completo mediante \texttt{freeze\_hyperparameters()}, que fija permanentemente:

\begin{itemize}
\item Los pesos $\{\theta_b\}_{b=1}^{B}$ de los $B$ modelos LSTM
\item Los parámetros de normalización MinMax: $(y_{\min}, y_{\max})$
\item Las distribuciones empíricas de scores conformales: $\{E_i^{\text{lo}}\}_{i=1}^{T}$ y $\{E_i^{\text{hi}}\}_{i=1}^{T}$
\end{itemize}

Durante la evaluación rolling window, para cada punto de prueba $t = T+1, \ldots, T+T'$, se extrae la ventana de entrada, se normalizan los datos con parámetros congelados, se generan predicciones de los $B$ modelos congelados, se agregan mediante media aritmética, se conformalizan usando scores congelados, se des-normalizan, y se ajusta la distribución Skew-Normal final. Este protocolo garantiza ausencia total de data leakage y evaluación justa comparable con otros métodos.

\paragraph{Justificación del Espacio Reducido}

La decisión de limitar el espacio de búsqueda a solo 2 configuraciones se fundamenta en:

\begin{enumerate}
\item \textbf{Costo computacional prohibitivo:} Cada configuración requiere entrenar $B=3$ LSTMs completos. Una búsqueda exhaustiva sobre $\{10,15,20\} \times \{24,32,64\} \times \{20,25,30\}$ implicaría 27 entrenamientos por serie, inviable para el benchmark de 100 series.

\item \textbf{Evidencia de rendimientos decrecientes:} Experimentos piloto mostraron que configuraciones intermedias rara vez superaban los extremos, sugiriendo un comportamiento bimodal del desempeño.

\item \textbf{Priorización de diversidad metodológica:} Los recursos computacionales se asignan a evaluar múltiples arquitecturas fundamentalmente diferentes (LSPM, MCPS, AREPD, DeepAR, EnCQR) en lugar de explorar exhaustivamente el espacio de un solo método.
\end{enumerate}

Esta estrategia de optimización restringida permite evaluar el potencial de EnCQR-LSTM manteniendo el estudio computacionalmente viable, reconociendo que representaciones más sofisticadas (búsqueda bayesiana, algoritmos genéticos) quedan fuera del alcance de esta investigación.

% ============================================================================
% SIMULACIONES COMPLEMENTARIAS
% ============================================================================


% ============================================================================
% SIMULACIONES COMPLEMENTARIAS
% ============================================================================

\section{Simulaciones Complementarias}
\label{sec:simulaciones_complementarias}

Además del diseño factorial principal descrito en la Sección~\ref{sec:diseño_experimental}, se implementaron cinco conjuntos de simulaciones complementarias diseñadas para resolver dilemas metodológicos específicos sobre el preprocesamiento, la persistencia de los datos, la arquitectura de la muestra y la propagación de la incertidumbre en horizontes lejanos. Estas simulaciones abordan preguntas fundamentales que no pueden responderse mediante el diseño principal debido a su estructura particular de ventana rodante a un paso.

% ----------------------------------------------------------------------------
\subsection{Simulación 1: Impacto de la Diferenciación en ARIMA ($d=1$)}
\label{subsec:sim1_diferenciacion}

\subsubsection{Motivación}

Esta simulación evalúa una pregunta metodológica fundamental: ¿los métodos conformales capturan mejor la variabilidad cuando operan sobre la serie diferenciada estacionaria ($\Delta Y_t$) o sobre los niveles integrados ($Y_t$)? Esta cuestión es relevante porque, aunque la diferenciación elimina la no estacionariedad y simplifica el modelado, también puede introducir pérdida de información sobre el nivel de la serie y afectar la estructura de autocorrelación \parencite{Hyndman2021FPP3}.

\subsubsection{Diseño Experimental}

Se utilizan las 7 configuraciones ARIMA$(p,1,q)$ del diseño principal (Sección~\ref{sec:procesos_generadores}), combinadas con las 5 distribuciones de ruido y 4 niveles de varianza, generando:

\begin{equation}
N_{\text{config}} = 7 \text{ procesos} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones base}
\end{equation}

Cada configuración se ejecuta bajo dos modalidades:

\begin{enumerate}
\item \textbf{SIN\_DIFF:} El modelo recibe la serie integrada $Y_t$ directamente y genera una distribución predictiva $\hat{F}_{Y_{t+1}}$ para el siguiente valor en niveles.

\item \textbf{CON\_DIFF:} El modelo recibe la serie diferenciada $\Delta Y_t = Y_t - Y_{t-1}$. La predicción generada $\widehat{\Delta Y}_{t+1}$ se integra mediante:
\begin{equation}
\hat{Y}_{t+1} = Y_t + \widehat{\Delta Y}_{t+1}
\end{equation}
para calcular el ECRPS en el espacio de niveles, permitiendo una comparación directa con la modalidad SIN\_DIFF.
\end{enumerate}

\subsubsection{Protocolo de Evaluación}

Siguiendo el esquema de la simulación principal:
\begin{itemize}
\item Serie simulada: $n_{\text{total}} = 252$ observaciones efectivas (200 entrenamiento + 40 calibración + 12 prueba)
\item Esquema de ventana rodante con 12 pasos de predicción
\item Evaluación de los 9 modelos conformales mediante ECRPS contra la distribución teórica
\end{itemize}

Esto genera:
\begin{equation}
N_{\text{filas}} = 140 \times 2 \text{ modalidades} \times 12 \text{ pasos} = 3{,}360 \text{ evaluaciones}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Simulación 2: Límites de Integración y Persistencia (Multi-D)}
\label{subsec:sim2_multi_d}

\subsubsection{Motivación}

Se investiga la estabilidad numérica de los métodos conformales ante órdenes de integración elevados $d \in \{1, 2, 3, 4, 5, 6, 7, 10\}$. A medida que $d$ aumenta, la serie integrada $Y_t$ exhibe una persistencia extrema y rangos de valores cada vez más amplios, lo que puede desestabilizar modelos que no utilizan diferenciación previa adecuada.

\subsubsection{Diseño Experimental}

A partir de las 7 configuraciones ARMA$(p,q)$ base, se genera el proceso ARIMA$(p,d,q)$ mediante:

\begin{equation}
Y_t = \sum_{j=0}^{d-1} \nabla^j W_t, \quad \text{donde } W_t \sim \text{ARMA}(p,q)
\end{equation}

Con la estructura factorial completa:
\begin{equation}
N_{\text{config}} = 7 \text{ ARMA} \times 8 \text{ valores de } d \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 1{,}120 \text{ configuraciones}
\end{equation}

Cada configuración se evalúa bajo las dos modalidades (SIN\_DIFF y CON\_DIFF) en 12 pasos:

\begin{equation}
N_{\text{filas}} = 1{,}120 \times 2 \times 12 = 26{,}880 \text{ evaluaciones}
\end{equation}

\subsubsection{Hipótesis}

Se hipotetiza que para $d \geq 4$, el rango explosivo de $Y_t$ desestabilizará a los modelos en modalidad SIN\_DIFF, mientras que la modalidad CON\_DIFF mantendrá estabilidad numérica al operar en el espacio diferenciado acotado.

% ----------------------------------------------------------------------------
\subsection{Simulación 3: Efectos del Tamaño Muestral Absoluto}
\label{subsec:sim3_tamano_muestral}

\subsubsection{Motivación}

Esta simulación caracteriza la tasa de convergencia de las distribuciones predictivas empíricas hacia la densidad teórica a medida que el volumen de datos aumenta. Permite cuantificar el trade-off entre calidad de estimación (que mejora con más datos de entrenamiento) y precisión de calibración (que mejora con más datos de calibración). A diferencia del diseño principal que mantiene fijos los tamaños $n_{\text{train}} = 200$ y $n_{\text{calib}} = 40$, aquí se explora sistemáticamente el espacio de tamaños absolutos manteniendo una proporción fija entre entrenamiento y calibración.

\subsubsection{Diseño Experimental}

Se evalúan los tres tipos de procesos (ARMA, ARIMA, SETAR) con sus 7 configuraciones cada uno, bajo cinco tamaños muestrales totales manteniendo una proporción fija de aproximadamente 83\% para entrenamiento y 17\% para calibración:

\begin{table}[htbp]
\centering
\caption{Tamaños muestrales evaluados con proporción fija}
\label{tab:combinaciones_muestrales}
\begin{tabular}{lcccc}
\toprule
\textbf{Etiqueta} & $n_{\text{train}}$ & $n_{\text{calib}}$ & $n_{\text{total}}$ & \textbf{Proporción} \\
\midrule
N=120 & 100 & 20 & 120 & 83:17 \\
N=240 & 199 & 41 & 240 & 83:17 \\
N=360 & 299 & 61 & 360 & 83:17 \\
N=600 & 498 & 102 & 600 & 83:17 \\
N=1200 & 996 & 204 & 1{,}200 & 83:17 \\
\bottomrule
\end{tabular}
\end{table}

Esta estructura permite evaluar el efecto del incremento simultáneo de datos de entrenamiento y calibración, manteniendo constante su proporción relativa. Esto aísla el efecto puro del tamaño muestral total sobre la calidad de las distribuciones predictivas.

La estructura factorial completa genera:
\begin{equation}
\begin{split}
N_{\text{config}} = & \ 3 \text{ tipos} \times 7 \text{ configs} \times 5 \text{ tamaños} \\
& \times 5 \text{ distribuciones} \times 4 \text{ varianzas} \\
= & \ 2{,}100 \text{ configuraciones}
\end{split}
\end{equation}

Con 12 pasos de evaluación por configuración:
\begin{equation}
N_{\text{filas}} = 2{,}100 \times 12 = 25{,}200 \text{ evaluaciones}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Simulación 4: Proporciones de Calibración con Tamaño Fijo}
\label{subsec:sim4_proporciones}

\subsubsection{Motivación}

En escenarios de datos limitados, existe un conflicto fundamental entre usar datos para mejorar el ajuste del modelo (\textit{training}) o para mejorar la precisión de los intervalos conformales (\textit{calibration}). Esta simulación busca determinar si existe una ``proporción óptima'' que minimice el ECRPS promedio cuando el presupuesto total de datos es fijo.

\subsubsection{Diseño Experimental}

Se fija un presupuesto total de $n_{\text{total}} = 240$ observaciones y se evalúan 5 proporciones de calibración diferentes:

\begin{table}[htbp]
\centering
\caption{Proporciones de calibración evaluadas (N=240 fijo)}
\label{tab:proporciones_calib}
\begin{tabular}{lccc}
\toprule
\textbf{Proporción} & $n_{\text{train}}$ & $n_{\text{calib}}$ & \textbf{Ratio} \\
\midrule
10\% & 216 & 24 & 9:1 \\
20\% & 192 & 48 & 4:1 \\
30\% & 168 & 72 & 7:3 \\
40\% & 144 & 96 & 3:2 \\
50\% & 120 & 120 & 1:1 \\
\bottomrule
\end{tabular}
\end{table}

La estructura factorial completa es:
\begin{equation}
\begin{split}
N_{\text{config}} = & \ 3 \text{ tipos} \times 7 \text{ configs} \times 5 \text{ proporciones} \\
& \times 5 \text{ distribuciones} \times 4 \text{ varianzas} \\
= & \ 2{,}100 \text{ configuraciones}
\end{split}
\end{equation}

Con 12 pasos de evaluación por configuración:
\begin{equation}
N_{\text{filas}} = 2{,}100 \times 12 = 25{,}200 \text{ evaluaciones}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Simulación 5: Predicción Multi-paso (Horizonte $h$)}
\label{subsec:sim5_multi_paso}

\subsubsection{Motivación}

El diseño factorial principal evalúa la predicción a un paso adelante mediante un esquema de ventana rodante, donde el modelo se actualiza con cada nueva observación antes de realizar la siguiente predicción. Sin embargo, muchas aplicaciones prácticas requieren pronósticos para múltiples períodos futuros sin acceso a observaciones intermedias \parencite{Hyndman2021FPP3}. Esta simulación evalúa cómo se degrada la calidad de la distribución predictiva cuando el modelo debe proyectar $h$ pasos hacia adelante de forma recursiva, alimentándose exclusivamente de sus propias predicciones anteriores.

\subsubsection{Fundamento Metodológico: Predicción Recursiva}

Existen dos estrategias principales para pronóstico multi-paso \parencite{Taieb2012}:

\begin{enumerate}
\item \textbf{Estrategia Directa:} Entrenar modelos separados para cada horizonte $h$, cada uno estimando directamente $Y_{t+h}$ desde $Y_{1:t}$. Aunque conceptualmente simple, requiere entrenar $H$ modelos independientes y no aprovecha la estructura secuencial del problema.

\item \textbf{Estrategia Recursiva (Iterativa):} Utilizar un único modelo de predicción a un paso y aplicarlo recursivamente:
\begin{equation}
\hat{Y}_{t+h} = f(\hat{Y}_{t+h-1}, \hat{Y}_{t+h-2}, \ldots, Y_t), \quad h = 2, 3, \ldots, H
\end{equation}
Esta estrategia, aunque propaga errores de predicción, es más eficiente computacionalmente y refleja mejor la práctica operativa donde no hay acceso a observaciones futuras verdaderas \parencite{Bontempi2013}.
\end{enumerate}

La presente simulación implementa la estrategia recursiva, que es la más relevante para evaluar métodos conformales en horizontes extendidos. La propagación de incertidumbre en este contexto ha sido estudiada por \textcite{Gneiting2014} y \textcite{Stankeviciute2021}, quienes demuestran que la distribución predictiva en el horizonte $h$ debe considerar tanto la incertidumbre del modelo como la acumulación de errores de pasos previos.

\subsubsection{Generación de Trayectorias Estocásticas}

Para cada configuración y modelo, se generan $N_{\text{traj}} = 100$ trayectorias completas desde el mismo punto de origen $t$. Cada trayectoria $m$ se construye mediante muestreo recursivo de las distribuciones predictivas:

\begin{equation}
\begin{aligned}
\hat{Y}_{t+1}^{(m)} & \sim \hat{F}_{\text{modelo}}(\cdot \mid Y_{1:t}) \\
\hat{Y}_{t+2}^{(m)} & \sim \hat{F}_{\text{modelo}}(\cdot \mid Y_{1:t}, \hat{Y}_{t+1}^{(m)}) \\
& \vdots \\
\hat{Y}_{t+h}^{(m)} & \sim \hat{F}_{\text{modelo}}\left(\cdot \mid Y_{1:t}, \hat{Y}_{t+1}^{(m)}, \ldots, \hat{Y}_{t+h-1}^{(m)}\right)
\end{aligned}
\end{equation}

donde $m = 1, \ldots, 100$ indexa las trayectorias independientes. Este procedimiento genera una distribución empírica para cada horizonte $h$, formada por las 100 realizaciones $\{\hat{Y}_{t+h}^{(1)}, \ldots, \hat{Y}_{t+h}^{(100)}\}$.

\subsubsection{Diseño Experimental}

Se evalúan únicamente 4 modelos representativos (LSPM, DeepAR, Sieve Bootstrap, MCPS) debido al alto costo computacional de generar 100 trayectorias completas por escenario. La selección incluye:

\begin{itemize}
\item \textbf{LSPM:} Método conformal clásico basado en cuantiles empíricos
\item \textbf{DeepAR:} Método paramétrico de aprendizaje profundo que modela distribuciones completas
\item \textbf{Sieve Bootstrap:} Método no paramétrico basado en remuestreo de residuos
\item \textbf{MCPS:} Método conformal contemporáneo que particiona el espacio de calibración
\end{itemize}

Para cada tipo de proceso (ARMA, ARIMA, SETAR), la estructura factorial es:
\begin{equation}
N_{\text{config}} = 7 \text{ configs} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones}
\end{equation}

Con 3 tipos de procesos y 12 horizontes de predicción:
\begin{equation}
N_{\text{filas}} = 3 \times 140 \times 12 = 5{,}040 \text{ evaluaciones}
\end{equation}

\subsubsection{Protocolo de Evaluación}

El protocolo sigue la estructura del diseño principal con las siguientes particularidades:

\begin{enumerate}
\item \textbf{Punto de origen fijo:} A diferencia de la ventana rodante, aquí todas las predicciones parten del mismo punto temporal $t = n_{\text{train}} + n_{\text{calib}}$.

\item \textbf{Sin actualización intermedia:} El modelo se ajusta una sola vez con los datos disponibles hasta $t$ y no se actualiza durante la proyección de los $H=12$ pasos.

\item \textbf{Evaluación por horizonte:} Para cada $h \in \{1, 2, \ldots, 12\}$, se calcula:
\begin{equation}
\text{ECRPS}_h = \text{ECRPS}\left(\{\hat{Y}_{t+h}^{(1)}, \ldots, \hat{Y}_{t+h}^{(100)}\}, \{Y_{t+h}^{(\text{true}, 1)}, \ldots, Y_{t+h}^{(\text{true}, 1000)}\}\right)
\end{equation}

\item \textbf{Análisis de degradación:} El experimento permite cuantificar la tasa de crecimiento de $\text{ECRPS}_h$ conforme $h$ aumenta, caracterizando la velocidad de degradación de la calidad predictiva.
\end{enumerate}

Esta simulación es particularmente relevante para aplicaciones donde las decisiones deben tomarse con base en pronósticos de mediano plazo sin posibilidad de actualización frecuente del modelo, como en planeación energética, gestión de inventarios o política monetaria \parencite{Hyndman2021FPP3}.

% ----------------------------------------------------------------------------
\subsection{Resumen de Evaluaciones Complementarias}
\label{subsec:resumen_complementarias}

La Tabla~\ref{tab:resumen_complementarias} consolida el alcance total de estos experimentos. El volumen combinado de estas simulaciones complementarias es comparable al del diseño factorial principal, reflejando la importancia de estos aspectos metodológicos para una evaluación comprehensiva.

\begin{table}[htbp]
\centering
\caption{Resumen de la carga experimental de simulaciones complementarias}
\label{tab:resumen_complementarias}
\small
\begin{tabular}{lp{6cm}c}
\toprule
\textbf{Simulación} & \textbf{Factor Variado} & \textbf{Filas} \\
\midrule
1. Diferenciación ($d=1$) & 
    Modalidad (SIN\_DIFF vs CON\_DIFF) & 
    3{,}360 \\
\addlinespace
2. Multi-D & 
    Orden de integración $d \in \{1, \ldots, 10\}$ & 
    26{,}880 \\
\addlinespace
3. Tamaño Muestral & 
    Tamaño total con proporción fija (83:17) & 
    25{,}200 \\
\addlinespace
4. Proporciones & 
    Proporción de calibración ($n_{\text{total}} = 240$ fijo) & 
    25{,}200\\
\addlinespace
5. Multi-paso & 
    Horizonte de predicción $h \in \{1, \ldots, 12\}$ & 
    5{,}040 \\
\midrule
\textbf{Total} & & \textbf{85{,}680} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Protocolo Común a Todas las Simulaciones}

Todas las simulaciones complementarias mantienen consistencia metodológica con el diseño principal descrito en la Sección~\ref{sec:diseño_experimental}:

\begin{itemize}
\item \textbf{Período de inicialización (burn-in):} 100 observaciones descartadas antes del inicio efectivo de la serie para eliminar efectos transitorios de las condiciones iniciales.

\item \textbf{Métrica de evaluación:} ECRPS entre la distribución predictiva empírica del modelo (basada en 1,000 muestras bootstrap o generadas por el modelo) y la distribución teórica verdadera del proceso generador de datos (representada por 1,000 muestras de la densidad real), como se define en la Sección~\ref{subsec:ecrps}.

\item \textbf{Modelos evaluados:} Los 9 métodos conformales especificados en la Sección~\ref{sec:modelos_predictivos} (Block Bootstrapping, Sieve Bootstrap, LSPM, LSPMW, AREPD, MCPS, AV-MCPS, DeepAR, EnCQR-LSTM), excepto en la Simulación 5 donde por razones de eficiencia computacional se evalúan únicamente 4 modelos representativos (LSPM, DeepAR, Sieve Bootstrap, MCPS).

\item \textbf{Calibración y ajuste de hiperparámetros:} Cada modelo se optimiza siguiendo un procedimiento de dos etapas. Primero, los hiperparámetros del modelo base se seleccionan mediante validación cruzada temporal en el conjunto de entrenamiento, minimizando el ECRPS en una ventana de validación. Segundo, los parámetros conformales (como el nivel de cobertura o los pesos de calibración) se ajustan usando el conjunto de calibración dedicado. Este procedimiento se describe en detalle en la Sección~\ref{sec:optimizacion_hiperparametros}.

\item \textbf{Control de reproducibilidad:} Semillas aleatorias fijas y documentadas para cada escenario, con incrementos determinísticos según el índice del escenario ($\text{seed} = \text{seed}_{\text{base}} + \text{id}_{\text{escenario}}$), permitiendo la replicación exacta de todos los experimentos.

\item \textbf{Esquema de evaluación:} 
\begin{itemize}
    \item \textit{Ventana rodante} (Simulaciones 1--4): El modelo se actualiza con cada nueva observación antes de predecir el siguiente paso, generando 12 predicciones secuenciales donde el horizonte siempre es $h=1$ pero el conjunto de entrenamiento crece.
    \item \textit{Proyección desde origen fijo} (Simulación 5): El modelo se ajusta una sola vez en $t$ y proyecta recursivamente los horizontes $h \in \{1, 2, \ldots, 12\}$ sin actualización intermedia, propagando la incertidumbre a través de predicciones iteradas.
\end{itemize}

\item \textbf{Estructura factorial completa:} Todas las simulaciones evalúan las 21 configuraciones de procesos (7 ARMA + 7 ARIMA + 7 SETAR) cruzadas con 5 distribuciones de ruido (normal, uniforme, exponencial, t-student, mezcla) y 4 niveles de varianza (0.2, 0.5, 1.0, 3.0), garantizando cobertura exhaustiva del espacio paramétrico.
\end{itemize}

La uniformidad en estos aspectos fundamentales garantiza que las diferencias observadas en el desempeño sean atribuibles exclusivamente a los factores experimentales bajo estudio (modalidad de diferenciación, orden de integración, tamaño muestral, proporción de calibración, u horizonte de predicción) y no a variaciones en el protocolo de evaluación.