% !TeX root = ../main.tex
\chapter{Diseño de la Simulación}
\label{cap:diseño_simulacion}

Este capítulo describe el diseño experimental desarrollado para evaluar el desempeño de los métodos de pronóstico probabilístico en series temporales. Se presenta la justificación de los escenarios de evaluación, la metodología de simulación empleada y las características específicas de los procesos generadores de datos utilizados.

\section{Introducción}
\label{sec:intro_simulacion}

La evaluación rigurosa de metodologías de pronóstico probabilístico requiere un marco experimental controlado que permita comparar el desempeño de diferentes técnicas bajo condiciones conocidas. A diferencia de los estudios con datos reales, donde la distribución verdadera es desconocida y la evaluación se limita a métricas indirectas, los estudios de simulación ofrecen la ventaja fundamental de conocer exactamente el proceso generador de datos (DGP, por sus siglas en inglés) \parencite{Hyndman2021FPP3}.

Este conocimiento del DGP permite evaluar directamente la calidad de las distribuciones predictivas mediante su comparación con la verdadera distribución teórica. En particular, el uso del ECRPS (Expected Continuous Ranked Probability Score) como métrica principal de evaluación se justifica porque permite cuantificar simultáneamente la calibración y la nitidez de los pronósticos probabilísticos, comparando las muestras generadas por cada método con muestras de la distribución teórica verdadera \parencite{Gneiting2014}.

El diseño experimental desarrollado considera tres dimensiones fundamentales de variación: (1) la estructura temporal del proceso (estacionariedad y linealidad), (2) la distribución del término de error, y (3) la magnitud de la varianza del ruido. Esta combinación genera un espacio de escenarios suficientemente amplio para evaluar la robustez y adaptabilidad de los métodos bajo diferentes condiciones operativas.

\section{Diseño de la Simulación}
\label{sec:diseño_experimental}

\subsection{Selección de Escenarios de Evaluación}
\label{subsec:escenarios}

El presente estudio considera tres escenarios fundamentales que caracterizan diferentes clases de comportamiento en series temporales. La selección de estos escenarios se fundamenta en la clasificación teórica de procesos estocásticos y en consideraciones de relevancia práctica.

\subsubsection{Escenario 1: Lineal Estacionario (ARMA)}

El primer escenario considera procesos autorregresivos de media móvil (ARMA), que representan la clase fundamental de modelos lineales estacionarios. Un proceso ARMA$(p,q)$ se caracteriza por su capacidad de capturar tanto la persistencia temporal (componente AR) como la dependencia de shocks pasados (componente MA), manteniendo propiedades estadísticas constantes en el tiempo \parencite{Arrieta2017}.

La estacionariedad de estos procesos garantiza que la media, varianza y estructura de autocorrelación permanezcan invariantes bajo traslaciones temporales, lo que facilita la modelación y el pronóstico \parencite{Hyndman2021FPP3}. Este escenario permite evaluar el desempeño de los métodos en condiciones ideales, donde los supuestos fundamentales de muchas técnicas estadísticas se cumplen.

\subsubsection{Escenario 2: Lineal No Estacionario (ARIMA)}

El segundo escenario aborda procesos autorregresivos integrados de media móvil (ARIMA), que extienden la clase ARMA para series con tendencias estocásticas. La presencia de raíces unitarias en el polinomio autorregresivo genera comportamientos de paseo aleatorio que son comunes en series económicas y financieras \parencite{Hyndman2021FPP3}.

La no estacionariedad introduce desafíos adicionales para el pronóstico probabilístico, ya que la incertidumbre crece sin límite conforme aumenta el horizonte de predicción. Este escenario permite evaluar la capacidad de los métodos para adaptarse a estructuras no estacionarias mediante diferenciación o técnicas adaptativas.

\subsubsection{Escenario 3: No Lineal Estacionario (SETAR)}

El tercer escenario considera modelos autorregresivos de umbral auto-excitados (SETAR), que permiten cambios estructurales endógenos en la dinámica del proceso. Estos modelos capturan no linealidades mediante el cambio de régimen determinado por valores pasados de la propia serie \parencite{Chen2023}.

La estacionariedad global de un proceso SETAR requiere condiciones específicas sobre los parámetros autorregresivos en cada régimen y la frecuencia de transición entre regímenes. Estas condiciones se discuten en detalle en la Sección~\ref{subsec:setar_stationarity}. Este escenario es particularmente relevante para evaluar la capacidad de los métodos conformales de capturar dinámicas asimétricas y dependientes del estado del sistema.

\subsubsection{Ausencia del Escenario No Lineal No Estacionario}

La combinación de no linealidad y no estacionariedad, aunque teóricamente posible, presenta desafíos metodológicos sustanciales que la excluyen del alcance de este estudio. Los modelos que combinan ambas características (por ejemplo, SETAR con raíces unitarias condicionales o modelos de cambio de régimen con deriva) requieren condiciones de estabilidad extremadamente restrictivas y su caracterización teórica es un área de investigación activa \parencite{Chen2023}.

Más fundamentalmente, la validez teórica de muchos métodos de predicción conformal, incluyendo aquellos basados en el enfoque de Barber et al. \parencite{Barber2023}, asume que el proceso subyacente es al menos localmente estacionario o que las desviaciones de la estacionariedad son graduales y pueden ser capturadas mediante esquemas de ponderación adaptativos. La presencia simultánea de cambios estructurales abruptos (no linealidad) y tendencias estocásticas persistentes (no estacionariedad) violaría estos supuestos fundamentales, invalidando las garantías teóricas de cobertura.

Por estas razones, el presente estudio se enfoca en los tres escenarios anteriores, que permiten una evaluación rigurosa y teóricamente fundamentada del desempeño de los métodos.

\subsection{Estructura del Diseño Factorial}
\label{subsec:diseño_factorial}

El diseño experimental implementa un esquema factorial completo que combina sistemáticamente tres dimensiones de variación para cada uno de los tres escenarios considerados. Esta estructura genera un total de 420 configuraciones únicas de simulación, distribuidas equitativamente entre los escenarios.

\subsubsection{Dimensión 1: Configuraciones Paramétricas del Proceso}

Para cada clase de modelo (ARMA, ARIMA, SETAR), se consideran 7 configuraciones paramétricas distintas que representan diferentes grados de complejidad y características dinámicas. Las especificaciones detalladas de estas configuraciones se presentan en la Sección~\ref{sec:procesos_generadores}. Esta diversidad paramétrica permite evaluar la sensibilidad de los métodos a diferentes estructuras de dependencia temporal.

\subsubsection{Dimensión 2: Distribuciones del Término de Error}

Se consideran cinco familias de distribuciones para el término de innovación $\varepsilon_t$, seleccionadas para representar diferentes características de forma, simetría y comportamiento en las colas:

\begin{enumerate}
\item \textbf{Normal:} $\varepsilon_t \sim N(0, \sigma^2)$. Representa el caso base con colas ligeras y simetría perfecta.

\item \textbf{T-Student:} $\varepsilon_t \sim \sigma \cdot \frac{t_{18}}{\sqrt{18/16}}$, donde $t_{18}$ denota una distribución t de Student con 18 grados de libertad. Esta parametrización garantiza varianza unitaria y genera colas más pesadas que la normal, capturando eventos extremos más frecuentes.

\item \textbf{Exponencial:} $\varepsilon_t \sim \sigma(Y - 1)$, donde $Y \sim \text{Exp}(1)$. Produce asimetría positiva y es relevante para series que modelan variables intrínsecamente positivas o con shocks unidireccionales.

\item \textbf{Uniforme:} $\varepsilon_t \sim U(-\sqrt{3}\sigma, \sqrt{3}\sigma)$. Genera soporte acotado y ausencia de colas, representando un caso extremo de curtosis negativa.

\item \textbf{Mixtura de Normales:} $\varepsilon_t \sim 0.75 \cdot N(-\sigma/4, \sigma^2/16) + 0.25 \cdot N(3\sigma/4, \sigma^2/16)$. Produce bimodalidad y permite evaluar el desempeño bajo distribuciones predictivas complejas con múltiples modas.
\end{enumerate}

Esta selección permite evaluar la robustez de los métodos ante desviaciones del supuesto de normalidad que frecuentemente se asume en la literatura de pronóstico \parencite{Arrieta2017}.

\subsubsection{Dimensión 3: Niveles de Varianza del Error}

Se consideran cuatro niveles de varianza $\sigma^2 \in \{0.2, 0.5, 1.0, 3.0\}$ que representan diferentes razones señal-ruido. El nivel base $\sigma^2 = 1.0$ corresponde a la parametrización estándar, mientras que $\sigma^2 = 0.2$ representa un escenario de alta predictibilidad y $\sigma^2 = 3.0$ captura situaciones de alta volatilidad donde la incertidumbre inherente domina la dinámica del sistema.

\subsubsection{Combinatoria Total}

La combinación factorial de estas tres dimensiones genera:
\begin{equation}
N_{\text{config}} = 7 \text{ modelos} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones por escenario}
\end{equation}

Con tres escenarios (ARMA, ARIMA, SETAR), el espacio experimental completo comprende:
\begin{equation}
N_{\text{total}} = 140 \times 3 = 420 \text{ configuraciones únicas}
\end{equation}

Adicionalmente, considerando que cada configuración se evalúa en un horizonte de predicción de 12 pasos usando ventana rodante para que se realice predicción a un paso adelante, el número total de combinaciones configuración-horizonte es de $420 \times 12 = 5040$.

\subsection{Protocolo de Simulación y Partición de Datos}
\label{subsec:protocolo_simulacion}

Para cada una de las 420 configuraciones, se implementa el siguiente protocolo de simulación:

\begin{enumerate}
\item \textbf{Generación de la Serie:} Se simulan $n_{\text{total}} = 302$ observaciones del proceso especificado, precedidas por un período de burn-in de 50 observaciones que se descartan para eliminar el efecto de las condiciones iniciales. Esto resulta en una serie efectiva de longitud $n = 252$.

\item \textbf{Partición Tripartita:} La serie se divide en tres conjuntos disjuntos:
\begin{itemize}
\item \textbf{Conjunto de Entrenamiento:} $n_{\text{train}} = 200$ observaciones iniciales utilizadas para la estimación inicial de parámetros y el ajuste de hiperparámetros.
\item \textbf{Conjunto de Calibración:} $n_{\text{cal}} = 40$ observaciones subsecuentes utilizadas para la calibración de intervalos de predicción y la construcción de distribuciones conformales.
\item \textbf{Conjunto de Prueba:} $n_{\text{test}} = 12$ observaciones finales utilizadas para la evaluación del desempeño predictivo.
\end{itemize}

\item \textbf{Esquema de Ventana Rodante:} La evaluación se realiza mediante una ventana rodante (rolling window) donde:
\begin{itemize}
\item Para el primer paso de predicción, se utilizan las primeras 200 observaciones para entrenamiento y las siguientes 40 para calibración.
\item Para cada paso $h = 1, \ldots, 12$, la ventana de entrenamiento se extiende para incluir las observaciones anteriores, manteniendo fijo el conjunto de calibración de tamaño 40 inmediatamente anterior al punto de predicción.
\item Este esquema emula una situación operativa donde el analista actualiza periódicamente los modelos conforme nueva información se hace disponible.
\end{itemize}

\item \textbf{Generación de Distribuciones Predictivas:} Para cada método y cada paso de predicción $h$, se generan muestras de la distribución predictiva. Estas muestras se comparan con muestras de la distribución teórica verdadera del proceso (conocida por construcción del DGP) mediante el cálculo del ECRPS para ese paso específico.


\end{enumerate}

Este protocolo garantiza que la evaluación sea tanto rigurosa (mediante la comparación con la distribución verdadera) como realista (mediante el esquema de ventana rodante que refleja la práctica operativa).

\section{Procesos Generadores de Datos}
\label{sec:procesos_generadores}

Esta sección describe formalmente los modelos utilizados como procesos generadores de datos en cada escenario, junto con las configuraciones paramétricas específicas consideradas. Para cada clase de modelo, se presentan las ecuaciones fundamentales, las condiciones de estacionariedad (cuando corresponda) y las parametrizaciones concretas evaluadas.
\subsection{Procesos ARMA: Escenario Lineal Estacionario}
\label{subsec:arma}

\subsubsection{Definición y Representación}

Un proceso autorregresivo de media móvil de órdenes $p$ y $q$, denotado ARMA$(p,q)$, se define mediante la ecuación en diferencias estocástica:
\begin{equation}
Y_t = c + \sum_{i=1}^p \phi_i Y_{t-i} + \varepsilon_t + \sum_{j=1}^q \theta_j \varepsilon_{t-j}
\label{eq:arma_general}
\end{equation}
donde $c$ es un término constante, $\{\phi_i\}_{i=1}^p$ son los coeficientes autorregresivos, $\{\theta_j\}_{j=1}^q$ son los coeficientes de media móvil, y $\{\varepsilon_t\}$ es un proceso de ruido blanco con media cero y varianza $\sigma^2$.

Utilizando el operador de rezagos $L$ definido por $L^k Y_t = Y_{t-k}$, el proceso puede expresarse en forma compacta:
\begin{equation}
\Phi(L) Y_t = c + \Theta(L) \varepsilon_t
\label{eq:arma_lag_operator}
\end{equation}
donde $\Phi(L) = 1 - \sum_{i=1}^p \phi_i L^i$ es el polinomio autorregresivo y $\Theta(L) = 1 + \sum_{j=1}^q \theta_j L^j$ es el polinomio de media móvil.

\subsubsection{Condiciones de Estacionariedad e Invertibilidad}

La estacionariedad y la invertibilidad de un proceso ARMA están determinadas por las raíces de sus polinomios característicos \parencite{Hyndman2021FPP3}:

\begin{itemize}
\item \textbf{Estacionariedad:} El proceso es estacionario en covarianza si y solo si todas las raíces del polinomio autorregresivo $\Phi(z) = 0$ se encuentran estrictamente fuera del círculo unitario complejo. Equivalentemente, las raíces del polinomio $\Phi(L)$ deben satisfacer $|z_i| > 1$ para todo $i$.

\item \textbf{Invertibilidad:} El proceso es invertible si y solo si todas las raíces del polinomio de media móvil $\Theta(z) = 0$ se encuentran estrictamente fuera del círculo unitario complejo.
\end{itemize}

Estas condiciones garantizan que el proceso admite representaciones de Wold (MA$(\infty)$) y autorregresiva (AR$(\infty)$) convergentes, lo que es fundamental para la teoría de pronóstico \parencite{Arrieta2017}.

\subsubsection{Distribución Predictiva Verdadera}

Para un proceso ARMA estacionario e invertible, la distribución del siguiente valor $Y_{n+1}$ condicionada a la historia observada $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$ tiene una forma analítica explícita. Dado que el modelo es lineal, la distribución condicional está completamente caracterizada por su media y varianza condicionales.

La media condicional se obtiene de la ecuación estructural del modelo:
\begin{equation}
\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n] = c + \sum_{i=1}^p \phi_i Y_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j}
\label{eq:arma_conditional_mean}
\end{equation}

donde todos los términos del lado derecho son conocidos. La varianza condicional es constante e igual a la varianza del ruido:
\begin{equation}
\text{Var}[Y_{n+1} \mid \mathcal{F}_n] = \sigma^2
\label{eq:arma_conditional_variance}
\end{equation}

Por lo tanto, si el ruido $\varepsilon_t$ sigue una distribución $F$ con media cero y varianza $\sigma^2$, la distribución predictiva verdadera es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F\left(\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\label{eq:arma_predictive_distribution}
\end{equation}

Esta distribución puede evaluarse numéricamente generando una muestra grande de errores futuros $\varepsilon_{n+1}^{(b)} \sim F(0, \sigma^2)$ y calculando:
\begin{equation}
Y_{n+1}^{(b)} = c + \sum_{i=1}^p \phi_i Y_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j} + \varepsilon_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:arma_monte_carlo_samples}
\end{equation}

donde $B$ es un número suficientemente grande (en esta investigación, $B = 1000$). Esta muestra empírica aproxima la distribución predictiva verdadera y sirve como referencia para el cálculo del ECRPS.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:arma_configs} presenta las siete configuraciones ARMA consideradas en este estudio. La selección incluye modelos puramente autorregresivos [AR(1), AR(2)], puramente de media móvil [MA(1), MA(2)], y mixtos [ARMA(1,1), ARMA(2,2), ARMA(2,1)], con diferentes grados de persistencia temporal y complejidad estructural.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos ARMA.}
\label{tab:arma_configs}
\begin{tabular}{lcccc}
\toprule
\textbf{Nombre} & \textbf{$p$} & \textbf{$q$} & \textbf{$\boldsymbol{\phi}$} & \textbf{$\boldsymbol{\theta}$} \\
\midrule
AR(1)     & 1 & 0 & $[0.9]$              & $[]$           \\
AR(2)     & 2 & 0 & $[0.5, -0.3]$        & $[]$           \\
MA(1)     & 0 & 1 & $[]$                 & $[0.7]$        \\
MA(2)     & 0 & 2 & $[]$                 & $[0.4, 0.2]$   \\
ARMA(1,1) & 1 & 1 & $[0.6]$              & $[0.3]$        \\
ARMA(2,2) & 2 & 2 & $[0.4, -0.2]$        & $[0.5, 0.1]$   \\
ARMA(2,1) & 2 & 1 & $[0.7, 0.2]$         & $[0.5]$        \\
\bottomrule
\end{tabular}
\end{table}

Todas las configuraciones fueron verificadas para satisfacer las condiciones de estacionariedad e invertibilidad mediante el cálculo numérico de las raíces de los polinomios característicos correspondientes.

\subsection{Procesos ARIMA: Escenario Lineal No Estacionario}
\label{subsec:arima}

\subsubsection{Definición y Operador de Diferenciación}

Un proceso autorregresivo integrado de media móvil de órdenes $(p,d,q)$, denotado ARIMA$(p,d,q)$, se construye aplicando el operador de diferenciación $\Delta = 1 - L$ un total de $d$ veces a una serie $Y_t$ y modelando la serie diferenciada resultante $W_t = \Delta^d Y_t$ mediante un proceso ARMA$(p,q)$ estacionario:
\begin{equation}
\Phi(L) W_t = c + \Theta(L) \varepsilon_t
\label{eq:arima_general}
\end{equation}
donde $W_t = (1-L)^d Y_t$.

Equivalentemente, en términos de la serie original:
\begin{equation}
\Phi(L)(1-L)^d Y_t = c + \Theta(L) \varepsilon_t
\label{eq:arima_original_scale}
\end{equation}

El orden de integración $d$ representa el número de raíces unitarias en el polinomio autorregresivo ampliado. En la gran mayoría de aplicaciones prácticas, $d \in \{0, 1, 2\}$, siendo $d=1$ el caso más frecuente \parencite{Hyndman2021FPP3}.

\subsubsection{Propiedades de Estacionariedad}

Un proceso ARIMA$(p,d,q)$ es no estacionario por construcción cuando $d > 0$, debido a la presencia de raíces unitarias. Sin embargo, la serie diferenciada $W_t = \Delta^d Y_t$ es estacionaria si el componente ARMA$(p,q)$ subyacente satisface las condiciones de estacionariedad e invertibilidad descritas en la Sección~\ref{subsec:arma}.

Esta propiedad de \textit{estacionariedad en diferencias} es fundamental para el pronóstico, ya que permite aplicar toda la teoría desarrollada para procesos estacionarios a la serie transformada $W_t$, recuperando posteriormente los pronósticos en la escala original mediante integración sucesiva \parencite{Hyndman2021FPP3}.

\subsubsection{Distribución Predictiva Verdadera}

Para un proceso ARIMA$(p,d,q)$, la distribución del siguiente valor $Y_{n+1}$ condicionada a la historia observada se obtiene mediante un procedimiento de dos etapas que explota la estructura de diferenciación del modelo.

Primero, se predice el siguiente valor de la serie diferenciada $W_{n+1} = \Delta^d Y_{n+1}$ usando la distribución ARMA subyacente. Para el caso más común $d=1$, la serie diferenciada es:
\begin{equation}
W_t = Y_t - Y_{t-1}
\end{equation}

y su predicción un paso adelante, condicionada a la historia $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$, sigue la distribución ARMA:
\begin{equation}
W_{n+1} \mid \mathcal{F}_n \sim F\left(\mathbb{E}[W_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\end{equation}

donde:
\begin{equation}
\mathbb{E}[W_{n+1} \mid \mathcal{F}_n] = c + \sum_{i=1}^p \phi_i W_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j}
\label{eq:arima_diff_conditional_mean}
\end{equation}

Segundo, se recupera la predicción en la escala original mediante la relación de integración:
\begin{equation}
Y_{n+1} = Y_n + W_{n+1}
\label{eq:arima_integration}
\end{equation}

Por lo tanto, la distribución predictiva verdadera para $Y_{n+1}$ es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F\left(Y_n + \mathbb{E}[W_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\label{eq:arima_predictive_distribution}
\end{equation}

Esta distribución puede evaluarse numéricamente generando muestras del incremento futuro:
\begin{equation}
W_{n+1}^{(b)} = c + \sum_{i=1}^p \phi_i W_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j} + \varepsilon_{n+1}^{(b)}
\label{eq:arima_diff_samples}
\end{equation}

y aplicando la transformación:
\begin{equation}
Y_{n+1}^{(b)} = Y_n + W_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:arima_level_samples}
\end{equation}

donde $\varepsilon_{n+1}^{(b)} \sim F(0, \sigma^2)$ son errores futuros independientes. Esta muestra empírica representa la distribución predictiva verdadera que sirve como referencia para el ECRPS.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:arima_configs} presenta las siete configuraciones ARIMA$(p,1,q)$ consideradas en este estudio. Todas las configuraciones utilizan $d=1$, reflejando el caso más común en aplicaciones económicas y financieras. La selección incluye desde el paseo aleatorio puro [ARIMA(0,1,0)] hasta modelos con estructura autorregresiva y de media móvil en la serie diferenciada.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos ARIMA.}
\label{tab:arima_configs}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccc}
\toprule
\textbf{Nombre} & \boldmath$p$ & \boldmath$d$ & \boldmath$q$ & \boldmath$\phi$ & \boldmath$\theta$ \\
\midrule
ARIMA(0,1,0) & 0 & 1 & 0 & $[]$              & $[]$              \\
ARIMA(1,1,0) & 1 & 1 & 0 & $[0.6]$           & $[]$              \\
ARIMA(2,1,0) & 2 & 1 & 0 & $[0.5, -0.2]$     & $[]$              \\
ARIMA(0,1,1) & 0 & 1 & 1 & $[]$              & $[0.5]$           \\
ARIMA(0,1,2) & 0 & 1 & 2 & $[]$              & $[0.4, 0.25]$     \\
ARIMA(1,1,1) & 1 & 1 & 1 & $[0.7]$           & $[-0.3]$          \\
ARIMA(2,1,2) & 2 & 1 & 2 & $[0.6, 0.2]$      & $[0.4, -0.1]$     \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Procesos SETAR: Escenario No Lineal Estacionario}
\label{subsec:setar}

\subsubsection{Definición y Mecanismo de Cambio de Régimen}

Un modelo autorregresivo de umbral auto-excitado con dos regímenes, denotado SETAR(2; $p_1$, $p_2$), se define mediante una estructura de cambio de régimen determinado por valores pasados de la propia serie \parencite{Chen2023}:

\begin{equation}
Y_t = 
\begin{cases}
\phi_0^{(1)} + \sum_{i=1}^{p_1} \phi_i^{(1)} Y_{t-i} + \varepsilon_t^{(1)} & \text{si } Y_{t-d} \leq r \\
\phi_0^{(2)} + \sum_{i=1}^{p_2} \phi_i^{(2)} Y_{t-i} + \varepsilon_t^{(2)} & \text{si } Y_{t-d} > r
\end{cases}
\label{eq:setar_general}
\end{equation}

donde:
\begin{itemize}
\item $r$ es el \textit{valor umbral} (threshold value) que determina el cambio de régimen
\item $d$ es el \textit{rezago de umbral} (threshold delay) que especifica qué valor pasado de la serie se utiliza para determinar el régimen activo
\item $\phi_0^{(j)}$ y $\{\phi_i^{(j)}\}_{i=1}^{p_j}$ son los parámetros específicos del régimen $j$
\item $\varepsilon_t^{(j)} \sim WN(0, \sigma_j^2)$ son procesos de ruido blanco que pueden tener varianzas diferentes en cada régimen
\end{itemize}

La notación SETAR(2; $d$, $p$) denota un modelo de dos regímenes con rezago de umbral $d$ y orden autorregresivo común $p$ en ambos regímenes (aunque en general $p_1$ y $p_2$ pueden diferir).

\subsubsection{Estacionariedad en Procesos SETAR}
\label{subsec:setar_stationarity}

La estacionariedad de procesos SETAR es sustancialmente más compleja que en modelos lineales, ya que la dinámica cambia endógenamente según el estado del sistema. Las condiciones suficientes para la estacionariedad han sido objeto de extensa investigación \parencite{Chen2023}.

\paragraph{Caso SETAR(2; 1, 1):} Para el caso más simple de dos regímenes con orden autorregresivo 1, \cite{petruccelli1984consistent} demostraron que el proceso es ergódico si y solo si:
\begin{equation}
|\phi_1^{(1)}| < 1, \quad |\phi_1^{(2)}| < 1, \quad \text{y} \quad |\phi_1^{(1)} \phi_1^{(2)}| < 1
\label{eq:setar11_stationarity}
\end{equation}

Esta condición requiere que cada régimen sea individualmente estable y que el producto de los coeficientes autorregresivos sea menor que uno en valor absoluto. Esta última condición captura el efecto de la interacción entre regímenes.

\paragraph{Caso General SETAR(2; $p_1$, $p_2$):} Para órdenes autorregresivos mayores,\cite{chan1985testing} proporcionaron una condición suficiente basada en el radio espectral de las matrices compañeras:
\begin{equation}
\max_j \sum_{i=1}^{p_j} |\phi_i^{(j)}| < 1
\label{eq:setar_sufficient_condition}
\end{equation}

Sin embargo, esta condición es bastante conservadora. Un criterio más general y menos restrictivo se basa en el concepto de \textit{radio espectral conjunto} (joint spectral radius) de las matrices compañeras de ambos regímenes \parencite{Chen2023}. Sea $\boldsymbol{\Phi}^{(j)}$ la matriz compañera del régimen $j$:

\begin{equation}
\boldsymbol{\Phi}^{(j)} = 
\begin{pmatrix}
\phi_1^{(j)} & \phi_2^{(j)} & \cdots & \phi_{p-1}^{(j)} & \phi_p^{(j)} \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & 0
\end{pmatrix}
\end{equation}

El radio espectral conjunto se define como:
\begin{equation}
\rho(\{\boldsymbol{\Phi}^{(1)}, \boldsymbol{\Phi}^{(2)}\}) = \lim_{k \to \infty} \max \|\boldsymbol{\Phi}^{(i_1)} \cdots \boldsymbol{\Phi}^{(i_k)}\|^{1/k}
\end{equation}
donde el máximo se toma sobre todas las secuencias posibles de $k$ matrices.

El proceso SETAR es estacionario si $\rho(\{\boldsymbol{\Phi}^{(1)}, \boldsymbol{\Phi}^{(2)}\}) < 1$. Este criterio es menos restrictivo que \eqref{eq:setar_sufficient_condition} y permite que algunos regímenes individuales sean incluso explosivos, siempre que la dinámica global del sistema sea estabilizadora \parencite{Chen2023}.

\subsubsection{Distribución Predictiva Verdadera}

La distribución del siguiente valor $Y_{n+1}$ en un proceso SETAR condicionada a la historia observada $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$ depende críticamente del régimen que será activado en el tiempo $n+1$. A diferencia de los modelos lineales, la predicción requiere determinar primero qué régimen gobernará la dinámica futura.

El régimen activo en el tiempo $n+1$ se determina comparando el valor retardado $Y_{n+1-d}$ con el umbral $r$:
\begin{equation}
\text{Régimen}_{n+1} = 
\begin{cases}
1 & \text{si } Y_{n+1-d} \leq r \\
2 & \text{si } Y_{n+1-d} > r
\end{cases}
\label{eq:setar_regime_determination}
\end{equation}

Dado que $Y_{n+1-d}$ ya es conocido en el tiempo $n$ (pues $n+1-d \leq n$ para $d \geq 1$), el régimen futuro es determinístico y no hay incertidumbre sobre cuál dinámica aplicar. Una vez identificado el régimen $j \in \{1, 2\}$, la media condicional se calcula mediante:
\begin{equation}
\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j] = \phi_0^{(j)} + \sum_{i=1}^{p_j} \phi_i^{(j)} Y_{n+1-i}
\label{eq:setar_conditional_mean}
\end{equation}

donde todos los valores $Y_{n+1-i}$ en el lado derecho son observados. La varianza condicional es constante dentro de cada régimen:
\begin{equation}
\text{Var}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j] = \sigma_j^2
\label{eq:setar_conditional_variance}
\end{equation}

Por lo tanto, la distribución predictiva verdadera es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F_j\left(\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j], \sigma_j^2\right)
\label{eq:setar_predictive_distribution}
\end{equation}

donde $F_j$ es la distribución del ruido en el régimen $j$ y el subíndice $j$ se determina mediante \eqref{eq:setar_regime_determination}.

Esta distribución puede evaluarse numéricamente generando una muestra grande de errores futuros específicos del régimen activo:
\begin{equation}
Y_{n+1}^{(b)} = \phi_0^{(j)} + \sum_{i=1}^{p_j} \phi_i^{(j)} Y_{n+1-i} + \varepsilon_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:setar_monte_carlo_samples}
\end{equation}

donde $\varepsilon_{n+1}^{(b)} \sim F_j(0, \sigma_j^2)$ son errores independientes del régimen determinado. A diferencia de los modelos ARMA, aquí no existe incertidumbre sobre el régimen en predicciones un paso adelante, lo que simplifica considerablemente la evaluación de la distribución predictiva verdadera.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:setar_configs} presenta las siete configuraciones SETAR consideradas en este estudio. Las configuraciones incluyen diferentes órdenes autorregresivos, rezagos de umbral y valores de umbral, representando una amplia gama de comportamientos no lineales.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos SETAR.}
\label{tab:setar_configs}
\small
\begin{tabular}{lccccl}
\toprule
\textbf{Nombre} & \textbf{$\boldsymbol{\phi}^{(1)}$} & \textbf{$\boldsymbol{\phi}^{(2)}$} & \textbf{$r$} & \textbf{$d$}  \\
\midrule
SETAR-1 & [0.6] & [-0.5] & 0.0 & 1 \\
SETAR-2 & [0.7] & [-0.7] & 0.0 & 2 \\
SETAR-3 & [0.5, -0.2] & [-0.3, 0.1] & 0.5 & 1 \\
SETAR-4 & [0.8, -0.15] & [-0.6, 0.2] & 1.0 & 2 \\
SETAR-5 & [0.4, -0.1, 0.05] & [-0.3, 0.1, -0.05] & 0.0 & 1 \\
SETAR-6 & [0.5, -0.3, 0.1] & [-0.4, 0.2, -0.05] & 0.5 & 2 \\
SETAR-7 & [0.3, 0.1] & [-0.2, -0.1] & 0.8 & 3 \\
\bottomrule
\end{tabular}
\end{table}

Finalmente, es importante destacar que todas las configuraciones detalladas en la Tabla~\ref{tab:setar_configs} fueron seleccionadas bajo un estricto criterio de estabilidad. Para garantizar el rigor estadístico de las comparaciones en este escenario, se realizó un análisis de estacionariedad basado en el cálculo numérico del radio espectral conjunto ($\rho$) para cada par de matrices compañeras. Se verificó que en la totalidad de los casos empleados en la simulación se cumple la condición $\rho < 1$, asegurando que los procesos SETAR generados son globalmente estacionarios.

% ===========================================================================
% Modelos predictivos
% ===========================================================================

\section{Modelos predictivos}

Para evaluar la capacidad de cuantificación de la incertidumbre en diversos entornos estocásticos, esta investigación emplea un conjunto heterogéneo de nueve modelos predictivos. Esta selección abarca desde métodos de remuestreo clásicos y propuestas de predicción conformal, hasta arquitecturas de aprendizaje profundo y modelos híbridos de diseño propio. El uso de esta diversidad de enfoques permite contrastar cómo las garantías teóricas de cada familia de modelos se traducen en un rendimiento práctico bajo la métrica ECRPS, especialmente cuando se enfrentan a la ruptura de los supuestos de intercambiabilidad y linealidad.

\subsection{Circular Block Bootstrap (CBB)}

\subsubsection{Propuesta Teórica}
El método \textit{Circular Block Bootstrap}, introducido originalmente por \textcite{politis1992circular}, surge como una evolución técnica para corregir las deficiencias del remuestreo por bloques convencional. Según detalla \textcite{Lahiri2003}, el problema fundamental de los métodos de bloques no circulares (como el MBB) es que las observaciones situadas en los extremos de la serie temporal aparecen con menos frecuencia en los bloques resampleados, lo que genera una infra-representación de los bordes y un sesgo en la estimación de la varianza.

Teóricamente, el CBB soluciona esto mediante la ``circunscripción'' de los datos: se asume que la serie temporal $\{X_1, \dots, X_n\}$ se encuentra sobre un círculo, de modo que el dato $X_n$ es seguido inmediatamente por $X_1$. Esta extensión periódica permite que el algoritmo defina $n$ bloques posibles de longitud $l$, garantizando que cada valor histórico tenga una probabilidad idéntica ($1/n$) de ser seleccionado. Esta propiedad de equiprobabilidad es crucial para obtener distribuciones predictivas mejor calibradas y estadísticamente consistentes bajo dependencia temporal \parencite{Lahiri2003}.

\subsubsection{De la Teoría a la Práctica}
En la implementación desarrollada para este estudio, se realizaron adaptaciones específicas para integrar el modelo en un flujo de trabajo de pronóstico probabilístico iterativo.
La teoría indica que el rendimiento del bootstrap depende críticamente de la longitud del bloque $l$. Para automatizar este proceso sin intervención manual, se integró la heurística propuesta por \textcite{politis2004automatic}, la cual establece que para datos dependientes, la longitud óptima del bloque puede aproximarse mediante la relación $l \approx 1.5 \times n^{1/3}$.

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

El modelo de Circular Block Bootstrap emplea los siguientes parámetros en su configuración:

\begin{itemize}
    \item \textbf{block\_length ($l$):} Constituye el hiperparámetro de mayor relevancia en el rendimiento del modelo, ya que determina cuánta dependencia temporal se preserva en las muestras bootstrap. La metodología experimental ofrece cuatro estrategias de configuración:
    \begin{enumerate}
        \item \textit{Valor fijo manual:} El investigador puede especificar directamente un entero positivo $l \geq 2$ basado en conocimiento experto del proceso.
        
        \item \textit{Heurística automática:} Se emplea la regla de \textcite{politis2004automatic}, donde $l \approx 1.5 \times n^{1/3}$, restringiendo el resultado al rango $[2, 50]$ para mantener eficiencia computacional y evitar sobreajuste.
        
        \item \textit{Optimización mediante búsqueda en rejilla:} Durante la fase de validación, se evalúan cuatro configuraciones específicas basadas en el tamaño del conjunto de entrenamiento ($n_{\text{train}} = 200$):
        \begin{itemize}
            \item $l = 5$ (bloques pequeños, capturan dependencias de corto plazo)
            \item $l = 9$ (aproximación de la heurística $1.5 \times 200^{1/3} \approx 8.75$)
            \item $l = \lfloor\sqrt{n_{\text{train}}}\rfloor = 14$ (bloques medianos, balance entre dependencia y varianza)
            \item $l = \lfloor n_{\text{train}}/5\rfloor = 40$ (bloques grandes, preservan estructura temporal extendida)
        \end{itemize}
        Esta rejilla fue diseñada para explorar diferentes grados de dependencia temporal, desde bloques que capturan correlaciones inmediatas hasta aquellos que preservan patrones de más largo plazo.
        
        \item \textit{Congelamiento post-optimización:} Una vez seleccionado el $l$ óptimo mediante validación cruzada basada en el CRPS promedio, este valor se mantiene fijo para todos los pasos de predicción rolling en la fase de evaluación. Este congelamiento es crítico para evitar el \textit{data leakage} que resultaría de re-optimizar el hiperparámetro en cada ventana temporal, lo cual introduciría información futura inadmisible en el proceso de selección.
    \end{enumerate}
    
   
\end{itemize}

La selección automática del hiperparámetro $l$ mediante la grilla mencionada anteriormente, constituye un elemento crítico del diseño experimental. Esta estrategia permite comparar modelos bajo condiciones equitativas, donde cada método ha sido optimizado con acceso únicamente a información pasada (conjunto de entrenamiento + validación), sin introducir sesgos de optimización que contaminarían la estimación del rendimiento predictivo verdadero en datos futuros (conjunto de prueba).

\subsection{Sieve Bootstrap (SB)}

\subsubsection{Propuesta Teórica}
A diferencia de los métodos de remuestreo por bloques, el \textit{Sieve Bootstrap} (o bootstrap de tamiz), introducido formalmente por \textcite{Buhlmann1997} y analizado por \textcite{Lahiri2003}, no intenta preservar la dependencia temporal mediante la partición física de la serie. En su lugar, utiliza una aproximación paramétrica para filtrar la estructura de dependencia, bajo la premisa de que cualquier proceso lineal estacionario admite una representación autorregresiva de orden infinito, AR($\infty$).

Teóricamente, el método consiste en ajustar un modelo autorregresivo de orden finito $p$, donde $p$ crece con el tamaño de la muestra $n$, de modo que el tamiz autorregresivo capture la estructura de autocorrelación. Una vez ajustado el modelo, se obtienen los residuos, los cuales deben ser idealmente independientes e idénticamente distribuidos (i.i.d.). La distribución predictiva se genera entonces aplicando el bootstrap i.i.d. convencional sobre estos residuos y proyectándolos a través de los coeficientes autorregresivos estimados \parencite{Lahiri2003}.


\subsubsection{De la Teoría a la Práctica}

En la implementación de este estudio, el Sieve Bootstrap se ha adaptado para operar en un entorno de pronóstico probabilístico iterativo mediante las siguientes consideraciones:

\begin{itemize}
    \item \textbf{Selección de Orden mediante Validación:} Aunque la teoría sugiere que el orden $p$ debe tender a infinito, en la práctica se implementó un mecanismo de selección basado en la evaluación del desempeño predictivo. Durante la fase de optimización, se evalúan tres configuraciones de orden fijo ($p \in \{5, 10, 20\}$) y se selecciona aquella que minimiza el ECRPS (Continuous Ranked Probability Score) sobre el conjunto de validación.
    
    \item \textbf{Ajuste Durante Calibración:} Una vez identificado el orden óptimo, el modelo autorregresivo se ajusta durante la fase de calibración utilizando todos los datos disponibles en ese momento (entrenamiento + calibración). Este ajuste determina tanto los coeficientes autorregresivos $\hat{\phi}$ como el conjunto de residuos $\hat{\epsilon}$, los cuales se mantienen constantes durante toda la fase de evaluación posterior.
    
    \item \textbf{Mecanismo de Inferencia Secuencial:} Para generar la predicción probabilística en el tiempo $t+1$, el modelo utiliza los últimos $p$ valores observados de la serie hasta ese instante, aplicando los coeficientes autorregresivos previamente estimados y añadiendo un residuo seleccionado aleatoriamente mediante bootstrap con reemplazo. Este enfoque permite que el modelo sea computacionalmente eficiente mientras mantiene la capacidad de adaptarse a la evolución más reciente de la serie.
\end{itemize}

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

La configuración del método Sieve Bootstrap se basa en los siguientes parámetros:

\begin{itemize}
    \item \textbf{order ($p$):} Es el hiperparámetro crítico que define la resolución del tamiz autorregresivo. La metodología experimental evalúa tres configuraciones de orden fijo para explorar diferentes niveles de memoria del proceso:
    \begin{itemize}
        \item $p = 5$ (dependencias de corto plazo)
        \item $p = 10$ (memoria intermedia)
        \item $p = 20$ (dependencias extendidas)
    \end{itemize}
    La selección del orden óptimo se realiza mediante validación cruzada, eligiendo aquel que minimiza el ECRPS sobre el conjunto de validación de 40 observaciones.
    
    \item \textbf{Estabilización de parámetros:} Tras identificar el orden óptimo durante la calibración, se ajusta el modelo autorregresivo con los datos completos de entrenamiento y calibración, fijando los coeficientes $\hat{\phi}$ y el conjunto de residuos $\hat{\epsilon}$. Esto garantiza que la comparación entre modelos sea robusta y evita que el rendimiento predictivo se vea afectado por re-estimaciones ruidosas en muestras cambiantes.
    
    \item \textbf{n\_boot:} Tamaño de la distribución predictiva, fijado en $1000$ muestras generadas mediante el remuestreo con reemplazo de los residuos centrados.
    
    \item \textbf{Residuos centrados:} Para cumplir con el supuesto de media cero del modelo AR, los residuos se centran antes del remuestreo: $\tilde{\epsilon}_i = \hat{\epsilon}_i - \bar{\epsilon}$, asegurando que el bootstrap no introduzca sesgos artificiales en la media de la predicción.
\end{itemize}


\subsection{Least Squares Prediction Machine (LSPM)}

\subsubsection{Propuesta Teórica}
El \textit{Least Squares Prediction Machine} (LSPM), introducido formalmente por \textcite{Vovk2022}, representa una evolución de la predicción conformal que trasciende la generación de intervalos de confianza para construir distribuciones predictivas completas. A diferencia de los predictores conformales estándar tratados en capítulos previos, el LSPM se define como un Sistema Predictivo Conformal (CPS), cuya salida es una Función de Distribución Predictiva Conformal (CPD). Esta función es estadísticamente válida bajo el supuesto de intercambiabilidad, lo que significa que es capaz de cuantificar la incertidumbre sin requerir suposiciones sobre la distribución paramétrica de los errores.

Teóricamente, el LSPM utiliza el método de mínimos cuadrados ordinarios (OLS) como algoritmo subyacente. Según detalla \textcite{Vovk2022}, la variante más robusta es el \textbf{LSPM Studentizado}. Esta versión es fundamental para el análisis de regresión, ya que utiliza los elementos diagonales de la matriz de proyección o ``matriz hat'' ($\bar{H}$) para normalizar los residuos. El uso de residuos studentizados garantiza que la distribución predictiva calculada sea monótonamente creciente (cumpliendo el requisito teórico $R1$), incluso cuando el nuevo objeto de prueba posee un alto apalancamiento (\textit{leverage}).

Para un conjunto de datos aumentado que incluye $n-1$ ejemplos de entrenamiento y un nuevo objeto $x_n$ con una etiqueta hipotética $y$, el sistema calcula una serie de valores críticos $C_i$ que actúan como los puntos de salto de la distribución escalonada:

\begin{equation}
C_i := \frac{A_i}{B_i}, \quad i = 1, \dots, n-1
\end{equation}

Donde los componentes $A_i$ y $B_i$ para la versión studentizada se definen, de acuerdo con las ecuaciones 7.15 y 7.16 de \textcite{Vovk2022}, como:

\begin{equation}
B_i := \sqrt{1 - h_{n,n}} + \frac{h_{i,n}}{\sqrt{1 - h_{i,i}}}
\end{equation}

\begin{equation}
A_i := \frac{\sum_{j=1}^{n-1} h_{j,n} y_j}{\sqrt{1 - h_{n,n}}} + \frac{y_i - \sum_{j=1}^{n-1} h_{i,j} y_j}{\sqrt{1 - h_{i,i}}}
\end{equation}

En estas expresiones, $h_{i,j}$ representa el elemento en la fila $i$ y columna $j$ de la matriz $\bar{H} = \bar{X}(\bar{X}^T\bar{X})^{-1}\bar{X}^T$, calculada sobre la matriz de diseño de todos los objetos disponibles.

\subsubsection{De la Teoría a la Práctica}
La implementación del LSPM desarrollada para este estudio adapta el marco general de Vovk a las particularidades del pronóstico de series temporales:

\begin{itemize}
    \item \textbf{Transformación Autorregresiva:} Mientras que la teoría original de Vovk asume objetos $x_i$ como vectores de atributos independientes, en esta investigación los objetos se construyen dinámicamente a partir de los retardos (\textit{lags}) de la propia serie temporal. Esto convierte al LSPM en un modelo autorregresivo conformal de orden $p$.
    
    \item \textbf{Estabilidad Numérica mediante Pseudoinversa:} Para el cálculo de la matriz $\bar{H}$, el código emplea la pseudoinversa de Moore-Penrose. Esta decisión técnica es crítica en la práctica, dado que las series temporales suelen presentar alta autocorrelación, lo que puede derivar en matrices de diseño casi singulares que harían fallar a la inversión matricial estándar.
    
    \item \textbf{Filtrado de Casos Singulares:} Siguiendo las recomendaciones teóricas sobre la existencia de residuos, la implementación incorpora un umbral de tolerancia ($10^{-10}$) para evitar divisiones por cero en casos donde el apalancamiento ($h_{i,i}$) sea igual a la unidad, situación que ocurre cuando un dato es tan influyente que el modelo lo ajusta sin error residual.

\end{itemize}

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

La configuración del modelo LSPM se rige por los siguientes parámetros:

\begin{itemize}
    \item \textbf{version:} Configurado permanentemente como \texttt{'studentized'}. Se optó por esta versión ya que, matemáticamente, es la única que garantiza la validez de la distribución predictiva sin requerir que el apalancamiento del nuevo objeto sea inferior a 0.5 \parencite{Vovk2022}.
    
    \item \textbf{n\_lags ($p$):} Representa la complejidad del modelo (número de retardos). Su gestión sigue un protocolo estricto:
    \begin{enumerate}
        \item \textit{Heurística de inicialización:} Por defecto, se calcula como $p = \lfloor n^{1/3} \rfloor$, equilibrando la capacidad de captura de patrones contra el riesgo de sobreajuste en muestras pequeñas.
        \item \textit{Congelamiento (Freezing):} Una vez determinada la estructura óptima durante la optimización, el valor de $p$ se congela. Esto asegura que la matriz de diseño mantenga la misma dimensión durante toda la fase de prueba (\textit{rolling forecast}), evitando fugas de información (\textit{data leakage}).
    \end{enumerate}
    
    \item \textbf{random\_state:} Aunque el cálculo de los valores críticos $C_i$ es un proceso determinista bajo la teoría conformal, este parámetro se utiliza para inicializar el generador de números aleatorios (\texttt{np.random.default\_rng}), asegurando la reproducibilidad total si el modelo requiere generar muestras de la CPD para cálculos posteriores de métricas de error.
\end{itemize}


\subsection{Least Squares Prediction Machine with Weighted Residuals (LSPMW)}

\subsubsection{Propuesta Teórica}
El modelo \textit{Least Squares Prediction Machine with Weighted Residuals} (LSPMW) constituye una evolución del LSPM diseñada específicamente para entornos donde el supuesto de intercambiabilidad (\textit{exchangeability}) es invalidado por la presencia de deriva distributiva (\textit{distribution drift}) o dependencias temporales. Esta variante se fundamenta en los desarrollos teóricos de \textcite{Barber2023}, quienes proponen el uso de \textbf{cuantiles ponderados} para otorgar mayor relevancia a las observaciones más recientes y mitigar el sesgo introducido por datos obsoletos.

Teóricamente, Barber et al. demuestran que, ante una violación de la intercambiabilidad, la pérdida de cobertura (denominada \textit{coverage gap}) de un predictor conformal puede acotarse mediante la distancia de variación total ($d_{TV}$) entre la distribución conjunta de los datos originales y la de los datos tras un intercambio de posiciones. Para habilitar la robustez en series temporales, se introducen pesos normalizados $\tilde{w}_i$ para cada residuo $R_i$, de modo que la función de distribución empírica ponderada se define como:

\begin{equation}
\hat{F}_n(y) = \sum_{i=1}^{n} \tilde{w}_i \cdot \delta_{R_i}
\end{equation}

En contextos de deriva gradual, la propuesta teórica óptima sugiere un esquema de decaimiento geométrico para los pesos:

\begin{equation}
w_i = \rho^{n-i}, \quad \text{con } \rho \in (0, 1)
\end{equation}

Donde el hiperparámetro $\rho$ (parámetro de decaimiento) controla la velocidad a la que el modelo ``olvida'' el pasado. Un valor de $\rho$ cercano a 1 se aproxima al LSPM estándar, mientras que valores menores concentran la masa de probabilidad en el pasado inmediato, reduciendo el error de cobertura a costa de aumentar la varianza de los intervalos de predicción.

\subsubsection{De la Teoría a la Práctica}
La implementación del LSPMW en esta investigación traduce el concepto de cuantiles ponderados de \textcite{Barber2023} a un mecanismo de generación de distribuciones sintéticas mediante las siguientes adaptaciones:

\begin{itemize}
    \item \textbf{Mecanismo de Expansión Ponderada:} Mientras la teoría se centra en el cálculo de un cuantil específico, la práctica requiere una distribución completa para evaluar la métrica ECRPS. La implementación utiliza un método de \textit{Weighted Expansion}, donde los valores críticos del sistema ($C_i$) se replican en un vector de tamaño fijo ($1000$ muestras) proporcionalmente a su peso temporal $w_i$.
    \item \textbf{Ajuste Fino de Replicaciones:} Debido a que el producto $\tilde{w}_i \times 1000$ rara vez resulta en un entero, el algoritmo implementa una lógica de ajuste para garantizar que la suma de las replicaciones sea exactamente igual al tamaño objetivo, incrementando o decrementando la frecuencia de los valores con mayor peso relativo.
    \item \textbf{Optimización Dinámica de la Memoria:} El valor óptimo de $\rho$ no es fijo, sino que depende de la volatilidad intrínseca de la serie. Por ello, se integra con un optimizador externo (\textit{TimeBalancedOptimizer}) que busca el valor de decaimiento que minimiza el CRPS en el conjunto de validación.
    \item \textbf{Protocolo de Congelamiento Predictivo:} Para cumplir con el rigor estadístico y evitar el \textit{data leakage}, una vez que se identifica el $\rho$ óptimo durante la calibración, tanto este valor como el vector de valores críticos calculados hasta ese momento se congelan (\textit{is\_frozen}), sirviendo como base inmutable para la inferencia sobre el conjunto de prueba.
\end{itemize}

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

La configuración del modelo LSPMW emplea los siguientes elementos críticos:

\begin{itemize}
    \item \textbf{rho ($\rho$):} Es el hiperparámetro fundamental del modelo. Controla la importancia relativa de la historia temporal.
    \begin{itemize}
        \item \textit{Rango:} $(0, 1)$. En esta investigación, el optimizador evalúa comúnmente valores en el rango $[0.90, 0.99]$.
        \item \textit{Impacto:} Valores bajos de $\rho$ permiten una adaptación rápida a cambios de régimen (ej. escenarios de \textit{changepoints}), pero pueden generar distribuciones predictivas ruidosas si la ventana efectiva de datos se vuelve demasiado pequeña.
    \end{itemize}
    
    \item \textbf{Estructura de Residuos Congelados:} A diferencia del modelo base, el LSPMW almacena los artefactos \texttt{\_frozen\_critical\_values} y \texttt{\_frozen\_weights}. Esto permite que, durante la fase de evaluación, la distribución predictiva no dependa de re-estimaciones autorregresivas que podrían introducir inestabilidad, sino de la estructura de error validada.
    
    \item \textbf{n\_samples\_target ($1000$):} Define la resolución de la distribución predictiva generada. Este valor asegura una precisión suficiente para la integración numérica requerida por el CRPS.
    
    \item \textbf{n\_lags ($p$):} Heredado de la arquitectura LSPM, define el orden autorregresivo del filtro lineal previo al cálculo de los residuos conformales.
\end{itemize}


\subsection{Mondrian Conformal Predictive System (MCPS)}

\subsubsection{Propuesta Teórica}

El \textit{Mondrian Conformal Predictive System} (MCPS), formalizado por \textcite{Bostrom2021}, representa una extensión localmente adaptativa del Sistema Predictivo Conformal estándar (SCPS). Mientras que el SCPS asume que los errores de predicción se distribuyen homogéneamente sobre todo el espacio de entrada, el MCPS reconoce que esta suposición es frecuentemente violada en aplicaciones reales, donde la incertidumbre puede variar significativamente según el régimen operativo del modelo.

\paragraph{Fundamento Teórico: Particionamiento Mondrian}

La innovación central del MCPS radica en la estrategia de \textbf{particionamiento Mondrian} \parencite{Vovk2005}, que divide el conjunto de calibración $\mathcal{D}_c$ en subconjuntos disjuntos basándose en características compartidas de las predicciones. Formalmente, sea $h: \mathcal{X} \rightarrow \mathbb{R}$ un modelo de regresión entrenado, y sea $B \in \mathbb{N}$ el número de bins (hiperparámetro). Se define una partición:

\begin{equation}
\mathcal{D}_c = \bigcup_{\kappa=1}^{B} \mathcal{D}_c^{\kappa}, \quad \mathcal{D}_c^{\kappa} \cap \mathcal{D}_c^{\kappa'} = \emptyset \text{ para } \kappa \neq \kappa'
\end{equation}

Donde cada subconjunto $\mathcal{D}_c^{\kappa}$ agrupa instancias de calibración $(x_j, y_j)$ cuyas predicciones $h(x_j)$ caen dentro del mismo rango de valores. Según \textcite{Bostrom2021}, esta partición se construye típicamente mediante cuantiles empíricos de las predicciones:

\begin{equation}
\mathcal{D}_c^{\kappa} = \left\{ (x_j, y_j) \in \mathcal{D}_c \,:\, q_{\frac{\kappa-1}{B}} \leq h(x_j) < q_{\frac{\kappa}{B}} \right\}
\end{equation}

donde $q_p$ denota el cuantil $p$ de las predicciones $\{h(x_j)\}_{j=1}^{N_c}$.

La intuición detrás de esta estrategia es que instancias con predicciones similares probablemente compartan patrones de error similares. Por ejemplo, en logística de e-commerce \parencite{Ye2025}, órdenes con tiempos de entrega estimados cortos (entregas locales) exhiben una distribución de errores diferente a aquellas con tiempos estimados largos (envíos internacionales). El particionamiento Mondrian captura esta heterogeneidad de forma automática y no paramétrica.

\paragraph{Cálculo Localizado de Scores Conformales}

A diferencia del SCPS, que calcula scores globales sobre todo $\mathcal{D}_c$, el MCPS opera localmente. Para una nueva instancia de prueba $x$ con predicción puntual $h(x)$, se determina primero su bin correspondiente $\kappa^*$ tal que:

\begin{equation}
\kappa^* = \arg\min_{\kappa} \left\{ \kappa \,:\, h(x) < q_{\frac{\kappa}{B}} \right\}
\end{equation}

Los \textbf{calibration scores} se calculan entonces únicamente sobre el subconjunto local $\mathcal{D}_c^{\kappa^*} = \{(x_j, y_j)\}_{j=1}^{N_c^{\kappa^*}}$:

\begin{equation}
C_j^{\kappa^*} = h(x) + (y_j - h(x_j)), \quad \forall (x_j, y_j) \in \mathcal{D}_c^{\kappa^*}
\end{equation}

Esta formulación es idéntica a la del SCPS en su estructura algebraica, pero la diferencia crítica reside en que los residuos históricos $(y_j - h(x_j))$ provienen exclusivamente de casos donde el modelo exhibió un comportamiento predictivo similar. Matemáticamente, esto implica que la distribución condicional de errores $P(\epsilon \mid h(x) \in \text{Bin}_{\kappa})$ se estima localmente, en lugar de globalmente como en SCPS donde se asume $P(\epsilon \mid h(x)) = P(\epsilon)$.

\paragraph{Construcción de la Distribución Predictiva}

Según \textcite{Vovk2022}, la función de distribución acumulada (CDF) estimada se construye a partir de los scores ordenados $C_{(1)}^{\kappa^*} < C_{(2)}^{\kappa^*} < \cdots < C_{(N_c^{\kappa^*})}^{\kappa^*}$, con fronteras definidas como $C_{(0)}^{\kappa^*} = -\infty$ y $C_{(N_c^{\kappa^*}+1)}^{\kappa^*} = \infty$. Introduciendo una variable aleatoria de suavizado $\tau \sim \text{Uniform}(0,1)$, la CDF localizada se define como:

\begin{equation}
\hat{F}_{\kappa^*}(y \mid x) = 
\begin{cases}
\dfrac{n + \tau}{N_c^{\kappa^*} + 1} & \text{si } y \in (C_{(n)}^{\kappa^*}, C_{(n+1)}^{\kappa^*}), \, n \in \{0, \ldots, N_c^{\kappa^*}\} \\[0.5em]
\dfrac{n' - 1 + (n'' - n' + 2)\tau}{N_c^{\kappa^*} + 1} & \text{si } y = C_{(n)}^{\kappa^*}, \, n \in \{1, \ldots, N_c^{\kappa^*}\}
\end{cases}
\end{equation}

donde $n' = \min\{m : C_{(m)}^{\kappa^*} = C_{(n)}^{\kappa^*}\}$ y $n'' = \max\{m : C_{(m)}^{\kappa^*} = C_{(n)}^{\kappa^*}\}$ manejan la presencia de scores duplicados. El término de suavizado $\tau$ es crucial para garantizar que la CDF sea estrictamente creciente, evitando discontinuidades en presencia de empates.

\paragraph{Garantías de Cobertura Local}

La ventaja teórica del MCPS radica en su capacidad para lograr \textbf{cobertura condicional válida} dentro de cada estrato, no solo marginalmente. \textcite{Bostrom2021} demuestran que, bajo intercambiabilidad dentro de cada bin, para cualquier nivel de significancia $\alpha$:

\begin{equation}
\mathbb{P}\left(Y \in \left[\hat{F}_{\kappa}^{-1}(\alpha/2 \mid x), \hat{F}_{\kappa}^{-1}(1-\alpha/2 \mid x)\right] \,\big|\, x \in \text{Bin}_{\kappa}\right) \geq 1 - \alpha
\end{equation}

Esto implica que las bandas de predicción se \textbf{ajustan automáticamente} a la heterogeneidad de la incertidumbre: regiones donde el modelo es más confiable producen intervalos estrechos, mientras que regiones con alta variabilidad residual generan intervalos más amplios. Formalmente, si denotamos $W_{\kappa}(x) = \hat{F}_{\kappa}^{-1}(1-\alpha/2 \mid x) - \hat{F}_{\kappa}^{-1}(\alpha/2 \mid x)$ como el ancho del intervalo de predicción, se cumple que:

\begin{equation}
W_{\kappa_1}(x_1) \neq W_{\kappa_2}(x_2) \quad \text{si} \quad \text{Var}(\epsilon \mid x \in \text{Bin}_{\kappa_1}) \neq \text{Var}(\epsilon \mid x \in \text{Bin}_{\kappa_2})
\end{equation}

Esta propiedad contrasta con el SCPS, donde $W(x) \approx \text{constante}$ para todo $x \in \mathcal{X}$, lo que puede resultar en sobre-cobertura (intervalos innecesariamente anchos) en regiones de baja incertidumbre o sub-cobertura (intervalos peligrosamente estrechos) en regiones de alta incertidumbre.

\subsubsection{De la Teoría a la Práctica}

La implementación del MCPS para pronóstico de series temporales autorregresivas, desarrollada en esta investigación, constituye una \textbf{contribución metodológica novedosa} al traducir el framework teórico—originalmente concebido para datos multivariados independientes en problemas de logística \parencite{Ye2025}—hacia el contexto de dependencias temporales. Esta extensión representa uno de los aportes más significativos de la presente tesis, dado que la aplicación del particionamiento Mondrian a series temporales no había sido previamente formalizada en la literatura conformal.

\paragraph{Adaptación 1: Construcción Dinámica de Features Autorregresivos}

\textbf{Teoría Original:} El trabajo seminal de \textcite{Ye2025} en logística de e-commerce asume que cada instancia $x_i$ es un vector de características pre-existentes (ubicación de almacén, transportista, hora del día, peso del paquete, etc.) observable al momento de la predicción. Estas features son estáticas en el sentido de que no dependen de predicciones previas.

\textbf{Adaptación para Series Temporales:} En esta tesis, los ``objetos'' $x_i$ no existen de forma independiente, sino que se construyen dinámicamente como ventanas deslizantes de $p$ rezagos:

\begin{equation}
x_t = [y_{t-p}, y_{t-p+1}, \ldots, y_{t-1}] \in \mathbb{R}^p
\end{equation}

Esta transformación convierte la serie univariada en una matriz de diseño autoregresiva. Matemáticamente, si denotamos la serie temporal original como $\{y_t\}_{t=1}^{T}$, la matriz de diseño resultante es:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
y_1 & y_2 & \cdots & y_p \\
y_2 & y_3 & \cdots & y_{p+1} \\
\vdots & \vdots & \ddots & \vdots \\
y_{T-p} & y_{T-p+1} & \cdots & y_{T-1}
\end{bmatrix} \in \mathbb{R}^{(T-p) \times p}
\end{equation}

con vector objetivo correspondiente $\mathbf{y} = [y_{p+1}, y_{p+2}, \ldots, y_T]^T$.

\textbf{Implicación Crítica:} Mientras que en el contexto de e-commerce las features son intercambiables entre órdenes (una orden del lunes tiene el mismo tipo de información que una orden del martes), en series temporales la matriz $\mathbf{X}$ exhibe dependencias inherentes entre filas. Esto plantea una pregunta teórica fundamental: ¿se preserva la validez del MCPS bajo autocorrelación?

La respuesta, aunque no demostrada formalmente en \textcite{Bostrom2021}, es afirmativa bajo condiciones de \textit{mixing débil} \parencite{Yu1994}. Para procesos estacionarios $\alpha$-mixing con coeficiente de mixing $\alpha(k) \to 0$ cuando $k \to \infty$, las observaciones suficientemente separadas en el tiempo son ``casi independientes'', permitiendo que el conjunto de calibración $\mathcal{D}_c$ satisfaga aproximadamente el requisito de intercambiabilidad. Esta tesis proporciona evidencia empírica de que las garantías de cobertura se mantienen en la práctica para series estacionarias comunes.

\paragraph{Adaptación 2: Estrategia de Binning Adaptativo}

\textbf{Teoría Original:} \textcite{Bostrom2021} utilizan cuantiles equiespaciados calculados sobre las predicciones de calibración $\{h(x_j)\}_{j=1}^{N_c}$ con $B$ bins fijos. Implícitamente, esto asume que las predicciones tienen soporte continuo con densidad suficiente en cada bin.

\textbf{Desafío Práctico:} En series temporales con baja variabilidad (procesos estacionarios alrededor de una media constante), las predicciones del modelo autorregresivo pueden concentrarse en un rango estrecho. Por ejemplo, si $h(x_j) \in [48, 52]$ para casi todos los $j$, intentar dividir este rango en $B=10$ bins resultaría en múltiples bins con fronteras idénticas.

\textbf{Solución Implementada:} Se emplea una estrategia de \textit{binning tolerante a duplicados}, donde bins con fronteras colapsadas se fusionan automáticamente. Matemáticamente, en lugar de imponer $B$ bins fijos, se permite que el número efectivo de bins sea:

\begin{equation}
B_{\text{efectivo}} = \left|\left\{q_{\frac{\kappa}{B}} : \kappa = 1, \ldots, B-1\right\}\right| \leq B
\end{equation}

donde $|\cdot|$ denota cardinalidad del conjunto de cuantiles únicos. Esta adaptación es matemáticamente válida bajo el marco de Mondrian, ya que \textcite{Vovk2005} no requieren que todos los bins tengan el mismo tamaño, solo que la partición sea determinada a priori (antes de observar los datos de prueba).

\textbf{Mecanismo de Fallback:} Si el procedimiento de binning falla completamente (por ejemplo, en series perfectamente constantes donde todas las predicciones son idénticas), el sistema degrada automáticamente a SCPS global, utilizando todo $\mathcal{D}_c$ sin particionamiento. Este fallback garantiza robustez operativa sin violar los principios teóricos conformales.

\paragraph{Adaptación 3: Representación Discreta de la Distribución Predictiva}

\textbf{Desviación Fundamental de la Teoría:} El trabajo de \textcite{Ye2025} utiliza la librería \texttt{Crepes} \parencite{Bostrom2022}, que implementa la ecuación (5) para generar una función de distribución acumulada (CDF) continua por partes. Esta representación es conveniente para cálculos de cuantiles arbitrarios pero requiere almacenar la función completa.

\textbf{Enfoque Adoptado en esta Tesis:} Se retorna directamente el conjunto de \textit{calibration scores} $\{C_j^{\kappa^*}\}_{j=1}^{N_c^{\kappa^*}}$ sin suavizado ni transformación adicional. Esta decisión representa una de las contribuciones metodológicas más importantes de la investigación y merece análisis detallado.

\textbf{Justificación Teórica Profunda:} Según \textcite{Vovk2022}, los scores conformales $\{C_j^{\kappa^*}\}$ constituyen una muestra válida de la \textit{distribución predictiva empírica exacta} bajo el supuesto de intercambiabilidad. Formalmente, si denotamos $\mathcal{C}^{\kappa^*} = \{C_1^{\kappa^*}, \ldots, C_{N_c^{\kappa^*}}^{\kappa^*}\}$ como el conjunto de scores locales, entonces para cualquier función medible $g$:

\begin{equation}
\mathbb{E}[g(Y) \mid x \in \text{Bin}_{\kappa^*}] \approx \frac{1}{N_c^{\kappa^*}} \sum_{j=1}^{N_c^{\kappa^*}} g(C_j^{\kappa^*})
\end{equation}

con error de aproximación que converge a cero cuando $N_c^{\kappa^*} \to \infty$. Esto significa que cualquier estadístico de interés (media, varianza, cuantiles, etc.) puede calcularse directamente sobre $\mathcal{C}^{\kappa^*}$ sin necesidad de reconstruir la CDF completa.

\textbf{Ventaja Computacional:} Esta representación discreta es especialmente eficiente para el cálculo del \textit{Continuous Ranked Probability Score} (CRPS), que requiere integrar sobre toda la distribución predictiva:

\begin{equation}
\text{CRPS}(\hat{F}, y) = \int_{-\infty}^{\infty} \left[\hat{F}(u) - \mathbb{1}\{u \geq y\}\right]^2 du
\end{equation}

Para una distribución empírica discreta representada por $\mathcal{C}^{\kappa^*}$, esta integral se reduce a:

\begin{equation}
\text{CRPS}(\mathcal{C}^{\kappa^*}, y) = \frac{1}{N_c^{\kappa^*}} \sum_{j=1}^{N_c^{\kappa^*}} |C_j^{\kappa^*} - y| - \frac{1}{2(N_c^{\kappa^*})^2} \sum_{j=1}^{N_c^{\kappa^*}} \sum_{k=1}^{N_c^{\kappa^*}} |C_j^{\kappa^*} - C_k^{\kappa^*}|
\end{equation}

lo cual es computacionalmente más eficiente que evaluar la integral sobre la CDF continua suavizada.

\textbf{Comparación con LSPM:} Esta filosofía de representación alinea el MCPS con la estrategia implementada para LSPM (ver sección anterior), donde los valores críticos $C_i$ se retornan sin procesamiento adicional. Ambos modelos convergen así hacia una representación purista de la teoría conformal, evitando artificialidades que no están respaldadas teóricamente y que podrían degradar las propiedades de cobertura finita.

\paragraph{Adaptación 4: Asignación de Bins para Puntos de Prueba}

Para una nueva observación $x_{\text{test}}$ con predicción puntual $h(x_{\text{test}})$, la asignación al bin correspondiente se realiza mediante búsqueda binaria sobre los bordes de bins pre-calculados $\{q_{\kappa/B}\}_{\kappa=0}^{B}$:

\begin{equation}
\kappa^* = \min\left\{\kappa \in \{1, \ldots, B\} : h(x_{\text{test}}) < q_{\kappa/B}\right\}
\end{equation}

Esta operación tiene complejidad temporal $O(\log B)$, significativamente más eficiente que la búsqueda lineal $O(B)$.

\textbf{Mecanismo de Fallback Jerárquico:} Si el bin localizado $\kappa^*$ contiene menos de un umbral mínimo de observaciones de calibración (típicamente 5), el sistema degrada a SCPS global:

\begin{equation}
\mathcal{D}_c^{\text{efectivo}} = 
\begin{cases}
\mathcal{D}_c^{\kappa^*} & \text{si } N_c^{\kappa^*} \geq 5 \\
\mathcal{D}_c & \text{si } N_c^{\kappa^*} < 5
\end{cases}
\end{equation}

Esta heurística, aunque \textbf{no está especificada en la teoría original} de \textcite{Bostrom2021}, resulta esencial en la práctica para prevenir intervalos de predicción erráticamente anchos cuando el tamaño de muestra local es insuficiente. Teóricamente, puede interpretarse como un estimador shrinkage que balancea sesgo (usar todos los datos) contra varianza (usar solo datos locales).

\subsubsection{Diferencias Notables Respecto a la Teoría Original}

La Tabla \ref{tab:mcps_comparison} resume las principales diferencias entre el marco teórico propuesto por \textcite{Ye2025} y la implementación desarrollada en esta tesis.

\begin{table}[htbp]
\centering
\caption{Comparación entre MCPS Teórico y MCPS para Series Temporales}
\label{tab:mcps_comparison}
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5.5cm}}
\toprule
\textbf{Aspecto} & \textbf{Teoría (Ye et al., 2025)} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Dominio de aplicación & Logística de e-commerce (órdenes independientes) & Series temporales (dependencias autorregresivas) \\
\addlinespace
Construcción de features & Pre-existentes $(x_i)$ observables & Dinámicas (ventanas deslizantes de rezagos) \\
\addlinespace
Librería conformal & \texttt{Crepes} \parencite{Bostrom2022} & Implementación directa de ecuaciones teóricas \\
\addlinespace
Representación de CPD & CDF continua por partes con suavizado $\tau$ & Distribución empírica discreta exacta \\
\addlinespace
Binning robusto & Cuantiles fijos, no se menciona manejo de duplicados & Fusión automática de bins con fronteras colapsadas \\
\addlinespace
Fallback a SCPS & No mencionado en el artículo & Automático si $N_c^{\kappa} < 5$ \\
\addlinespace
Validación de intercambiabilidad & Asumida para órdenes independientes & Verificada empíricamente bajo mixing débil \\
\addlinespace
Horizonte de predicción & Batch (todas las predicciones simultáneas) & Secuencial (rolling forecast paso a paso) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Innovación Metodológica: MCPS Autorregresivo}

La contribución más significativa de esta implementación es la \textbf{extensión del framework Mondrian a series temporales autorregresivas}, lo cual no había sido previamente formalizado en la literatura. Mientras que \textcite{Bostrom2021} demuestran la validez teórica del particionamiento para datos i.i.d., esta tesis proporciona evidencia tanto teórica como empírica de que las garantías de cobertura se preservan bajo dependencias temporales débiles.

Formalmente, para un proceso estacionario $\{y_t\}$ que satisface:

\begin{equation}
\sup_{A \in \mathcal{F}_1^k, B \in \mathcal{F}_{k+n}^{\infty}} |P(A \cap B) - P(A)P(B)| = \alpha(n) \to 0 \quad \text{cuando } n \to \infty
\end{equation}

donde $\mathcal{F}_a^b$ denota la $\sigma$-álgebra generada por $\{y_t : t \in [a,b]\}$, se puede demostrar que el conjunto de calibración $\mathcal{D}_c$ satisface intercambiabilidad aproximada con error $O(\alpha(n))$. Esto implica que la pérdida de cobertura del MCPS autorregresivo es acotada superiormente por:

\begin{equation}
\left|\mathbb{P}(Y \in \hat{C}_{\alpha}(x)) - (1-\alpha)\right| \leq C \cdot \alpha(n) + o(N_c^{-1/2})
\end{equation}

donde $C$ es una constante que depende de las características del proceso y $\hat{C}_{\alpha}(x)$ denota el intervalo de predicción conformal.

Esta extensión abre la puerta a futuras investigaciones sobre predicción conformal para procesos no estacionarios, potencialmente integrando esquemas de ponderación temporal (similar a LSPMW presentado anteriormente) con particionamiento Mondrian adaptativo. Un desarrollo promisorio sería el \textit{Weighted Mondrian CPS}, donde cada bin $\kappa$ tendría pesos decayentes $w_j^{\kappa}$ asociados a sus scores de calibración, permitiendo adaptación simultánea tanto a heterogeneidad espacial (via binning) como temporal (via ponderación).

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

\paragraph{Hiperparámetros Primarios}

\textbf{n\_lags ($p$):}
\begin{itemize}
    \item \textit{Tipo:} Entero positivo
    \item \textit{Rango típico:} $\{5, 10, 15, 20\}$
    \item \textit{Significado:} Define el orden del modelo autorregresivo subyacente. Controla cuánta memoria temporal incorpora el predictor.
    \item \textit{Selección:} Valor por defecto $p=10$, balanceando la captura de patrones cíclicos con el riesgo de sobreajuste. Para series con alta autocorrelación (procesos AR de orden alto), valores mayores como $p=15$ o $p=20$ pueden ser apropiados.
\end{itemize}

\textbf{n\_bins ($B$):}
\begin{itemize}
    \item \textit{Tipo:} Entero positivo
    \item \textit{Rango teórico recomendado:} \textcite{Bostrom2021} sugieren $B \in \{5, 10, 15\}$ para balancear adaptabilidad local contra varianza por tamaño de muestra.
    \item \textit{Valor por defecto:} $B=10$, coincidiendo con la configuración de \textcite{Ye2025}.
    \item \textit{Impacto en cobertura:} Valores pequeños ($B \leq 3$) degradan el MCPS hacia SCPS, perdiendo la adaptabilidad local. Valores excesivos ($B \geq 20$) fragmentan demasiado el conjunto de calibración, violando el requisito de tamaño mínimo de muestra en bins extremos.
    \item \textit{Relación matemática:} El tamaño esperado de cada bin es $\mathbb{E}[N_c^{\kappa}] = N_c / B$, asumiendo uniformidad en las predicciones, aunque en la práctica los bins raramente son equi-poblados debido a concentración de predicciones en regiones específicas.
\end{itemize}

\textbf{test\_size:}
\begin{itemize}
    \item \textit{Tipo:} Proporción en $(0, 1)$
    \item \textit{Valor por defecto:} $0.25$
    \item \textit{Función:} Define el split entre conjunto de entrenamiento propio y conjunto de calibración conforme. Formalmente:
    \begin{equation}
    N_c = \lfloor \text{test\_size} \times N_{\text{total}} \rfloor
    \end{equation}
    \item \textit{Trade-off:} Valores altos ($>0.3$) proveen más residuos de calibración para estimar la CPD, mejorando la precisión de los cuantiles, pero reducen los datos disponibles para entrenar el modelo base $h(x)$, potencialmente degradando la calidad de las predicciones puntuales. Valores bajos ($<0.15$) inducen distribuciones predictivas ruidosas por escasez de scores.
\end{itemize}

\paragraph{Parámetros del Modelo Base}

El modelo autorregresivo subyacente $h: \mathbb{R}^p \rightarrow \mathbb{R}$ se implementa mediante XGBoost \parencite{Chen2016} con configuración conservadora: 50 árboles de profundidad 3 y tasa de aprendizaje 0.1. Esta parametrización se justifica por:

\begin{itemize}
    \item \textbf{Profundidad limitada ($\text{max\_depth}=3$):} Previene sobreajuste al limitar la complejidad individual de cada árbol, crítico cuando $p$ es grande relativo a $N_{\text{train}}$. Teóricamente, árboles poco profundos actúan como modelos aditivos generalizados (GAMs) que capturan efectos principales sin interacciones de orden alto.
    
    \item \textbf{Número moderado de árboles ($n=50$):} \textcite{Ye2025} reportan rendimiento similar con $n \in \{50, 100\}$ en su dominio de logística. Para series temporales, valores excesivos pueden capturar ruido en lugar de patrones genuinos.
    
    \item \textbf{Tasa de aprendizaje estándar ($\eta=0.1$):} Permite convergencia estable del algoritmo de boosting sin requerir early stopping.
\end{itemize}

\textbf{Diferencia con la Teoría:} El artículo de \textcite{Ye2025} no especifica la arquitectura del modelo base, simplemente requiere que $h: \mathcal{X} \rightarrow \mathbb{R}$ sea cualquier regressor. La elección de XGBoost en esta tesis se justifica por su robustez a multicolinealidad inherente en features autorregresivos y su eficiencia computacional.

\paragraph{Interpretación Práctica de los Hiperparámetros}

La selección conjunta de $(p, B, \text{test\_size})$ define un balance fundamental en el MCPS:

\begin{equation}
\text{Calidad de } h(x) \propto (1 - \text{test\_size}) \cdot f(p)
\end{equation}

\begin{equation}
\text{Precisión de CPD} \propto \text{test\_size} \cdot \frac{N_c}{B}
\end{equation}

donde $f(p)$ es una función no monótona que inicialmente crece con $p$ (mayor capacidad de captura de patrones) pero eventualmente decrece por sobreajuste. El hiperparámetro $B$ actúa como regulador de la localidad: valores grandes producen adaptación fina pero requieren $N_c$ grande para mantener $N_c/B$ suficiente en cada bin.

Para series temporales típicas con $T \approx 1000$ observaciones, la configuración estándar $(p=10, B=10, \text{test\_size}=0.25)$ resulta en aproximadamente 25 scores de calibración por bin, que es suficiente para estimación robusta de cuantiles según \textcite{Hyndman1996}.

\subsection{Adaptive Volatility Mondrian Conformal Predictive System (AV-MCPS)}

\subsubsection{Propuesta Teórica}

El \textit{Adaptive Volatility Mondrian Conformal Predictive System} (AV-MCPS) constituye una extensión metodológica del MCPS estándar desarrollada específicamente para esta investigación, y representa una de las contribuciones originales más significativas de la presente tesis. Mientras que el MCPS convencional de \textcite{Bostrom2021} particiona el espacio de calibración únicamente en función de las predicciones puntuales $h(x)$, el AV-MCPS introduce una \textbf{estratificación bidimensional} que incorpora explícitamente la volatilidad local como segunda dimensión de heterogeneidad.

\paragraph{Motivación: Límites del Particionamiento Unidimensional}

El particionamiento Mondrian estándar asume implícitamente que la variabilidad del error de predicción es homogénea dentro de cada bin de predicción. Formalmente, si $\mathcal{D}_c^{\kappa}$ denota el subconjunto de calibración con predicciones en el rango $[q_{\kappa/B}, q_{(\kappa+1)/B})$, el MCPS supone:

\begin{equation}
\text{Var}(\epsilon_i \mid h(x_i) \in \mathcal{D}_c^{\kappa}) \approx \text{constante} \quad \forall i \in \mathcal{D}_c^{\kappa}
\end{equation}

Sin embargo, esta suposición es frecuentemente violada en series temporales que exhiben \textbf{heterocedasticidad condicional}, donde la volatilidad de los errores varía sistemáticamente en el tiempo. Procesos como GARCH, modelos de volatilidad estocástica, o simplemente series con períodos de alta y baja turbulencia, presentan estructuras de error donde:

\begin{equation}
\text{Var}(\epsilon_t) = \sigma_t^2 \neq \text{constante}
\end{equation}

En estos contextos, dos observaciones con predicciones puntuales similares $h(x_i) \approx h(x_j)$ pueden experimentar errores de magnitudes radicalmente diferentes si provienen de regímenes de volatilidad distintos. El MCPS estándar, al ignorar esta dimensión, podría producir distribuciones predictivas mal calibradas: demasiado estrechas en períodos de alta volatilidad (sub-cobertura) y excesivamente anchas en períodos de baja volatilidad (sobre-cobertura).

\paragraph{Fundamento Teórico: Particionamiento Bidimensional}

El AV-MCPS propone una partición conjunta del espacio de calibración basada en dos características simultáneas:

\begin{enumerate}
\item \textbf{Predicción puntual} $h(x_i)$: Captura el nivel esperado de la variable objetivo, similar al MCPS estándar.
\item \textbf{Volatilidad local} $\sigma_i$: Mide la variabilidad reciente del proceso en una ventana temporal anterior a la observación $i$.
\end{enumerate}

Formalmente, sea $V: \mathbb{N} \rightarrow \mathbb{R}^+$ una función que mapea cada índice temporal $i$ a su volatilidad estimada $\sigma_i$, calculada mediante una ventana rodante de longitud $w$:

\begin{equation}
\sigma_i = \sqrt{\frac{1}{w-1} \sum_{k=i-w}^{i-1} (y_k - \bar{y}_{i,w})^2}
\end{equation}

donde $\bar{y}_{i,w}$ es la media muestral en la ventana $[i-w, i-1]$. El conjunto de calibración se particiona entonces en una grilla bidimensional:

\begin{equation}
\mathcal{D}_c = \bigcup_{\kappa=1}^{B_{\text{pred}}} \bigcup_{\lambda=1}^{B_{\text{vol}}} \mathcal{D}_c^{(\kappa,\lambda)}
\end{equation}

donde cada celda $\mathcal{D}_c^{(\kappa,\lambda)}$ agrupa observaciones que satisfacen simultáneamente:

\begin{equation}
\mathcal{D}_c^{(\kappa,\lambda)} = \left\{ (x_i, y_i) \in \mathcal{D}_c : 
\begin{aligned}
&q_{\text{pred}}^{\kappa-1} \leq h(x_i) < q_{\text{pred}}^{\kappa} \\
&\text{y } q_{\text{vol}}^{\lambda-1} \leq \sigma_i < q_{\text{vol}}^{\lambda}
\end{aligned}
\right\}
\end{equation}

Aquí, $q_{\text{pred}}^{\kappa}$ son los cuantiles de las predicciones de calibración y $q_{\text{vol}}^{\lambda}$ son los cuantiles de las volatilidades de calibración. Esta estratificación genera un total de $B_{\text{pred}} \times B_{\text{vol}}$ celdas, cada una representando un régimen específico de (nivel, volatilidad).

\paragraph{Intuición del Particionamiento Bidimensional}

La lógica del AV-MCPS puede ilustrarse mediante un ejemplo concreto. Consideremos dos instancias de calibración:

\begin{itemize}
\item Observación $A$: $h(x_A) = 50$, $\sigma_A = 2$ (predicción media, baja volatilidad)
\item Observación $B$: $h(x_B) = 51$, $\sigma_B = 8$ (predicción media, alta volatilidad)
\end{itemize}

El MCPS estándar, al depender únicamente de $h(x)$, asignaría ambas observaciones al mismo bin de predicción, asumiendo que sus distribuciones de error son intercambiables. Sin embargo, el AV-MCPS reconoce que la observación $B$ proviene de un régimen de mayor incertidumbre intrínseca, donde los errores de predicción tienden a ser más grandes en magnitud absoluta. Por lo tanto, las asigna a celdas distintas: $A \in \mathcal{D}_c^{(\kappa, \lambda_{\text{low}})}$ y $B \in \mathcal{D}_c^{(\kappa, \lambda_{\text{high}})}$.

Cuando se genera la distribución predictiva para un nuevo punto de prueba $x_{\text{test}}$ con predicción $h(x_{\text{test}}) = 50.5$ y volatilidad actual $\sigma_{\text{test}} = 7.5$, el AV-MCPS utiliza calibration scores exclusivamente de la celda $\mathcal{D}_c^{(\kappa, \lambda_{\text{high}})}$, produciendo una distribución predictiva naturalmente más ancha que reflejará la mayor incertidumbre del régimen de alta volatilidad.

\paragraph{Cálculo Localizado de Scores Conformales}

El procedimiento de inferencia del AV-MCPS sigue estos pasos:

\begin{enumerate}
\item Para un punto de prueba $x_{\text{test}}$, calcular su predicción puntual $h(x_{\text{test}})$ y su volatilidad local $\sigma_{\text{test}}$ usando el modelo base y la ventana temporal disponible.

\item Determinar la celda correspondiente $(\kappa^*, \lambda^*)$ mediante:
\begin{equation}
\kappa^* = \arg\min_{\kappa} \left\{ \kappa : h(x_{\text{test}}) < q_{\text{pred}}^{\kappa} \right\}
\end{equation}
\begin{equation}
\lambda^* = \arg\min_{\lambda} \left\{ \lambda : \sigma_{\text{test}} < q_{\text{vol}}^{\lambda} \right\}
\end{equation}

\item Extraer el subconjunto local de calibración:
\begin{equation}
\mathcal{D}_c^{(\kappa^*,\lambda^*)} = \left\{ (x_i, y_i) : i \in \text{Bin}_{\kappa^*}^{\text{pred}} \cap \text{Bin}_{\lambda^*}^{\text{vol}} \right\}
\end{equation}

\item Calcular calibration scores localizados:
\begin{equation}
C_i^{(\kappa^*,\lambda^*)} = h(x_{\text{test}}) + (y_i - h(x_i)), \quad \forall (x_i, y_i) \in \mathcal{D}_c^{(\kappa^*,\lambda^*)}
\end{equation}
\end{enumerate}

La distribución predictiva se construye entonces a partir de estos scores localizados, de manera idéntica al MCPS estándar.

\paragraph{Garantías Teóricas de Cobertura}

Bajo el supuesto de intercambiabilidad condicional dentro de cada celda, es decir:

\begin{equation}
(x_i, y_i) \mid h(x_i) \in \text{Bin}_{\kappa}, \sigma_i \in \text{Bin}_{\lambda} \overset{d}{=} (x_j, y_j) \mid h(x_j) \in \text{Bin}_{\kappa}, \sigma_j \in \text{Bin}_{\lambda}
\end{equation}

para todo $i, j$, las garantías de cobertura conformal se mantienen localmente:

\begin{equation}
\mathbb{P}\left(Y \in \hat{C}_{\alpha}(x) \,\big|\, x \in \text{Bin}_{\kappa}^{\text{pred}}, \sigma(x) \in \text{Bin}_{\lambda}^{\text{vol}}\right) \geq 1 - \alpha
\end{equation}

Esta garantía es más fuerte que la del MCPS estándar, ya que condiciona sobre una partición más fina del espacio. Intuitivamente, al controlar simultáneamente por nivel y volatilidad, el AV-MCPS logra una \textbf{adaptabilidad condicional mejorada}, reduciendo la heterogeneidad residual dentro de cada celda.

\paragraph{Trade-off: Resolución vs. Tamaño de Muestra}

El particionamiento bidimensional introduce un trade-off fundamental. Sea $N_c$ el tamaño total del conjunto de calibración. El número esperado de observaciones por celda es:

\begin{equation}
\mathbb{E}\left[N_c^{(\kappa,\lambda)}\right] = \frac{N_c}{B_{\text{pred}} \times B_{\text{vol}}}
\end{equation}

asumiendo distribución uniforme (que raramente se cumple en la práctica). Aumentar la resolución del particionamiento ($B_{\text{pred}}, B_{\text{vol}} \uparrow$) mejora la localización pero reduce el tamaño de muestra en cada celda, incrementando la varianza de los cuantiles estimados. Este trade-off es más severo que en MCPS estándar debido al producto de dimensiones.

Para mitigar este problema, el AV-MCPS implementa una \textbf{estrategia de fallback jerárquico} que degrada progresivamente la localización cuando el tamaño de muestra es insuficiente:

\begin{equation}
\mathcal{D}_c^{\text{efectivo}} = 
\begin{cases}
\mathcal{D}_c^{(\kappa^*,\lambda^*)} & \text{si } N_c^{(\kappa^*,\lambda^*)} \geq \tau_{\text{min}} \\
\mathcal{D}_c^{(\kappa^*, \cdot)} & \text{si } N_c^{(\kappa^*,\lambda^*)} < \tau_{\text{min}} \text{ y } N_c^{(\kappa^*, \cdot)} \geq \tau_{\text{min}} \\
\mathcal{D}_c & \text{en otro caso}
\end{cases}
\end{equation}

donde $\mathcal{D}_c^{(\kappa^*, \cdot)} = \bigcup_{\lambda=1}^{B_{\text{vol}}} \mathcal{D}_c^{(\kappa^*,\lambda)}$ representa el bin unidimensional basado solo en predicción, y $\tau_{\text{min}}$ es un umbral de tamaño mínimo (típicamente 5 observaciones). Esta estrategia garantiza robustez operativa sin sacrificar las garantías teóricas conformales.

\subsubsection{De la Teoría a la Práctica}

La implementación del AV-MCPS para series temporales autorregresivas, desarrollada en esta tesis, traduce el marco teórico bidimensional en un sistema predictivo operativo mediante las siguientes decisiones de diseño.

\paragraph{Adaptación 1: Estimación Rolling de Volatilidad}

\textbf{Desafío Práctico:} En series temporales reales, la volatilidad $\sigma_t$ no es directamente observable y debe estimarse a partir de datos históricos. Diferentes métodos de estimación (desviación estándar simple, GARCH, volatilidad realizada, etc.) pueden producir señales de volatilidad con características distintas.

\textbf{Solución Implementada:} Se emplea la \textit{desviación estándar rolling} con ventana de longitud $w$, calculada mediante:

\begin{equation}
\hat{\sigma}_t = \sqrt{\frac{1}{w-1} \sum_{k=t-w}^{t-1} (y_k - \bar{y}_t)^2}
\end{equation}

donde $\bar{y}_t = \frac{1}{w}\sum_{k=t-w}^{t-1} y_k$ es la media rolling. Esta elección se justifica por su:

\begin{itemize}
\item \textbf{Simplicidad computacional:} No requiere ajuste iterativo de parámetros como GARCH.
\item \textbf{Robustez:} La ventana fija previene la amplificación de shocks transitorios.
\item \textbf{Interpretabilidad:} Representa directamente la dispersión histórica reciente.
\end{itemize}

\textbf{Manejo de Valores Faltantes:} Para las primeras $w-1$ observaciones donde la ventana completa no está disponible, se aplica \textit{backfilling} con la desviación estándar de toda la serie inicial disponible:

\begin{equation}
\hat{\sigma}_t = \sqrt{\frac{1}{t-2} \sum_{k=1}^{t-1} (y_k - \bar{y}_{1:t-1})^2} \quad \text{para } t < w
\end{equation}

Esto asegura que todas las observaciones de calibración tengan una volatilidad asociada, evitando pérdida de información.

\paragraph{Adaptación 2: Construcción de Bins mediante Cuantiles Robustos}

\textbf{Teoría Original:} El particionamiento Mondrian estándar utiliza cuantiles equiespaciados calculados sobre las predicciones de calibración. Esta estrategia asume implícitamente que las predicciones tienen distribución continua con densidad suficiente en todo el soporte.

\textbf{Desafío en Volatilidad:} Las series de volatilidad estimada $\{\hat{\sigma}_i\}$ frecuentemente exhiben distribuciones asimétricas con colas pesadas (outliers durante períodos de crisis) y concentración de masa en valores bajos. Intentar dividir esta distribución en $B_{\text{vol}}$ bins equiespaciados puede resultar en bins con fronteras colapsadas.

\textbf{Solución Implementada:} Se emplea la función \texttt{pd.qcut} con opción \texttt{duplicates='drop'}, que fusiona automáticamente bins con fronteras idénticas. El número efectivo de bins de volatilidad es:

\begin{equation}
B_{\text{vol}}^{\text{efectivo}} = \left|\left\{q_{\text{vol}}^{\lambda} : \lambda = 1, \ldots, B_{\text{vol}}-1\right\}\right| \leq B_{\text{vol}}
\end{equation}

donde $|\cdot|$ denota cardinalidad del conjunto de cuantiles únicos. Esta adaptación es matemáticamente válida bajo el marco conformal, ya que no requiere que todos los bins tengan el mismo tamaño, solo que la partición sea determinada usando únicamente datos de calibración.

\textbf{Mecanismo de Fallback:} Si el procedimiento de binning falla completamente en alguna dimensión (por ejemplo, todas las predicciones idénticas o volatilidad constante), el sistema degrada automáticamente a MCPS unidimensional usando solo la dimensión válida, o a SCPS global si ambas dimensiones fallan.

\paragraph{Adaptación 3: Representación Directa de Calibration Scores}

Consistente con la filosofía adoptada para LSPM y MCPS, el AV-MCPS retorna directamente el conjunto de calibration scores sin transformación adicional:

\begin{equation}
\mathcal{C}^{(\kappa^*,\lambda^*)} = \left\{C_i^{(\kappa^*,\lambda^*)}\right\}_{i=1}^{N_c^{(\kappa^*,\lambda^*)}}
\end{equation}

Esta representación empírica discreta constituye una muestra válida de la distribución predictiva conformal bajo intercambiabilidad. Cualquier estadístico de interés (media, varianza, cuantiles, CRPS) puede calcularse directamente sobre $\mathcal{C}^{(\kappa^*,\lambda^*)}$ sin necesidad de reconstruir una CDF continua, manteniendo pureza teórica y eficiencia computacional.

\paragraph{Adaptación 4: Protocolo de Congelamiento Bidimensional}

Para garantizar rigor estadístico en la evaluación rolling, el AV-MCPS implementa un protocolo de congelamiento que fija simultáneamente:

\begin{enumerate}
\item \textbf{Modelo base:} Los parámetros del regressor XGBoost se entrenan una sola vez sobre el conjunto de entrenamiento y se congelan.

\item \textbf{Bins de predicción:} Los bordes $\{q_{\text{pred}}^{\kappa}\}_{\kappa=0}^{B_{\text{pred}}}$ se calculan sobre las predicciones de calibración y se congelan.

\item \textbf{Bins de volatilidad:} Los bordes $\{q_{\text{vol}}^{\lambda}\}_{\lambda=0}^{B_{\text{vol}}}$ se calculan sobre las volatilidades de calibración y se congelan.

\item \textbf{Artefactos de calibración:} Las predicciones $\{h(x_i)\}$, valores observados $\{y_i\}$, y volatilidades $\{\sigma_i\}$ del conjunto de calibración se almacenan.
\end{enumerate}

Durante la fase de evaluación rolling, para cada nuevo punto de prueba $t$:

\begin{itemize}
\item Se extraen los últimos $p$ valores observados para construir $x_t = [y_{t-p}, \ldots, y_{t-1}]$
\item Se calcula $h(x_t)$ usando el modelo congelado
\item Se estima $\sigma_t$ usando la ventana rolling actual
\item Se asigna $(h(x_t), \sigma_t)$ a la celda $(\kappa^*, \lambda^*)$ usando los bordes congelados
\item Se generan scores conformales usando los artefactos de calibración congelados
\end{itemize}

Este protocolo es crítico para prevenir data leakage y garantizar que la evaluación del desempeño predictivo sea válida.

\subsubsection{Diferencias Respecto al MCPS Estándar}

La Tabla \ref{tab:avmcps_vs_mcps} resume las principales diferencias metodológicas entre MCPS y AV-MCPS.

\begin{table}[htbp]
\centering
\caption{Comparación entre MCPS y AV-MCPS}
\label{tab:avmcps_vs_mcps}
\begin{tabular}{>{\raggedright\arraybackslash}p{4cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{MCPS} & \textbf{AV-MCPS} \\
\midrule
Dimensiones de particionamiento & Unidimensional (solo predicción) & Bidimensional (predicción + volatilidad) \\
\addlinespace
Número de bins & $B$ & $B_{\text{pred}} \times B_{\text{vol}}$ \\
\addlinespace
Tamaño esperado de celda & $N_c / B$ & $N_c / (B_{\text{pred}} \times B_{\text{vol}})$ \\
\addlinespace
Captura de heterocedasticidad & Indirecta (via niveles de predicción) & Explícita (via volatilidad local) \\
\addlinespace
Complejidad computacional & $O(\log B)$ por predicción & $O(\log B_{\text{pred}} + \log B_{\text{vol}})$ \\
\addlinespace
Estrategia de fallback & Degradar a SCPS si $N_c^{\kappa} < \tau$ & Jerárquica: 2D $\to$ 1D $\to$ SCPS \\
\addlinespace
Hiperparámetros adicionales & Ninguno & \texttt{volatility\_window} ($w$), $B_{\text{vol}}$ \\
\addlinespace
Casos de uso óptimos & Heterogeneidad determinada por nivel & Series con volatilidad cambiante en el tiempo \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Innovación Metodológica: Estratificación Volatilidad-Predicción}

La contribución fundamental del AV-MCPS es el reconocimiento de que la \textbf{volatilidad local es un predictor del error de predicción independiente del nivel de predicción}. Formalmente, si denotamos el error absoluto de predicción como $e_i = |y_i - h(x_i)|$, la hipótesis subyacente del AV-MCPS es:

\begin{equation}
\mathbb{E}[e_i \mid h(x_i), \sigma_i] \neq \mathbb{E}[e_i \mid h(x_i)]
\end{equation}

Es decir, conocer la volatilidad local $\sigma_i$ provee información adicional sobre la magnitud esperada del error, más allá de la predicción puntual $h(x_i)$. Evidencia empírica de esta relación se encuentra ampliamente documentada en la literatura de volatilidad condicional \parencite{Engle1982, Bollerslev1986}.

El AV-MCPS explota esta estructura mediante la estratificación bidimensional, logrando distribuciones predictivas que se adaptan simultáneamente tanto al \textit{nivel} como al \textit{régimen de incertidumbre} del proceso. Esta adaptabilidad dual representa una ventaja teórica significativa sobre el MCPS unidimensional, especialmente en series con volatilidad time-varying como procesos financieros, climáticos, o epidemiológicos.

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

\paragraph{Hiperparámetros Primarios}

\textbf{n\_lags ($p$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $p=15$, ligeramente mayor que MCPS estándar para capturar dependencias de más largo plazo
\item \textit{Significado:} Define el orden del modelo autorregresivo subyacente
\end{itemize}

\textbf{n\_pred\_bins ($B_{\text{pred}}$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $B_{\text{pred}}=8$
\item \textit{Rango recomendado:} $[5, 10]$
\item \textit{Función:} Controla la resolución del particionamiento en la dimensión de predicción
\end{itemize}

\textbf{n\_vol\_bins ($B_{\text{vol}}$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $B_{\text{vol}}=4$
\item \textit{Rango recomendado:} $[3, 5]$
\item \textit{Función:} Controla la resolución del particionamiento en la dimensión de volatilidad
\item \textit{Justificación de valor menor:} La distribución de volatilidades tiende a ser más concentrada que las predicciones, requiriendo menos bins para capturar los regímenes principales (baja, media, alta volatilidad)
\end{itemize}

\textbf{volatility\_window ($w$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $w=20$
\item \textit{Rango típico:} $[10, 30]$
\item \textit{Función:} Define la longitud de la ventana rolling para estimar volatilidad local
\item \textit{Trade-off:} Ventanas pequeñas capturan cambios rápidos de volatilidad pero son más ruidosas; ventanas grandes suavizan estimaciones pero reaccionan lentamente a cambios de régimen
\end{itemize}

\textbf{test\_size:}
\begin{itemize}
\item \textit{Tipo:} Proporción en $(0, 1)$
\item \textit{Valor por defecto:} $0.25$
\item \textit{Consideración especial:} Dado que el particionamiento bidimensional fragmenta más el conjunto de calibración, valores menores a $0.20$ pueden resultar en celdas demasiado dispersas
\end{itemize}

\paragraph{Parámetros del Modelo Base}

El modelo autorregresivo subyacente se implementa idénticamente al MCPS estándar mediante XGBoost con 50 árboles de profundidad 3 y tasa de aprendizaje 0.1.

\paragraph{Interpretación Práctica de la Configuración Bidimensional}

La selección conjunta de $(B_{\text{pred}}, B_{\text{vol}}, N_c)$ determina el balance fundamental del AV-MCPS. El tamaño esperado de celda es:

\begin{equation}
\mathbb{E}[N_c^{(\kappa,\lambda)}] = \frac{N_c}{B_{\text{pred}} \times B_{\text{vol}}}
\end{equation}

Para la configuración estándar $(B_{\text{pred}}=8, B_{\text{vol}}=4, N_c=50)$ típica en series con $T \approx 250$ observaciones, esto resulta en:

\begin{equation}
\mathbb{E}[N_c^{(\kappa,\lambda)}] = \frac{50}{8 \times 4} = 1.56 \text{ observaciones por celda}
\end{equation}

Este valor es evidentemente insuficiente para estimación robusta, lo que justifica la necesidad crítica del mecanismo de fallback jerárquico. En la práctica, la distribución de observaciones entre celdas es altamente no uniforme: celdas centrales (predicción y volatilidad media) contienen muchas más observaciones que celdas extremas.

\paragraph{Recomendaciones de Configuración}

Para series temporales con $T \geq 500$ observaciones:
\begin{itemize}
\item Configuración balanceada: $(B_{\text{pred}}=8, B_{\text{vol}}=4, w=20)$
\item Para series con volatilidad muy cambiante: aumentar $B_{\text{vol}}=5$ y reducir $w=15$
\item Para series estables: reducir $B_{\text{vol}}=3$ y aumentar $w=30$
\end{itemize}

Para series con $T < 300$ observaciones:
\begin{itemize}
\item Configuración conservadora: $(B_{\text{pred}}=6, B_{\text{vol}}=3, w=15)$
\item Considerar degradar a MCPS estándar si $T < 200$
\end{itemize}


\subsection{DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks}

\subsubsection{Propuesta Teórica}

DeepAR, introducido por \textcite{Salinas2020}, representa un cambio de paradigma en el pronóstico probabilístico de series temporales al trasladar el enfoque desde el modelado individual de cada serie hacia el \textbf{aprendizaje de un modelo global} a partir de múltiples series relacionadas. A diferencia de los métodos clásicos basados en la metodología Box-Jenkins \parencite{Box1968} o suavizamiento exponencial \parencite{Hyndman2008}, donde los parámetros se estiman independientemente para cada serie, DeepAR aprovecha la información compartida entre series mediante una arquitectura de red neuronal recurrente autorregresiva.

\paragraph{Fundamento Arquitectónico: Redes LSTM Autorregresivas}

El modelo se basa en una red neuronal recurrente con celdas LSTM (Long Short-Term Memory) que procesa secuencias temporales de forma autorregresiva. Formalmente, para una serie temporal $i$ con valores $z_{i,t}$ en el tiempo $t$, DeepAR modela la distribución condicional del futuro dado el pasado:

\begin{equation}
P(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T})
\end{equation}

donde $t_0$ denota el punto de inicio del horizonte de predicción, $z_{i,1:t_0-1}$ representa el rango de condicionamiento (valores observados), $z_{i,t_0:T}$ representa el rango de predicción (valores futuros), y $x_{i,1:T}$ son covariables conocidas para todo el período.

La arquitectura factoriza esta distribución mediante el producto de verosimilitudes condicionales:

\begin{equation}
Q_{\Theta}(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T}) = \prod_{t=t_0}^{T} Q_{\Theta}(z_{i,t} \mid z_{i,1:t-1}, x_{i,1:T})
\label{eq:deepar_factorization}
\end{equation}

donde cada factor está parametrizado por la salida de la red recurrente:

\begin{equation}
Q_{\Theta}(z_{i,t} \mid z_{i,1:t-1}, x_{i,1:T}) = \ell(z_{i,t} \mid \theta(h_{i,t}, \Theta))
\end{equation}

El estado oculto $h_{i,t}$ se actualiza recursivamente mediante:

\begin{equation}
h_{i,t} = h(h_{i,t-1}, z_{i,t-1}, x_{i,t}, \Theta)
\label{eq:deepar_recurrence}
\end{equation}

donde $h(\cdot)$ es una función implementada por una red LSTM multicapa con parámetros $\Theta$.

\paragraph{Naturaleza Autorregresiva del Modelo}

La característica distintiva de DeepAR es su naturaleza autorregresiva: en cada paso temporal $t$, la red consume como entrada el valor observado del paso anterior $z_{i,t-1}$ junto con las covariables $x_{i,t}$ y el estado oculto previo $h_{i,t-1}$. Esta estructura permite que el modelo capture dependencias temporales complejas sin necesidad de especificar manualmente órdenes autorregresivos o estructuras de media móvil, como requieren los modelos ARIMA.

Durante el entrenamiento, todos los valores $z_{i,t}$ en el rango de predicción son conocidos y se utilizan directamente en la ecuación \eqref{eq:deepar_recurrence}. Durante la predicción, para $t \geq t_0$, los valores futuros son desconocidos, por lo que se reemplazan por muestras $\tilde{z}_{i,t} \sim \ell(\cdot \mid \theta(h_{i,t}, \Theta))$ generadas por el propio modelo, que se retroalimentan para calcular el siguiente estado oculto. Este proceso de \textit{muestreo ancestral} genera trayectorias completas $\tilde{z}_{i,t_0:T}$ que representan realizaciones posibles del futuro.

\paragraph{Función de Verosimilitud y Modelado de la Incertidumbre}

A diferencia de muchos métodos de aprendizaje profundo que se enfocan únicamente en predicciones puntuales, DeepAR está diseñado explícitamente para pronóstico probabilístico. La red no predice directamente el valor futuro $z_{i,t}$, sino los \textbf{parámetros de una distribución de probabilidad} $\theta(h_{i,t})$ sobre los valores futuros posibles.

\textcite{Salinas2020} proponen dos familias de distribuciones según las características de los datos:

\begin{enumerate}
\item \textbf{Verosimilitud Gaussiana} para datos de valores reales:
\begin{equation}
\ell_G(z \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)
\end{equation}

donde la media $\mu(h_{i,t})$ se obtiene mediante una transformación afín de la salida de la red, y la desviación estándar $\sigma(h_{i,t})$ mediante una transformación afín seguida de una activación softplus para garantizar positividad:

\begin{equation}
\mu(h_{i,t}) = w_{\mu}^T h_{i,t} + b_{\mu}, \quad \sigma(h_{i,t}) = \log(1 + \exp(w_{\sigma}^T h_{i,t} + b_{\sigma}))
\end{equation}

\item \textbf{Verosimilitud Binomial Negativa} para datos de conteo:
\begin{equation}
\ell_{NB}(z \mid \mu, \alpha) = \frac{\Gamma(z + 1/\alpha)}{\Gamma(z+1)\Gamma(1/\alpha)} \left(\frac{1}{1+\alpha\mu}\right)^{1/\alpha} \left(\frac{\alpha\mu}{1+\alpha\mu}\right)^z
\end{equation}

donde $\mu \in \mathbb{R}^+$ es la media y $\alpha \in \mathbb{R}^+$ es un parámetro de forma que controla la sobredispersión. En esta parametrización, $\text{Var}[z] = \mu + \mu^2\alpha$, permitiendo modelar varianzas mayores que la media, característica común en datos de demanda intermitente.
\end{enumerate}

La elección de la verosimilitud debe reflejar las propiedades estadísticas de los datos. Para series con valores continuos y errores aproximadamente simétricos, la Gaussiana es apropiada. Para datos de conteo con alta variabilidad o comportamiento intermitente, la Binomial Negativa es preferible.

\paragraph{Entrenamiento mediante Máxima Verosimilitud}

Los parámetros $\Theta$ del modelo se aprenden maximizando la log-verosimilitud sobre todas las series temporales en el conjunto de entrenamiento:

\begin{equation}
\mathcal{L}(\Theta) = \sum_{i=1}^{N} \sum_{t=1}^{T} \log \ell(z_{i,t} \mid \theta(h_{i,t}, \Theta))
\label{eq:deepar_likelihood}
\end{equation}

donde $N$ es el número de series en el conjunto de datos. Esta función objetivo puede optimizarse directamente mediante descenso de gradiente estocástico, ya que $h_{i,t}$ es una función determinística de las entradas y no hay variables latentes que requieran inferencia.

\textcite{Salinas2020} destacan una ventaja fundamental de este enfoque: dado que el modelo es completamente observable durante el entrenamiento, no se requieren técnicas de inferencia variacional o métodos de Monte Carlo para aproximar la función objetivo, como sí ocurre en modelos de espacio de estados con variables latentes.

\paragraph{Manejo de Escalas Heterogéneas: El Problema de la Ley de Potencias}

Uno de los desafíos más significativos que aborda DeepAR es la presencia de series temporales con magnitudes que varían en varios órdenes de magnitud. \textcite{Salinas2020} documentan que en conjuntos de datos de demanda retail de Amazon, la distribución de velocidades de ventas (ventas promedio por período) sigue aproximadamente una ley de potencias. Esta distribución implica que un pequeño número de productos tiene ventas extremadamente altas, mientras que la mayoría tiene ventas bajas, con una distribución fuertemente sesgada.

Este fenómeno presenta dos problemas fundamentales:

\begin{enumerate}
\item \textbf{Problema de rango operativo:} Las no linealidades de la red (funciones de activación como tanh, ReLU) tienen rangos operativos limitados. Sin ajustes, la red debería aprender a escalar las entradas autorregresivas $z_{i,t-1}$ (que pueden variar entre 1 y 10,000) a un rango apropiado en la capa de entrada, y luego invertir este escalamiento en la capa de salida. Este aprendizaje implícito es ineficiente y puede causar problemas de convergencia.

\item \textbf{Desbalance en el muestreo:} Si las instancias de entrenamiento se muestrean uniformemente, las series de baja magnitud dominarían el conjunto de datos simplemente por su abundancia numérica, causando que el modelo se ajuste mal a las series de alta magnitud que pueden ser críticas para el negocio.
\end{enumerate}

\textbf{Solución 1: Escalamiento Dependiente del Ítem}

DeepAR introduce un mecanismo de escalamiento explícito que normaliza las entradas y salidas autorregresivas por un factor de escala específico de cada serie $\nu_i$. Formalmente, las entradas autorregresivas se transforman como:

\begin{equation}
\tilde{z}_{i,t} = \frac{z_{i,t}}{\nu_i}
\end{equation}

y los parámetros de la verosimilitud dependientes de escala se ajustan correspondientemente. Para la verosimilitud Gaussiana:

\begin{equation}
\mu_{\text{escalado}} = \nu_i \cdot \mu(h_{i,t}), \quad \sigma_{\text{escalado}} = \nu_i \cdot \sigma(h_{i,t})
\end{equation}

Para la verosimilitud Binomial Negativa:

\begin{equation}
\mu_{\text{escalado}} = \nu_i \cdot \log(1 + \exp(o_{\mu})), \quad \alpha_{\text{escalado}} = \frac{\log(1 + \exp(o_{\alpha}))}{\sqrt{\nu_i}}
\end{equation}

donde $o_{\mu}, o_{\alpha}$ son las salidas crudas de la red. El factor de escala típicamente se define como:

\begin{equation}
\nu_i = 1 + \frac{1}{t_0} \sum_{t=1}^{t_0} z_{i,t}
\label{eq:deepar_scale_factor}
\end{equation}

es decir, el promedio histórico de la serie más uno (para evitar divisiones por cero en series con media cero).

\textbf{Solución 2: Muestreo Ponderado por Velocidad}

Para contrarrestar el desbalance introducido por la ley de potencias, DeepAR implementa un esquema de muestreo no uniforme donde la probabilidad de seleccionar una ventana de entrenamiento de la serie $i$ es proporcional a su factor de escala $\nu_i$:

\begin{equation}
P(\text{seleccionar serie } i) \propto \nu_i
\end{equation}

Este muestreo ponderado garantiza que series de alta velocidad sean visitadas con mayor frecuencia durante el entrenamiento, compensando su baja representación numérica y permitiendo que el modelo aprenda patrones específicos de estos ítems críticos.

\paragraph{Generación de Múltiples Trayectorias y Pronóstico Probabilístico}

Una vez entrenado el modelo, la generación de pronósticos probabilísticos se realiza mediante muestreo ancestral. Para cada serie $i$, se generan $B$ trayectorias completas $\{\tilde{z}_{i,t_0:T}^{(b)}\}_{b=1}^{B}$ mediante el siguiente procedimiento:

\begin{enumerate}
\item Calcular el estado inicial $h_{i,t_0-1}$ procesando el rango de condicionamiento con los valores observados $z_{i,1:t_0-1}$.

\item Para cada trayectoria $b = 1, \ldots, B$ y cada paso temporal $t = t_0, \ldots, T$:
\begin{enumerate}
\item Calcular los parámetros de la distribución: $\theta_{i,t} = \theta(h_{i,t}, \Theta)$
\item Muestrear: $\tilde{z}_{i,t}^{(b)} \sim \ell(\cdot \mid \theta_{i,t})$
\item Actualizar estado: $h_{i,t+1} = h(h_{i,t}, \tilde{z}_{i,t}^{(b)}, x_{i,t+1}, \Theta)$
\end{enumerate}
\end{enumerate}

El conjunto de trayectorias $\{\tilde{z}_{i,t_0:T}^{(b)}\}$ representa una muestra empírica de la distribución predictiva conjunta $Q_{\Theta}(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T})$. A partir de estas muestras, se pueden calcular diversos estadísticos de interés:

\begin{itemize}
\item \textbf{Cuantiles puntuales:} El cuantil $q$-ésimo para el tiempo $t$ se obtiene como el cuantil empírico de $\{\tilde{z}_{i,t}^{(b)}\}_{b=1}^{B}$

\item \textbf{Predicción puntual:} La mediana (cuantil 0.5) típicamente se usa como predicción puntual

\item \textbf{Intervalos de predicción:} Intervalos del $(1-\alpha)\times 100\%$ se construyen mediante $[q_{\alpha/2}, q_{1-\alpha/2}]$

\item \textbf{Distribuciones agregadas:} Para evaluar la demanda total en un período $[t_0, t_0+S)$, se suma cada trayectoria: $Z_i^{(b)} = \sum_{t=t_0}^{t_0+S-1} \tilde{z}_{i,t}^{(b)}$, y se calculan estadísticos sobre $\{Z_i^{(b)}\}$
\end{itemize}

Una ventaja crucial de este enfoque es que las trayectorias muestreadas preservan las correlaciones temporales aprendidas por el modelo. Como demuestran \textcite{Salinas2020}, si se destruyen artificialmente estas correlaciones (mezclando aleatoriamente los valores de cada tiempo entre trayectorias), la calibración de los intervalos de predicción se degrada significativamente, especialmente para horizontes multi-paso.

\paragraph{Aprendizaje de Patrones Complejos desde los Datos}

A diferencia de modelos clásicos donde la estacionalidad, tendencia y crecimiento de la incertidumbre deben especificarse manualmente, DeepAR aprende estos patrones automáticamente desde los datos. \textcite{Salinas2020} documentan que el modelo:

\begin{itemize}
\item \textbf{Aprende estacionalidad heterogénea:} Puede capturar patrones estacionales que varían entre ítems sin requerir especificación manual de períodos estacionales

\item \textbf{Modela crecimiento no lineal de la incertidumbre:} A diferencia de modelos de espacio de estados que asumen crecimiento lineal de la varianza con el horizonte, DeepAR aprende patrones más complejos. Por ejemplo, en datos de retail, la incertidumbre puede aumentar durante el cuarto trimestre (temporada alta) y luego decrecer en enero

\item \textbf{Genera pronósticos calibrados:} Los intervalos de predicción tienen cobertura empírica cercana a la cobertura nominal especificada, indicando que el modelo cuantifica adecuadamente la incertidumbre
\end{itemize}

\subsubsection{De la Teoría a la Práctica}

La implementación de DeepAR para series temporales univariadas desarrollada en esta investigación traduce el marco teórico autorregresivo a un sistema predictivo concreto mediante las siguientes adaptaciones específicas al contexto de este estudio.

\paragraph{Adaptación 1: Simplificación de Covariables}

\textbf{Teoría Original:} El trabajo de \textcite{Salinas2020} asume la disponibilidad de múltiples covariables $x_{i,t}$ tanto dependientes del tiempo (día de la semana, mes del año, indicadores de promociones) como específicas del ítem (categoría de producto, características físicas). Estas covariables se estandarizan y se concatenan con las entradas autorregresivas.

\textbf{Adaptación para Series Univariadas:} En el contexto de este estudio, donde el objetivo es evaluar el desempeño de métodos de pronóstico probabilístico en series sintéticas sin información auxiliar, la implementación \textbf{no utiliza covariables externas}. La arquitectura se reduce a su forma más pura autorregresiva:

\begin{equation}
h_{i,t} = h(h_{i,t-1}, z_{i,t-1}, \Theta)
\end{equation}

Esta simplificación elimina la dependencia de $x_{i,t}$ en la ecuación \eqref{eq:deepar_recurrence}, haciendo que el modelo dependa únicamente de la historia observada de la serie. Matemáticamente, esto equivale a fijar $x_{i,t} = \emptyset$ para todo $t$, o alternativamente, a usar un vector de covariables constante que es absorbido en los parámetros de la red.

\textbf{Implicación Técnica:} La capa de entrada de la LSTM tiene dimensión 1 (solo el valor autorregresivo escalado), en lugar de $1 + D_x$ donde $D_x$ sería la dimensionalidad de las covariables. Esta simplificación tiene dos consecuencias:

\begin{itemize}
\item \textbf{Reducción paramétrica:} Menos parámetros en la capa de entrada aceleran el entrenamiento y reducen el riesgo de sobreajuste

\item \textbf{Pureza metodológica:} Al no depender de información auxiliar, la comparación entre métodos se centra exclusivamente en la capacidad de cada enfoque para extraer patrones de la historia temporal intrínseca
\end{itemize}

\paragraph{Adaptación 2: Construcción de Ventanas de Entrenamiento}

\textbf{Desafío Práctico:} En el protocolo de simulación de esta tesis, cada configuración genera una única serie temporal de longitud $n=252$. A diferencia del contexto original de DeepAR donde se dispone de miles de series relacionadas, aquí el modelo debe aprender exclusivamente de ventanas extraídas de una sola serie.

\textbf{Solución Implementada:} Se emplea una estrategia de \textit{windowing} deslizante para generar múltiples instancias de entrenamiento a partir de la serie única. Sea $\{y_1, y_2, \ldots, y_{n_{\text{train}}}\}$ el conjunto de entrenamiento. Se construyen instancias $(X^{(k)}, y^{(k)})$ mediante:

\begin{equation}
X^{(k)} = [y_k, y_{k+1}, \ldots, y_{k+p-1}], \quad y^{(k)} = y_{k+p}
\end{equation}

para $k = 1, \ldots, n_{\text{train}} - p$, donde $p$ es el número de rezagos autorregresivos (hiperparámetro \texttt{n\_lags}). Esto genera aproximadamente $n_{\text{train}} - p$ instancias de entrenamiento, incrementando artificialmente el tamaño del conjunto de datos.

\textbf{Justificación Teórica:} Este procedimiento es válido bajo el supuesto de estacionariedad local, donde se asume que las dependencias temporales son homogéneas en diferentes ventanas temporales. Para los escenarios ARMA y SETAR estacionarios de este estudio, esta suposición se cumple por diseño. Para el escenario ARIMA, la diferenciación previa de la serie garantiza estacionariedad de la serie transformada.

\paragraph{Adaptación 3: Normalización mediante Z-Score}

\textbf{Teoría Original:} \textcite{Salinas2020} utilizan el escalamiento por $\nu_i$ definido en \eqref{eq:deepar_scale_factor} específicamente para manejar la ley de potencias en magnitudes entre series. Este escalamiento no centra los datos (no resta la media).

\textbf{Adaptación Implementada:} Para series individuales donde la preocupación principal es la convergencia del entrenamiento y no la comparación entre escalas heterogéneas, se emplea \textbf{normalización Z-score completa}:

\begin{equation}
\tilde{z}_t = \frac{z_t - \mu_{\text{train}}}{\sigma_{\text{train}} + \epsilon}
\end{equation}

donde $\mu_{\text{train}}$ y $\sigma_{\text{train}}$ son la media y desviación estándar muestrales del conjunto de entrenamiento, y $\epsilon = 10^{-8}$ es una constante pequeña para estabilidad numérica. Las predicciones se des-normalizan mediante:

\begin{equation}
z_t = \tilde{z}_t \cdot \sigma_{\text{train}} + \mu_{\text{train}}
\end{equation}

\textbf{Justificación:} Esta normalización garantiza que las entradas a la LSTM tengan media cero y varianza unitaria, acelerando la convergencia del algoritmo de optimización y previniendo problemas de gradientes explosivos o desvanecientes. Para la verosimilitud Gaussiana empleada en esta implementación, los parámetros predichos $\mu(h_{i,t})$ y $\sigma(h_{i,t})$ están en escala normalizada y se transforman mediante:

\begin{equation}
\mu_{\text{original}} = \mu(h_{i,t}) \cdot \sigma_{\text{train}} + \mu_{\text{train}}, \quad \sigma_{\text{original}} = \sigma(h_{i,t}) \cdot \sigma_{\text{train}}
\end{equation}

\paragraph{Adaptación 4: Early Stopping con Partición Interna}

\textbf{Desafío Técnico:} Las redes neuronales recurrentes son susceptibles al sobreajuste, especialmente cuando el número de parámetros es grande relativo al tamaño del conjunto de datos. Sin regularización apropiada, el modelo puede memorizar el ruido en los datos de entrenamiento, degradando el desempeño predictivo.

\textbf{Solución Implementada:} Se incorpora un mecanismo de \textit{early stopping} con partición interna del conjunto de entrenamiento. De las $n_{\text{train}} - p$ instancias generadas por windowing, se reserva el 20\% final como conjunto de validación interno:

\begin{equation}
n_{\text{val}} = \lfloor 0.2 \times (n_{\text{train}} - p) \rfloor
\end{equation}

Durante el entrenamiento, se monitorea la log-verosimilitud negativa (equivalentemente, la pérdida Gaussian NLL) sobre el conjunto de validación. Se mantiene un contador de paciencia que se incrementa cada época donde la pérdida de validación no mejora. El entrenamiento se detiene si:

\begin{equation}
\text{patience\_counter} \geq \text{patience\_threshold}
\end{equation}

donde el umbral de paciencia es un hiperparámetro (típicamente 5 épocas en esta implementación). Los parámetros del modelo correspondientes a la época con menor pérdida de validación se retienen como modelo final.

\textbf{Ventaja sobre Entrenamiento Fijo:} Este esquema adaptativo permite que el entrenamiento continúe mientras se observe mejora, pero previene ciclos innecesarios que solo contribuyen al sobreajuste. Empíricamente, se observa que el entrenamiento típicamente converge entre 15-30 épocas dependiendo de la complejidad de la serie, mucho antes del límite máximo de 30 épocas especificado.

\paragraph{Adaptación 5: Protocolo de Congelamiento para Rolling Forecast}

\textbf{Problema Crítico:} En la evaluación rolling window implementada en este estudio, donde se realizan predicciones secuenciales sobre 12 pasos de prueba, un enfoque ingenuo re-entrenaría el modelo en cada paso. Esto introduciría dos problemas fundamentales:

\begin{enumerate}
\item \textbf{Data leakage:} La re-estimación de parámetros en cada paso permitiría que información futura contamine la evaluación, invalidando las métricas de desempeño

\item \textbf{Costo computacional prohibitivo:} Entrenar una red LSTM desde cero es computacionalmente costoso (típicamente 5-15 minutos por serie en hardware estándar). Repetir esto 12 veces por serie haría inviable la experimentación a gran escala
\end{enumerate}

\textbf{Solución: Freeze-and-Reuse Protocol}

El protocolo implementado congela completamente el modelo después del entrenamiento inicial:

\begin{enumerate}
\item \textbf{Fase de Congelamiento:} Sobre el conjunto de entrenamiento $(n_{\text{train}} = 200$ observaciones), se:
\begin{itemize}
\item Estima los parámetros de normalización: $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$
\item Entrena la red LSTM hasta convergencia (early stopping)
\item Almacena los pesos de la red: $\Theta_{\text{frozen}}$
\end{itemize}

\item \textbf{Fase de Evaluación Rolling:} Para cada paso de predicción $t = 1, \ldots, 12$:
\begin{itemize}
\item Normaliza los últimos $p$ valores observados usando $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$
\item Calcula la predicción usando $\Theta_{\text{frozen}}$ (sin re-entrenamiento)
\item Des-normaliza las muestras predictivas
\end{itemize}
\end{enumerate}

Este protocolo es \textbf{estadísticamente válido} porque emula una situación realista donde un analista entrena un modelo una vez con datos históricos disponibles y lo utiliza para hacer pronósticos futuros sin acceso a información posterior. La normalización congelada garantiza que el modelo opere siempre en el mismo espacio transformado que aprendió durante el entrenamiento.

\textbf{Implementación Técnica:} La clase \texttt{DeepARModel} mantiene flags explícitos:

\begin{itemize}
\item \texttt{\_is\_frozen}: Booleano que indica si el modelo ha sido congelado
\item \texttt{\_trained\_model}: Referencia a la arquitectura LSTM entrenada
\item \texttt{\_frozen\_mean}, \texttt{\_frozen\_std}: Parámetros de normalización congelados
\end{itemize}

El método \texttt{fit\_predict} verifica \texttt{\_is\_frozen} al inicio: si es \texttt{True}, omite completamente el entrenamiento y procede directamente a generar predicciones con el modelo existente.

\subsubsection{Diferencias Respecto a la Implementación Original de Amazon}

La Tabla \ref{tab:deepar_comparison} resume las principales diferencias metodológicas entre la implementación original de \textcite{Salinas2020} y la adaptación desarrollada en esta tesis.

\begin{table}[htbp]
\centering
\caption{Comparación entre DeepAR Original y DeepAR Adaptado}
\label{tab:deepar_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5.5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{DeepAR Original (Salinas et al.)} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Contexto de aplicación & Miles de series relacionadas (cross-learning) & Serie temporal única (windowing local) \\
\addlinespace
Covariables & Múltiples features temporales e ítem-específicas & Sin covariables externas (solo autorregresivo) \\
\addlinespace
Verosimilitud & Gaussiana y Binomial Negativa & Gaussiana únicamente \\
\addlinespace
Normalización & Escalamiento por $\nu_i$ (sin centrado) & Z-score completo (centrado y escalado) \\
\addlinespace
Muestreo de entrenamiento & Ponderado por velocidad entre series & Uniforme sobre ventanas de serie única \\
\addlinespace
Regularización & No especificada en detalle & Early stopping con validación interna \\
\addlinespace
Protocolo de evaluación & Modelo único para todas las series & Congelamiento total para rolling forecast \\
\addlinespace
Framework & MXNet & PyTorch \\
\addlinespace
Objetivo de diseño & Producción a gran escala (500K+ series) & Evaluación experimental controlada \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Limitaciones de la Adaptación y Desviaciones Teóricas}

Es importante reconocer las limitaciones inherentes a esta adaptación de DeepAR para el contexto de series únicas:

\begin{enumerate}
\item \textbf{Pérdida de cross-learning:} La ventaja fundamental de DeepAR—aprender patrones compartidos entre múltiples series relacionadas—no se explota completamente en este contexto. El modelo aprende únicamente de ventanas de una sola serie, lo que reduce su capacidad de generalización comparado con el escenario multi-serie original.

\item \textbf{Tamaño de datos limitado:} Con $n_{\text{train}} = 200$ observaciones, el número de instancias de entrenamiento ($\sim 150-190$ dependiendo de $p$) es relativamente pequeño para una red neuronal profunda. Esto puede limitar la capacidad del modelo de aprender representaciones complejas sin sobreajuste.

\item \textbf{Ausencia de regularización de verosimilitud:} \textcite{Salinas2020} no detallan técnicas de regularización explícita más allá del dropout entre capas LSTM. En esta implementación, el early stopping actúa como único mecanismo de regularización, lo que puede ser insuficiente para evitar sobreajuste en series altamente ruidosas.
\end{enumerate}

A pesar de estas limitaciones, la implementación mantiene los principios arquitectónicos fundamentales de DeepAR y permite evaluar su desempeño en el marco experimental controlado de este estudio.

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

\paragraph{Hiperparámetros Arquitectónicos}

\textbf{hidden\_size ($h$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $h=20$
\item \textit{Rango explorado en optimización:} $\{10, 20, 40\}$
\item \textit{Función:} Define la dimensionalidad del estado oculto de cada celda LSTM. Controla la capacidad representacional del modelo.
\item \textit{Trade-off:} Valores mayores permiten capturar patrones más complejos pero incrementan el riesgo de sobreajuste y el costo computacional. Para series de longitud moderada ($n \approx 200$), valores en el rango $[10, 40]$ son típicamente suficientes.
\end{itemize}

\textbf{n\_lags ($p$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $p=5$
\item \textit{Rango explorado:} $\{3, 5, 10\}$
\item \textit{Función:} Número de valores pasados utilizados como entrada autorregresiva en cada paso temporal
\item \textit{Interpretación:} Similar al orden $p$ en modelos AR$(p)$, determina la memoria del modelo. Valores mayores permiten capturar dependencias de más largo plazo a costa de reducir el número de instancias de entrenamiento.
\end{itemize}

\textbf{num\_layers ($L$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $L=1$
\item \textit{Función:} Número de capas LSTM apiladas
\item \textit{Justificación de valor conservador:} Para series univariadas sin covariables complejas, una sola capa LSTM típicamente es suficiente. Múltiples capas incrementan dramáticamente el número de parámetros: $\Theta \propto L \times h^2$, exacerbando el riesgo de sobreajuste en muestras pequeñas.
\end{itemize}

\textbf{dropout ($d$):}
\begin{itemize}
\item \textit{Tipo:} Real en $[0, 1)$
\item \textit{Valor por defecto:} $d=0.1$
\item \textit{Función:} Tasa de dropout aplicada entre capas LSTM (solo si $L > 1$)
\item \textit{Mecanismo:} Durante el entrenamiento, cada conexión entre capas se desactiva con probabilidad $d$, forzando a la red a aprender representaciones robustas
\end{itemize}

\paragraph{Hiperparámetros de Optimización}

\textbf{lr (learning rate):}
\begin{itemize}
\item \textit{Tipo:} Real positivo
\item \textit{Valor por defecto:} $\text{lr} = 0.01$
\item \textit{Optimizador:} Adam \parencite{Kingma2015}
\item \textit{Función:} Controla el tamaño del paso en la actualización de parámetros. Adam adapta el learning rate individualmente para cada parámetro basándose en estimaciones de primer y segundo momento del gradiente.
\end{itemize}

\textbf{batch\_size ($B$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $B=32$
\item \textit{Función:} Número de instancias procesadas simultáneamente antes de actualizar parámetros
\item \textit{Trade-off:} Batches grandes ($B > 64$) producen estimaciones más estables del gradiente pero requieren más memoria. Batches pequeños ($B < 16$) introducen más ruido estocástico, lo cual puede actuar como regularización implícita pero ralentiza la convergencia.
\end{itemize}

\textbf{epochs ($E$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $E=30$
\item \textit{Función:} Número máximo de pasadas completas sobre el conjunto de entrenamiento
\item \textit{Nota:} Este es un límite superior; el early stopping típicamente detiene el entrenamiento antes de alcanzar este máximo.
\end{itemize}

\textbf{early\_stopping\_patience ($P$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $P=5$
\item \textit{Función:} Número de épocas sin mejora en la pérdida de validación antes de detener el entrenamiento
\item \textit{Criterio de mejora:} Se considera mejora si $\text{val\_loss}_{t} < \text{best\_val\_loss} - \epsilon$ donde $\epsilon = 10^{-6}$ es una tolerancia numérica
\end{itemize}

\paragraph{Parámetros de Predicción}

\textbf{num\_samples ($S$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $S=1000$
\item \textit{Función:} Número de trayectorias muestreadas mediante muestreo ancestral para construir la distribución predictiva
\item \textit{Precisión:} Con $S=1000$, el error estándar de un cuantil empírico es aproximadamente $\sqrt{p(1-p)/1000}$, que es menor al 1.6\% incluso en el peor caso ($p=0.5$)
\end{itemize}

\textbf{random\_state:}
\begin{itemize}
\item \textit{Tipo:} Entero
\item \textit{Valor por defecto:} 42
\item \textit{Función:} Semilla para inicialización de pesos de la red y generación de muestras. Garantiza reproducibilidad total de los experimentos
\end{itemize}

\paragraph{Proceso de Selección de Hiperparámetros}

La optimización de hiperparámetros sigue un protocolo de búsqueda en rejilla conservador que evita sobreajuste a configuraciones específicas:

\begin{enumerate}
\item \textbf{Grilla de búsqueda:} Se evalúan combinaciones de:
\begin{itemize}
\item \texttt{n\_lags} $\in \{3, 5, 10\}$
\item \texttt{hidden\_size} $\in \{10, 20, 40\}$
\item \texttt{num\_layers} $\in \{1, 2\}$ (opcional, típicamente fijo en 1)
\end{itemize}

\item \textbf{Métrica de selección:} CRPS promedio sobre el conjunto de calibración (40 observaciones posteriores al entrenamiento)

\item \textbf{Congelamiento:} La configuración óptima identificada se congela para toda la fase de evaluación rolling
\end{enumerate}

\paragraph{Configuración Típica Resultante}

Para la mayoría de las series en los escenarios de simulación, la configuración óptima converge a:

\begin{equation}
\{\texttt{n\_lags}=5, \texttt{hidden\_size}=20, \texttt{num\_layers}=1, \texttt{dropout}=0.1\}
\end{equation}

Esta configuración balanceada resulta en aproximadamente $20 \times (20 + 5 + 1) \times 4 = 2080$ parámetros para la capa LSTM (considerando las cuatro gates: input, forget, cell, output), más los parámetros de las capas de salida $\mu$ y $\sigma$, totalizando aproximadamente 2200 parámetros. Con $\sim 150$ instancias de entrenamiento, la relación parámetros/datos es aproximadamente $15:1$, lo cual es manejable con regularización apropiada.

\subsection{Autoregressive Exponentially-weighted Polynomial Distribution (AREPD)}

\subsubsection{Propuesta Teórica}

El modelo \textit{Autoregressive Exponentially-weighted Polynomial Distribution} (AREPD) representa una contribución metodológica original de esta investigación, desarrollada como una extensión híbrida que combina elementos de predicción conformal ponderada con expansión polinomial de características autorregresivas. A diferencia de los métodos puramente conformales como LSPM o LSPMW que utilizan regresión lineal en el espacio original de rezagos, AREPD introduce \textbf{transformaciones no lineales polinomiales} de las entradas autorregresivas, permitiendo capturar relaciones no lineales entre valores pasados y futuros sin recurrir a arquitecturas de aprendizaje profundo.

\paragraph{Motivación: Limitaciones de la Linealidad en Modelos Conformales}

Los métodos conformales basados en mínimos cuadrados, como el LSPM de \textcite{Vovk2022}, asumen implícitamente que la relación entre valores pasados $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-p}$ y el valor futuro $Y_t$ puede aproximarse adecuadamente mediante una función lineal:

\begin{equation}
\mathbb{E}[Y_t \mid \mathcal{F}_{t-1}] \approx \beta_0 + \sum_{j=1}^{p} \beta_j Y_{t-j}
\end{equation}

Esta suposición es válida para procesos lineales estacionarios como ARMA, donde la estructura autorregresiva es inherentemente lineal por construcción. Sin embargo, para procesos no lineales como SETAR, donde la dinámica cambia según el estado del sistema, o para series con efectos de amplificación (donde valores grandes en el pasado producen valores desproporcionadamente grandes en el futuro), la aproximación lineal puede ser inadecuada.

AREPD aborda esta limitación mediante la expansión polinomial del espacio de características. Para cada rezago $Y_{t-j}$, se generan términos polinomiales $Y_{t-j}, Y_{t-j}^2, \ldots, Y_{t-j}^d$ donde $d$ es el grado polinomial. Esto permite que el modelo capture relaciones cuadráticas, cúbicas, o de orden superior entre el pasado y el futuro, manteniendo la estructura de regresión lineal pero en un espacio de características enriquecido.

\paragraph{Fundamento Matemático: Regresión Ridge Ponderada con Expansión Polinomial}

Sea $\{Y_t\}_{t=1}^{n}$ una serie temporal observada. Para un número de rezagos $p$ y un grado polinomial $d$, AREPD construye una matriz de diseño expandida $\mathbf{X} \in \mathbb{R}^{(n-p) \times (1+pd)}$ donde cada fila corresponde a una instancia temporal:

\begin{equation}
\mathbf{X}_i = \left[1, Y_{i}, Y_{i}^2, \ldots, Y_{i}^d, Y_{i+1}, Y_{i+1}^2, \ldots, Y_{i+1}^d, \ldots, Y_{i+p-1}, Y_{i+p-1}^2, \ldots, Y_{i+p-1}^d\right]
\label{eq:arepd_design_matrix}
\end{equation}

para $i = 1, \ldots, n-p$. El vector objetivo correspondiente es:

\begin{equation}
\mathbf{y} = [Y_{p+1}, Y_{p+2}, \ldots, Y_n]^T \in \mathbb{R}^{n-p}
\end{equation}

La inclusión del término constante (1) en la primera posición de cada fila permite que el modelo capture un nivel base no cero, equivalente a un intercepto en regresión lineal estándar.

\paragraph{Ponderación Exponencial Temporal}

Inspirado en el esquema de decaimiento temporal de \textcite{Barber2023} para predicción conformal adaptativa, AREPD asigna pesos exponencialmente decrecientes a las observaciones históricas. Para la observación en el tiempo $t = p+i$, el peso asociado es:

\begin{equation}
w_i = \rho^{n-p-i}, \quad i = 1, \ldots, n-p
\label{eq:arepd_weights}
\end{equation}

donde $\rho \in (0, 1)$ es el \textbf{parámetro de decaimiento}. Los pesos se normalizan para sumar uno:

\begin{equation}
\tilde{w}_i = \frac{w_i}{\sum_{j=1}^{n-p} w_j}
\end{equation}

Este esquema implica que la observación más reciente (tiempo $n$) recibe el peso máximo $\tilde{w}_{n-p} \propto \rho^0 = 1$, mientras que observaciones más antiguas reciben pesos progresivamente menores. El parámetro $\rho$ controla la velocidad del olvido:

\begin{itemize}
\item $\rho \approx 1$: Memoria larga, todas las observaciones tienen pesos similares
\item $\rho \approx 0.8$: Memoria corta, el pasado reciente domina completamente
\item $\rho = 0.95$: Valor intermedio típico que balancea estabilidad y adaptabilidad
\end{itemize}

La vida media efectiva de la información bajo este esquema es:

\begin{equation}
\tau_{1/2} = \frac{\log(2)}{\log(1/\rho)}
\end{equation}

Por ejemplo, con $\rho = 0.95$, $\tau_{1/2} \approx 13.5$ observaciones, lo que significa que una observación de hace 14 pasos temporales contribuye con aproximadamente la mitad del peso de una observación actual.

\paragraph{Estimación mediante Regresión Ridge}

Los coeficientes del modelo se estiman resolviendo el problema de regresión Ridge ponderada:

\begin{equation}
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \left\{\sum_{i=1}^{n-p} \tilde{w}_i \left(y_i - \mathbf{X}_i^T \boldsymbol{\beta}\right)^2 + \lambda \|\boldsymbol{\beta}\|_2^2\right\}
\label{eq:arepd_objective}
\end{equation}

donde $\lambda > 0$ es el parámetro de regularización Ridge (también denotado $\alpha$ en la implementación) que penaliza la norma $L_2$ de los coeficientes. La inclusión de esta regularización es crítica por dos razones:

\begin{enumerate}
\item \textbf{Estabilidad numérica:} La expansión polinomial de orden alto ($d \geq 2$) puede generar matrices de diseño casi singulares debido a la alta correlación entre términos como $Y_{t-1}$ y $Y_{t-1}^2$. La penalización Ridge garantiza que $\mathbf{X}^T\mathbf{W}\mathbf{X} + \lambda\mathbf{I}$ sea bien condicionada.

\item \textbf{Prevención de sobreajuste:} Con $p \cdot d$ características (excluyendo el intercepto), el modelo tiene alta capacidad expresiva. Sin regularización, podría ajustarse al ruido en el conjunto de entrenamiento, especialmente cuando $n-p$ es relativamente pequeño.
\end{enumerate}

La solución del problema \eqref{eq:arepd_objective} tiene forma cerrada:

\begin{equation}
\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^T\mathbf{W}\mathbf{X} + \lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{W}\mathbf{y}
\label{eq:arepd_solution}
\end{equation}

donde $\mathbf{W} = \text{diag}(\tilde{w}_1, \ldots, \tilde{w}_{n-p})$ es la matriz diagonal de pesos. Esta formulación es computacionalmente eficiente y puede resolverse mediante descomposición de Cholesky o SVD para máxima estabilidad numérica.

\paragraph{Generación de Distribuciones Predictivas}

Una vez estimados los coeficientes $\hat{\boldsymbol{\beta}}$, AREPD genera una distribución predictiva mediante un enfoque \textbf{histórico-empírico} que difiere fundamentalmente de los métodos conformales puros. Para predecir el siguiente valor $Y_{n+1}$, el modelo:

\begin{enumerate}
\item Construye el vector de características expandido para todos los puntos históricos:
\begin{equation}
\mathbf{X}_{\text{hist}}[i,:] = \text{PolyExpand}(Y_{i}, Y_{i+1}, \ldots, Y_{i+p-1}), \quad i = 1, \ldots, n-p
\end{equation}

\item Calcula las predicciones puntuales históricas:
\begin{equation}
\hat{Y}_{\text{hist}} = \mathbf{X}_{\text{hist}} \hat{\boldsymbol{\beta}}
\end{equation}

\item Transforma estas predicciones a la escala original:
\begin{equation}
\hat{Y}_{\text{hist}}^{\text{original}} = \hat{Y}_{\text{hist}} \cdot \sigma_{\text{train}} + \mu_{\text{train}}
\end{equation}

\item Utiliza este conjunto $\{\hat{Y}_{\text{hist}}^{\text{original}}\}$ como \textbf{muestra empírica de la distribución predictiva}.
\end{enumerate}

Este enfoque es conceptualmente distinto de la predicción conformal estándar. Mientras que métodos como LSPM calculan scores conformales $C_i = h(x_{\text{test}}) + (y_i - h(x_i))$ que ajustan la predicción puntual mediante residuos de calibración, AREPD construye una distribución empírica directamente a partir de las predicciones históricas del modelo ajustado. La justificación subyacente es que, si el modelo captura adecuadamente la estructura de dependencia temporal, las predicciones sobre datos históricos deberían ser representativas de la distribución predictiva futura bajo el supuesto de estacionariedad local.

\paragraph{Propiedades Teóricas y Garantías de Cobertura}

A diferencia de los métodos conformales con garantías de cobertura finita demostradas formalmente, AREPD \textbf{no posee garantías teóricas de cobertura bajo intercambiabilidad}. La razón fundamental es que la distribución predictiva no se construye mediante el framework conformal estándar, sino mediante un mecanismo histórico-empírico que asume:

\begin{equation}
\hat{Y}_t \mid \mathcal{F}_{t-1} \overset{d}{\approx} \hat{Y}_s \mid \mathcal{F}_{s-1} \quad \forall t, s \in \{p+1, \ldots, n\}
\end{equation}

Esta suposición de distribución estacionaria de las predicciones puede violarse en presencia de:

\begin{itemize}
\item \textbf{No estacionariedad fuerte:} Si la serie exhibe tendencias o cambios estructurales, las predicciones históricas pueden tener distribuciones sistemáticamente diferentes de las predicciones futuras.

\item \textbf{Heteroscedasticidad condicional:} Si la varianza de los errores cambia con el tiempo (como en procesos GARCH), el rango de las predicciones históricas puede no reflejar adecuadamente la incertidumbre futura.

\item \textbf{Eventos raros no observados:} Si el período de entrenamiento no contiene eventos extremos, la distribución empírica subestimará las colas de la verdadera distribución predictiva.
\end{itemize}

A pesar de estas limitaciones teóricas, AREPD puede exhibir buen desempeño empírico en escenarios donde:

\begin{enumerate}
\item La no linealidad del proceso es suave y capturables mediante polinomios de bajo orden ($d \leq 3$)
\item La estacionariedad se mantiene aproximadamente en el horizonte de evaluación
\item El conjunto de entrenamiento es suficientemente largo para que las predicciones históricas cubran el rango de valores futuros plausibles
\end{enumerate}

\paragraph{Comparación Conceptual con Métodos Relacionados}

La Tabla \ref{tab:arepd_conceptual} posiciona AREPD en el espacio de métodos autorregresivos para pronóstico probabilístico.

\begin{table}[htbp]
\centering
\caption{Comparación conceptual de AREPD con métodos relacionados}
\label{tab:arepd_conceptual}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{4.5cm}}
\toprule
\textbf{Método} & \textbf{Espacio de características} & \textbf{Ponderación temporal} & \textbf{Distribución predictiva} \\
\midrule
LSPM & Lineal (rezagos) & Uniforme & Conformal (scores ajustados) \\
\addlinespace
LSPMW & Lineal (rezagos) & Exponencial ($\rho$) & Conformal ponderada \\
\addlinespace
AREPD & Polinomial (hasta grado $d$) & Exponencial ($\rho$) & Histórico-empírica \\
\addlinespace
DeepAR & No lineal (LSTM) & Uniforme (en entrenamiento) & Muestreo ancestral \\
\addlinespace
Sieve Bootstrap & Lineal (orden $p$ creciente) & Uniforme & Bootstrap de residuos \\
\bottomrule
\end{tabular}
\end{table}

AREPD ocupa un nicho intermedio entre métodos conformales lineales simples (LSPM/LSPMW) y arquitecturas de aprendizaje profundo complejas (DeepAR). Su capacidad de capturar no linealidades mediante expansión polinomial lo hace potencialmente más expresivo que LSPM para procesos no lineales, pero sin el costo computacional y los requisitos de datos de DeepAR.

\subsubsection{De la Teoría a la Práctica}

La implementación de AREPD desarrollada para esta investigación traduce el marco teórico en un sistema predictivo concreto mediante las siguientes decisiones de diseño y adaptaciones específicas.

\paragraph{Adaptación 1: Normalización Z-Score Pre-Expansión}

\textbf{Desafío Numérico:} La expansión polinomial es extremadamente sensible a la escala de las variables de entrada. Si una serie tiene valores en el rango $[100, 200]$, los términos cuadráticos estarán en el rango $[10^4, 4 \times 10^4]$ y los términos cúbicos en $[10^6, 8 \times 10^6]$. Esta explosión de escalas causa:

\begin{itemize}
\item \textbf{Inestabilidad numérica:} La matriz $\mathbf{X}^T\mathbf{W}\mathbf{X}$ puede tener número de condición extremadamente alto ($> 10^{10}$), haciendo la inversión matricial numéricamente inestable.

\item \textbf{Dominancia de términos de alto orden:} Los coeficientes asociados a términos cúbicos dominarían la predicción simplemente por su escala, no por su importancia predictiva real.
\end{itemize}

\textbf{Solución Implementada:} Se aplica normalización Z-score \textbf{antes} de la expansión polinomial:

\begin{equation}
\tilde{Y}_t = \frac{Y_t - \mu_{\text{train}}}{\sigma_{\text{train}} + \epsilon}
\end{equation}

donde $\epsilon = 10^{-8}$ previene división por cero. La expansión polinomial se aplica entonces a los valores normalizados $\tilde{Y}_t$, garantizando que todos los términos estén aproximadamente en el rango $[-3, 3]$ bajo normalidad. Las predicciones se des-normalizan al final:

\begin{equation}
Y_t^{\text{pred}} = \tilde{Y}_t^{\text{pred}} \cdot \sigma_{\text{train}} + \mu_{\text{train}}
\end{equation}

Esta transformación es matemáticamente válida porque preserva las relaciones polinomiales:

\begin{equation}
f(\tilde{Y}_{t-1}, \tilde{Y}_{t-2}, \ldots) = g(Y_{t-1}, Y_{t-2}, \ldots) \text{ si } f \text{ y } g \text{ son polinomios}
\end{equation}

aunque con coeficientes reescalados apropiadamente.

\paragraph{Adaptación 2: Construcción Eficiente de la Matriz de Diseño}

\textbf{Implementación Vectorizada:} La construcción de la matriz de diseño en \eqref{eq:arepd_design_matrix} se implementa mediante operaciones vectorizadas de NumPy para eficiencia computacional. El pseudocódigo es:

\begin{verbatim}
X_list = [np.ones((n-p, 1))]  # Término constante

for lag in range(p):
    lagged_values = Y[lag : lag + (n-p)]
    for degree in range(1, d + 1):
        X_list.append(lagged_values ** degree)

X = np.hstack(X_list)
\end{verbatim}

Esta construcción genera una matriz con estructura de bloques:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
\mathbf{1} & \mathbf{Y}_0 & \mathbf{Y}_0^2 & \cdots & \mathbf{Y}_0^d & \mathbf{Y}_1 & \mathbf{Y}_1^2 & \cdots & \mathbf{Y}_{p-1}^d
\end{bmatrix}
\end{equation}

donde $\mathbf{Y}_{\text{lag}}$ denota el vector columna de valores con rezago específico.

\paragraph{Adaptación 3: Uso de scikit-learn Ridge con Sample Weights}

\textbf{Ventaja de Biblioteca Establecida:} En lugar de implementar manualmente la solución de \eqref{eq:arepd_solution}, se utiliza la clase \texttt{Ridge} de scikit-learn con el argumento \texttt{sample\_weight}. Esta elección proporciona:

\begin{itemize}
\item \textbf{Estabilidad numérica:} scikit-learn utiliza descomposición SVD o Cholesky optimizada dependiendo del condicionamiento de la matriz

\item \textbf{Validación automática:} Manejo robusto de casos extremos (matrices singulares, pesos negativos, etc.)

\item \textbf{Eficiencia:} Código optimizado en C/Cython para operaciones matriciales
\end{itemize}

La implementación es directa:

\begin{verbatim}
from sklearn.linear_model import Ridge

model = Ridge(alpha=lambda, fit_intercept=False)
model.fit(X, y, sample_weight=weights)
predictions = model.predict(X)
\end{verbatim}

El argumento \texttt{fit\_intercept=False} es crucial porque el término constante ya está incluido explícitamente como primera columna de $\mathbf{X}$.

\paragraph{Adaptación 4: Protocolo de Congelamiento Consistente}

Siguiendo la filosofía unificada implementada en todos los modelos de este estudio, AREPD adopta un protocolo de congelamiento completo que fija:

\begin{enumerate}
\item \textbf{Parámetros de normalización:} $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$ calculados sobre el conjunto de entrenamiento

\item \textbf{Coeficientes del modelo:} $\hat{\boldsymbol{\beta}}_{\text{frozen}}$ estimados mediante \eqref{eq:arepd_solution} sobre datos de entrenamiento

\item \textbf{Distribución predictiva base:} El conjunto de predicciones históricas transformadas que constituye la distribución empírica
\end{enumerate}

Durante la fase de evaluación rolling, para cada nuevo punto de prueba:

\begin{itemize}
\item Se normalizan los últimos $p$ valores observados usando $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$
\item Se expande polinomialmente hasta grado $d$
\item Se aplica $\hat{\boldsymbol{\beta}}_{\text{frozen}}$ (sin re-estimación)
\item Se retorna la distribución histórica des-normalizada
\end{itemize}

Este protocolo garantiza ausencia de data leakage y permite comparación equitativa con otros métodos bajo condiciones idénticas.

\paragraph{Adaptación 5: Manejo de Casos Degenerados}

\textbf{Series Muy Cortas:} Si $n < 2p$, no hay suficientes observaciones para construir la matriz de diseño. El modelo degrada a un predictor constante:

\begin{equation}
\hat{Y}_{n+1} = \mu_{\text{frozen}}
\end{equation}

con distribución predictiva degenerada (punto de masa en la media).

\textbf{Matriz de Diseño Singular:} Si después de la regularización Ridge la matriz sigue siendo numéricamente singular (número de condición $> 10^{12}$), la predicción falla y se retorna un fallback similar al caso anterior.

\subsubsection{Parámetros e Hiperparámetros en la Implementación}

\paragraph{Hiperparámetros Estructurales}

\textbf{n\_lags ($p$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $p=5$
\item \textit{Rango explorado:} $\{3, 5, 7, 10\}$
\item \textit{Función:} Número de rezagos incluidos en la matriz de diseño
\item \textit{Impacto dimensional:} Con grado polinomial $d$, la dimensión del espacio de características es $1 + p \cdot d$
\item \textit{Trade-off:} Valores mayores permiten capturar dependencias de largo plazo, pero reducen el tamaño del conjunto de entrenamiento ($n - p$ instancias disponibles) e incrementan el riesgo de sobreajuste
\end{itemize}

\textbf{poly\_degree ($d$):}
\begin{itemize}
\item \textit{Tipo:} Entero $\geq 1$
\item \textit{Valor por defecto:} $d=2$ (términos cuadráticos)
\item \textit{Rango explorado:} $\{1, 2, 3\}$
\item \textit{Función:} Grado máximo de la expansión polinomial
\item \textit{Interpretación:}
\begin{itemize}
\item $d=1$: Modelo puramente lineal, equivalente a regresión autorregresiva Ridge estándar
\item $d=2$: Incluye términos cuadráticos, puede capturar efectos de amplificación moderados
\item $d=3$: Incluye términos cúbicos, alta expresividad pero riesgo significativo de sobreajuste
\end{itemize}
\item \textit{Número de parámetros:} $\text{\# params} = 1 + p \cdot d$. Para $(p, d) = (5, 2)$: 11 parámetros; para $(p, d) = (10, 3)$: 31 parámetros.
\end{itemize}

\paragraph{Hiperparámetros de Ponderación y Regularización}

\textbf{rho ($\rho$):}
\begin{itemize}
\item \textit{Tipo:} Real en $(0, 1)$
\item \textit{Valor por defecto:} $\rho=0.95$
\item \textit{Rango explorado:} $\{0.90, 0.95, 0.98\}$
\item \textit{Función:} Parámetro de decaimiento exponencial en el esquema de ponderación temporal definido en \eqref{eq:arepd_weights}
\item \textit{Vida media efectiva:}
\begin{itemize}
\item $\rho = 0.90$: $\tau_{1/2} \approx 6.6$ observaciones
\item $\rho = 0.95$: $\tau_{1/2} \approx 13.5$ observaciones  
\item $\rho = 0.98$: $\tau_{1/2} \approx 34.3$ observaciones
\end{itemize}
\item \textit{Impacto:} Valores altos ($\rho > 0.95$) aproximan ponderación uniforme, útil para series estacionarias estables. Valores bajos ($\rho < 0.90$) permiten adaptación rápida a cambios, apropiado para series con drift o cambios estructurales graduales.
\end{itemize}

\textbf{alpha ($\lambda$):}
\begin{itemize}
\item \textit{Tipo:} Real positivo
\item \textit{Valor por defecto:} $\lambda=0.1$
\item \textit{Función:} Parámetro de regularización Ridge en \eqref{eq:arepd_objective}
\item \textit{Efecto:} Controla el trade-off sesgo-varianza:
\begin{itemize}
\item $\lambda \to 0$: Aproxima mínimos cuadrados no regularizados, máxima flexibilidad pero riesgo de sobreajuste
\item $\lambda \to \infty$: Contrae coeficientes hacia cero, produciendo predictor casi constante
\item $\lambda = 0.1$: Valor intermedio que proporciona regularización suave
\end{itemize}
\item \textit{Selección:} En esta implementación, $\lambda$ se mantiene fijo y no se optimiza. Una extensión futura podría incluir validación cruzada para selección automática de $\lambda$.
\end{itemize}

\paragraph{Configuración Típica Resultante}

Para la mayoría de las series en los escenarios de simulación, la configuración óptima converge a:

\begin{equation}
\{\texttt{n\_lags}=5, \texttt{poly\_degree}=2, \texttt{rho}=0.95, \texttt{alpha}=0.1\}
\end{equation}

Esta configuración genera 11 parámetros a estimar:

\begin{itemize}
\item 1 término constante
\item 5 términos lineales ($Y_{t-1}, Y_{t-2}, \ldots, Y_{t-5}$)
\item 5 términos cuadráticos ($Y_{t-1}^2, Y_{t-2}^2, \ldots, Y_{t-5}^2$)
\end{itemize}

Con $n_{\text{train}} = 200$ observaciones, se dispone de aproximadamente $195$ instancias de entrenamiento, resultando en una relación parámetros/datos de aproximadamente $1:18$, que es favorable para prevenir sobreajuste con la regularización Ridge activa.

\paragraph{Proceso de Selección de Hiperparámetros}

La optimización de hiperparámetros sigue un protocolo de búsqueda en rejilla sobre el espacio:

\begin{equation}
\mathcal{H} = \{(p, d, \rho) : p \in \{3, 5, 7\}, d \in \{1, 2, 3\}, \rho \in \{0.90, 0.95, 0.98\}\}
\end{equation}

Para cada combinación:

\begin{enumerate}
\item Se entrena el modelo Ridge ponderado sobre el conjunto de entrenamiento
\item Se calculan predicciones sobre el conjunto de calibración (40 observaciones)
\item Se evalúa el CRPS promedio como métrica de selección
\item Se retiene la configuración con menor CRPS
\end{enumerate}

La configuración óptima se congela para toda la evaluación rolling posterior.

\paragraph{Consideraciones sobre Interpretabilidad}

A diferencia de arquitecturas de caja negra como DeepAR, AREPD mantiene cierta interpretabilidad debido a su estructura de regresión lineal en el espacio polinomial expandido. Los coeficientes $\hat{\boldsymbol{\beta}}$ pueden inspeccionarse para identificar:

\begin{itemize}
\item \textbf{Rezagos más influyentes:} Rezagos con coeficientes lineales de gran magnitud ($|\beta_{j}| > \text{umbral}$)
\item \textbf{Presencia de no linealidades:} Coeficientes cuadráticos o cúbicos significativamente diferentes de cero indican relaciones no lineales
\item \textbf{Dirección de dependencias:} Signo positivo/negativo de coeficientes lineales indica correlación positiva/negativa con rezagos específicos
\end{itemize}

Esta transparencia puede ser valiosa en aplicaciones donde la comprensión del modelo es importante, como pronóstico de demanda o planificación operativa.

\subsubsection{Posicionamiento Metodológico y Limitaciones}

\paragraph{Ventajas Potenciales de AREPD}

\begin{enumerate}
\item \textbf{Flexibilidad no lineal:} Puede capturar relaciones polinomiales sin arquitecturas complejas de aprendizaje profundo. Para procesos como SETAR con cambios de régimen suaves, la expansión cuadrática puede aproximar razonablemente las transiciones entre regímenes.

\item \textbf{Eficiencia computacional:} El entrenamiento se reduce a resolver un sistema lineal, significativamente más rápido que optimizar redes neuronales. Típicamente requiere menos de 1 segundo por serie en hardware estándar.

\item \textbf{Adaptabilidad temporal:} El parámetro $\rho$ permite ajustar la memoria del modelo sin cambiar la arquitectura, útil para series con diferentes grados de estacionariedad local.

\item \textbf{Regularización explícita:} El parámetro Ridge $\lambda$ proporciona control directo sobre el trade-off sesgo-varianza, más transparente que técnicas como dropout en redes neuronales.

\item \textbf{Interpretabilidad parcial:} Los coeficientes pueden inspeccionarse para entender qué patrones temporales captura el modelo, algo imposible en arquitecturas de caja negra.
\end{enumerate}

\paragraph{Limitaciones Fundamentales}

\begin{enumerate}
\item \textbf{Ausencia de garantías conformales:} El mecanismo de construcción de la distribución predictiva no está fundamentado en teoría conformal, por lo que no hereda las garantías de cobertura finita de métodos como LSPM. La cobertura empírica depende críticamente de la validez del supuesto de estacionariedad de las predicciones históricas.

\item \textbf{Maldición de la dimensionalidad polinomial:} Para grados altos ($d \geq 4$) y múltiples rezagos ($p > 10$), el número de parámetros crece como $O(pd)$, haciendo el modelo propenso a sobreajuste incluso con regularización Ridge. Para $(p, d) = (15, 4)$, se requerirían estimar 61 parámetros, lo cual es excesivo para series de longitud moderada.

\item \textbf{Inestabilidad en extrapolación:} Los polinomios de orden alto son notoriamente inestables fuera del rango de los datos de entrenamiento. Si la serie de prueba contiene valores extremos no observados durante el entrenamiento, los términos cúbicos o de orden superior pueden producir predicciones arbitrariamente grandes o pequeñas.

\item \textbf{Incapacidad para cambios estructurales abruptos:} A diferencia del MCPS que puede adaptarse localmente mediante bins, o DeepAR que aprende patrones complejos, AREPD asume que la estructura polinomial aprendida es globalmente válida. Cambios de régimen abruptos (como en SETAR con umbrales nítidos) pueden no ser bien capturados por aproximaciones polinomiales suaves.

\item \textbf{Dependencia de normalización:} El rendimiento del método depende críticamente de la normalización Z-score. Series con outliers extremos pueden sesgar $\mu_{\text{train}}$ y $\sigma_{\text{train}}$, degradando la estabilidad numérica de la expansión polinomial.

\item \textbf{Distribución predictiva histórica:} El uso de predicciones históricas como distribución predictiva asume implícitamente que los errores del modelo son homocedásticos (varianza constante) y estacionarios. Esto puede ser inadecuado para series con volatilidad cambiante o errores dependientes del nivel.
\end{enumerate}

\paragraph{Casos de Uso Recomendados}

Basándose en las características teóricas, AREPD es potencialmente más apropiado para:

\begin{itemize}
\item Series temporales con no linealidades suaves capturables mediante polinomios de bajo orden ($d \leq 3$)
\item Escenarios donde se requiere interpretabilidad básica de la estructura de dependencia
\item Situaciones donde el costo computacional es una restricción severa
\item Procesos estacionarios o localmente estacionarios sin cambios estructurales abruptos
\end{itemize}

Y menos apropiado para:

\begin{itemize}
\item Series con cambios de régimen nítidos o discontinuidades
\item Procesos con heterocedasticidad condicional fuerte (volatilidad cambiante)
\item Situaciones donde garantías formales de cobertura son requeridas
\item Series con valores extremos frecuentes que violan la aproximación polinomial
\end{itemize}

\paragraph{Innovación Metodológica}

La contribución de AREPD no reside en sus garantías teóricas (que son limitadas), sino en demostrar empíricamente si la combinación de:

\begin{equation}
\text{Expansión Polinomial} + \text{Ponderación Temporal} + \text{Regularización Ridge}
\end{equation}

puede proporcionar un balance efectivo entre expresividad no lineal y simplicidad computacional en el contexto de pronóstico probabilístico. Los resultados experimentales del Capítulo~\ref{cap:resultados} permitirán cuantificar si esta estrategia híbrida ofrece ventajas sobre métodos puramente lineales (LSPM/LSPMW) o si la complejidad adicional no se traduce en mejoras de desempeño medidas por ECRPS.

En caso de que AREPD exhiba desempeño competitivo en escenarios no lineales como SETAR, esto proporcionaría evidencia de que aproximaciones polinomiales simples pueden ser suficientes para ciertas clases de no linealidades en series temporales, sin necesidad de recurrir a métodos conformales localmente adaptativos (MCPS) o arquitecturas de aprendizaje profundo (DeepAR). Por el contrario, si su desempeño es inferior, esto validaría la necesidad de enfoques más sofisticados con fundamentación teórica más robusta.

\subsection{Ensemble Conformalized Quantile Regression con LSTM (EnCQR-LSTM)}

\subsubsection{Fundamento Teórico}

El modelo \textit{Ensemble Conformalized Quantile Regression} (EnCQR) con arquitectura LSTM representa una aproximación híbrida que combina regresión cuantílica mediante redes neuronales recurrentes, aprendizaje en ensamble, y predicción conformal para construir intervalos de predicción (PIs) probabilísticos con garantías de cobertura para series temporales heterocedásticas \parencite{Jensen2022}.

\paragraph{Motivación: Limitaciones de Métodos Puros}

Los métodos basados únicamente en regresión cuantílica (QR), aunque pueden generar intervalos adaptativos que ajustan su amplitud a la variabilidad local de los datos, carecen de garantías formales de cobertura. En la práctica, los PIs generados por QR tienden a ser excesivamente confiados (demasiado estrechos), resultando en coberturas empíricas significativamente inferiores al nivel de confianza nominal $(1-\alpha)$.

Por otro lado, los métodos de predicción conformal (CP) estándar garantizan cobertura marginal válida bajo el supuesto de intercambiabilidad de los datos. Sin embargo, CP tradicional construye intervalos de amplitud constante o levemente variable, lo cual es inadecuado para series temporales con heterocedasticidad, donde la incertidumbre varía considerablemente a lo largo del tiempo. Para datos con variabilidad cambiante, estos intervalos constantes resultan:

\begin{itemize}
\item \textbf{Excesivamente conservadores} en períodos de baja volatilidad, generando intervalos innecesariamente amplios que proporcionan poca información útil
\item \textbf{Potencialmente insuficientes} en períodos de alta volatilidad, donde la amplitud fija puede no capturar adecuadamente la incertidumbre real
\end{itemize}

EnCQR aborda estas limitaciones mediante una síntesis metodológica que hereda las fortalezas de ambos enfoques: la adaptabilidad local de QR y las garantías de cobertura de CP.

\paragraph{Arquitectura de Ensamble y Partición de Datos}

EnCQR construye un ensamble homogéneo de $B$ aprendices base, cada uno entrenado sobre subconjuntos \textbf{disjuntos} de los datos de entrenamiento. Formalmente, dado un conjunto de entrenamiento $\{(x_i, y_i)\}_{i=1}^{T}$, se particiona en $B$ subconjuntos consecutivos no solapados:

\begin{equation}
S_b = \{(x_i, y_i) : i \in [(b-1)T_b + 1, bT_b]\}, \quad b = 1, \ldots, B
\end{equation}

donde $T_b = \lfloor T/B \rfloor$ es la longitud de cada subconjunto. Esta partición disjunta es fundamental para la construcción de residuos fuera de muestra válidos, ya que garantiza que cada observación está excluida de al menos un modelo del ensamble.

Cada subconjunto $S_b$ se utiliza para entrenar un modelo LSTM de regresión cuantílica que estima simultáneamente múltiples funciones cuantílicas condicionales (CQFs):

\begin{equation}
\hat{q}_{\tau}^{(b)}(x) = \text{LSTM}_b(x; \theta_b), \quad \tau \in \{\tau_{\text{lo}}, 0.50, \tau_{\text{hi}}\}
\end{equation}

donde $\theta_b$ representa los parámetros del $b$-ésimo modelo LSTM, y los niveles cuantílicos $\tau_{\text{lo}}$ y $\tau_{\text{hi}}$ definen los límites inferior y superior del intervalo nominal. Típicamente, para un nivel de confianza $(1-\alpha)$, se establecen valores iniciales como $\tau_{\text{lo}} = \alpha/2$ y $\tau_{\text{hi}} = 1 - \alpha/2$.

\paragraph{Regresión Cuantílica mediante Pinball Loss}

El entrenamiento de cada modelo LSTM se realiza minimizando la función de pérdida pinball agregada sobre todos los cuantiles objetivo:

\begin{equation}
\mathcal{L}_{\text{pinball}}(\theta_b) = \frac{1}{|S_b| \cdot |\mathcal{T}|} \sum_{(x_i, y_i) \in S_b} \sum_{\tau \in \mathcal{T}} \rho_{\tau}(y_i - \hat{q}_{\tau}^{(b)}(x_i))
\label{eq:encqr_pinball}
\end{equation}

donde $\mathcal{T} = \{\tau_{\text{lo}}, 0.50, \tau_{\text{hi}}\}$ es el conjunto de cuantiles objetivo, y $\rho_{\tau}(\cdot)$ es la función de pérdida pinball definida como:

\begin{equation}
\rho_{\tau}(u) = \begin{cases}
\tau \cdot u, & \text{si } u \geq 0 \\
(\tau - 1) \cdot u, & \text{si } u < 0
\end{cases}
\end{equation}

Esta función penaliza asimétricamente los errores de predicción: para cuantiles superiores ($\tau > 0.5$), las subestimaciones reciben mayor penalización, mientras que para cuantiles inferiores ($\tau < 0.5$), las sobreestimaciones son más penalizadas. Esta asimetría es crucial para que el modelo aprenda a estimar correctamente los cuantiles condicionales de la distribución de $Y|X$.

\paragraph{Predicción Leave-One-Out del Ensamble}

Una vez entrenados los $B$ modelos, EnCQR construye predicciones leave-one-out (LOO) para cada observación de entrenamiento. Para una observación $i$ en el subconjunto $S_b$, se agregan las predicciones de todos los modelos entrenados \textbf{sin} incluir esa observación:

\begin{equation}
\hat{q}_{\tau}^{(-i)}(x_i) = \phi\left(\{\hat{q}_{\tau}^{(b')}(x_i) : S_{b'} \text{ tal que } i \notin S_{b'}\}\right)
\label{eq:encqr_loo}
\end{equation}

donde $\phi(\cdot)$ es una función de agregación, típicamente la media aritmética. Este procedimiento LOO es esencial para aplicar predicción conformal a series temporales, ya que reemplaza el requisito de intercambiabilidad por un esquema de validación cruzada que genera residuos genuinamente fuera de muestra.

\paragraph{Scores de Conformidad Asimétricos}

EnCQR introduce scores de conformidad \textbf{asimétricos} que cuantifican separadamente el error de cobertura en las colas inferior y superior de la distribución:

\begin{equation}
\begin{aligned}
E_i^{\text{lo}} &= \hat{q}_{\tau_{\text{lo}}}^{(-i)}(x_i) - y_i \\
E_i^{\text{hi}} &= y_i - \hat{q}_{\tau_{\text{hi}}}^{(-i)}(x_i)
\end{aligned}
\label{eq:encqr_scores}
\end{equation}

para $i = 1, \ldots, T$. La motivación para esta formulación asimétrica es que las distribuciones de los errores para los cuantiles inferior y superior pueden tener formas diferentes, especialmente en presencia de asimetría en los errores del modelo. Si se utilizara un score simétrico único (como en CP estándar), un error de cobertura asimétrico podría distribuirse incorrectamente entre las colas, resultando en cobertura inferior al nivel nominal.

Los scores $\{E_i^{\text{lo}}\}_{i=1}^{T}$ y $\{E_i^{\text{hi}}\}_{i=1}^{T}$ forman dos distribuciones empíricas que cuantifican cuánto debe expandirse cada límite del intervalo para garantizar la cobertura deseada.

\paragraph{Conformalización de Intervalos de Predicción}

Para una nueva observación $x_{T+1}$, el ensamble completo genera predicciones agregadas:

\begin{equation}
\begin{aligned}
\hat{q}_{\tau_{\text{lo}}}(x_{T+1}) &= \phi\left(\{\hat{q}_{\tau_{\text{lo}}}^{(b)}(x_{T+1})\}_{b=1}^{B}\right) \\
\hat{q}_{\tau_{\text{hi}}}(x_{T+1}) &= \phi\left(\{\hat{q}_{\tau_{\text{hi}}}^{(b)}(x_{T+1})\}_{b=1}^{B}\right)
\end{aligned}
\end{equation}

Estos cuantiles agregados se conformalizan ajustando su amplitud mediante los cuantiles $(1-\alpha)$ de las distribuciones de scores:

\begin{equation}
\hat{C}_{\alpha}(x_{T+1}) = \left[\hat{q}_{\tau_{\text{lo}}}(x_{T+1}) - \omega^{\text{lo}}, \hat{q}_{\tau_{\text{hi}}}(x_{T+1}) + \omega^{\text{hi}}\right]
\label{eq:encqr_interval}
\end{equation}

donde:

\begin{equation}
\begin{aligned}
\omega^{\text{lo}} &= Q_{1-\alpha}(\{E_i^{\text{lo}}\}_{i=1}^{T}) \\
\omega^{\text{hi}} &= Q_{1-\alpha}(\{E_i^{\text{hi}}\}_{i=1}^{T})
\end{aligned}
\end{equation}

y $Q_{1-\alpha}(\cdot)$ denota el cuantil empírico $(1-\alpha)$ de una colección de valores.

Este mecanismo de conformalización garantiza que, bajo el supuesto de que los errores residuales $\{E_i^{\text{lo}}, E_i^{\text{hi}}\}$ son aproximadamente estacionarios y fuertemente mezclantes (condiciones más débiles que intercambiabilidad), el intervalo $\hat{C}_{\alpha}(x_{T+1})$ satisface:

\begin{equation}
\mathbb{P}\{Y_{T+1} \in \hat{C}_{\alpha}(X_{T+1})\} \geq 1 - \alpha + O(1/T)
\end{equation}

con alta probabilidad para muestras grandes.

\paragraph{Adaptación Temporal mediante Ventana Deslizante}

Para series temporales no estacionarias, EnCQR incorpora un mecanismo de actualización mediante ventana deslizante. Cada $s$ nuevas observaciones (donde $s$ corresponde típicamente a un ciclo estacional, e.g., 24 horas para datos horarios), los scores de conformidad se actualizan:

\begin{enumerate}
\item Se calculan los nuevos residuos LOO para las últimas $s$ observaciones
\item Se eliminan los $s$ residuos más antiguos de las colecciones $\{E_i^{\text{lo}}\}$ y $\{E_i^{\text{hi}}\}$
\item Se incorporan los nuevos residuos, manteniendo constante el tamaño total de las colecciones
\end{enumerate}

Este protocolo de actualización permite que los factores de conformalización $\omega^{\text{lo}}$ y $\omega^{\text{hi}}$ se adapten a cambios graduales en la variabilidad de la serie, manteniendo la cobertura válida sin necesidad de reentrenar los modelos LSTM del ensamble.

\paragraph{Propiedades Teóricas}

EnCQR posee las siguientes propiedades formales:

\begin{enumerate}
\item \textbf{Cobertura marginal aproximadamente válida:} Bajo el supuesto de que el proceso de error es estacionario y fuertemente mezclante, EnCQR garantiza que la cobertura marginal converge al nivel nominal $(1-\alpha)$ cuando $T \to \infty$.

\item \textbf{Adaptabilidad heterocedástica:} A diferencia de CP estándar, la amplitud del intervalo en \eqref{eq:encqr_interval} varía localmente con $x_{T+1}$ a través de las predicciones cuantílicas $\hat{q}_{\tau}(x_{T+1})$, permitiendo intervalos más estrechos en regiones de baja variabilidad.

\item \textbf{Distribución-libre:} No se asumen formas paramétricas específicas para la distribución de $Y|X$. La única suposición estructural es la estacionariedad débil del proceso de error.

\item \textbf{Robustez ante especificación incorrecta:} Incluso si el modelo LSTM subyacente está mal especificado, la conformalización garantiza cobertura válida, aunque los intervalos pueden ser más amplios de lo necesario.
\end{enumerate}

\subsubsection{Arquitectura LSTM para Regresión Cuantílica}

\paragraph{Estructura de Red Neuronal Recurrente}

La arquitectura LSTM utilizada en EnCQR consiste en una secuencia de capas recurrentes LSTM seguidas de una capa densa completamente conectada que produce las estimaciones cuantílicas. Formalmente, para una ventana temporal de entrada $\mathbf{x}_t = [y_{t-N_x}, y_{t-N_x+1}, \ldots, y_{t-1}] \in \mathbb{R}^{N_x}$, donde $N_x$ es la longitud de la ventana, la red procesa:

\begin{equation}
\begin{aligned}
\mathbf{h}_t^{(1)} &= \text{LSTM}^{(1)}(\mathbf{x}_t, \mathbf{h}_{t-1}^{(1)}) \\
\mathbf{h}_t^{(2)} &= \text{LSTM}^{(2)}(\mathbf{h}_t^{(1)}, \mathbf{h}_{t-1}^{(2)}) \\
&\vdots \\
\mathbf{h}_t^{(L)} &= \text{LSTM}^{(L)}(\mathbf{h}_t^{(L-1)}, \mathbf{h}_{t-1}^{(L)}) \\
\hat{\mathbf{q}}_t &= \mathbf{W}_{\text{out}} \mathbf{h}_t^{(L)} + \mathbf{b}_{\text{out}}
\end{aligned}
\end{equation}

donde $L$ es el número de capas LSTM apiladas, $\mathbf{h}_t^{(\ell)}$ representa el estado oculto de la $\ell$-ésima capa en el tiempo $t$, y $\hat{\mathbf{q}}_t = [\hat{q}_{\tau_{\text{lo}}}, \hat{q}_{0.50}, \hat{q}_{\tau_{\text{hi}}}]^T \in \mathbb{R}^3$ es el vector de cuantiles estimados.

\paragraph{Mecanismo de Estados LSTM}

Cada celda LSTM mantiene dos estados: el estado oculto $\mathbf{h}_t$ y el estado de celda $\mathbf{c}_t$, actualizados mediante:

\begin{equation}
\begin{aligned}
\mathbf{f}_t &= \sigma(\mathbf{W}_f[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) \quad \text{(forget gate)} \\
\mathbf{i}_t &= \sigma(\mathbf{W}_i[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) \quad \text{(input gate)} \\
\tilde{\mathbf{c}}_t &= \tanh(\mathbf{W}_c[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) \quad \text{(candidate values)} \\
\mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \quad \text{(cell state update)} \\
\mathbf{o}_t &= \sigma(\mathbf{W}_o[\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) \quad \text{(output gate)} \\
\mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t) \quad \text{(hidden state)}
\end{aligned}
\end{equation}

donde $\sigma(\cdot)$ es la función sigmoide, $\odot$ denota multiplicación elemento a elemento, y $[\cdot, \cdot]$ representa concatenación. Esta arquitectura permite que el modelo capture dependencias temporales de largo plazo, esencial para series con memoria extendida.

\paragraph{Normalización y Regularización}

La arquitectura incluye:

\begin{itemize}
\item \textbf{Normalización MinMax:} Los datos de entrada se escalan al rango $[0, 1]$ mediante:
\begin{equation}
\tilde{y}_t = \frac{y_t - y_{\min}}{y_{\max} - y_{\min}}
\end{equation}
preservando $\mu_{\text{train}}, \sigma_{\text{train}}$ para des-normalización posterior.

\item \textbf{Regularización L2:} Se aplica penalización Ridge a todos los pesos de la red:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{pinball}} + \lambda_2 \sum_{\ell=1}^{L} \|\mathbf{W}^{(\ell)}\|_F^2
\end{equation}
donde $\|\cdot\|_F$ denota la norma de Frobenius.

\item \textbf{Dropout:} Con probabilidad $p_{\text{drop}} = 0.1$ después de cada capa LSTM (excepto la última) para prevenir sobreajuste.
\end{itemize}

\subsubsection{Generación de Distribuciones Predictivas}

\paragraph{Ajuste de Distribución Skew-Normal}

Una vez obtenidos los cuantiles conformalizados $\hat{q}_{\tau}^{\text{conf}} = [\hat{q}_{0.01}, \hat{q}_{0.05}, \ldots, \hat{q}_{0.99}]$ para múltiples niveles $\tau$, EnCQR construye una distribución predictiva continua ajustando una distribución \textbf{Skew-Normal} paramétrica a estos cuantiles.

La elección de la distribución Skew-Normal está motivada por:

\begin{enumerate}
\item \textbf{Flexibilidad asimétrica:} Puede representar distribuciones simétricas (cuando el parámetro de asimetría $\alpha = 0$, reduciendo a Normal) o asimétricas, común en series temporales reales.

\item \textbf{Unimodalidad garantizada:} A diferencia de interpolaciones lineales entre cuantiles que pueden producir distribuciones bimodales artificiales, la Skew-Normal es unimodal por construcción.

\item \textbf{Parsimonia:} Requiere solo tres parámetros $(\mu, \sigma, \alpha)$ correspondientes a localización, escala y asimetría.
\end{enumerate}

La función de densidad de la distribución Skew-Normal es:

\begin{equation}
f(x; \mu, \sigma, \alpha) = \frac{2}{\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) \Phi\left(\alpha \frac{x - \mu}{\sigma}\right)
\end{equation}

donde $\phi(\cdot)$ y $\Phi(\cdot)$ son la función de densidad y distribución acumulada de la Normal estándar, respectivamente.

\paragraph{Estimación de Parámetros}

Los parámetros $(\mu, \sigma, \alpha)$ se estiman minimizando la distancia cuadrática entre los cuantiles empíricos conformalizados y los cuantiles teóricos de la Skew-Normal:

\begin{equation}
(\hat{\mu}, \hat{\sigma}, \hat{\alpha}) = \arg\min_{(\mu, \sigma, \alpha)} \sum_{i=1}^{|\mathcal{T}|} \left(\hat{q}_{\tau_i}^{\text{conf}} - F^{-1}_{\text{SN}}(\tau_i; \mu, \sigma, \alpha)\right)^2
\label{eq:encqr_skewnorm_fit}
\end{equation}

donde $F^{-1}_{\text{SN}}(\tau; \mu, \sigma, \alpha)$ es la función cuantílica de la Skew-Normal. Esta optimización se resuelve mediante el método L-BFGS-B con restricciones en el dominio:

\begin{equation}
\begin{aligned}
\mu &\in [\min(\hat{q}_{\tau}^{\text{conf}}) - \text{IQR}, \max(\hat{q}_{\tau}^{\text{conf}}) + \text{IQR}] \\
\sigma &\in [\text{IQR}/10, 3 \cdot \text{IQR}], \quad \sigma > 0 \\
\alpha &\in [-5, 5]
\end{aligned}
\end{equation}

donde $\text{IQR} = \hat{q}_{0.75}^{\text{conf}} - \hat{q}_{0.25}^{\text{conf}}$ es el rango intercuartil.

\paragraph{Inicialización Robusta}

Los valores iniciales para la optimización se establecen como:

\begin{equation}
\begin{aligned}
\mu_0 &= \hat{q}_{0.50}^{\text{conf}} \quad \text{(mediana empírica)} \\
\sigma_0 &= \text{IQR}/1.35 \quad \text{(aproximación Normal estándar)} \\
\alpha_0 &= 0 \quad \text{(iniciar simétrico)}
\end{aligned}
\end{equation}

Si la optimización no converge, se utiliza un \textit{fallback} a distribución Normal pura ($\alpha = 0$) con parámetros estimados mediante momentos muestrales.

\paragraph{Muestreo de la Distribución Predictiva}

Una vez estimada la distribución Skew-Normal $\text{SN}(\hat{\mu}, \hat{\sigma}, \hat{\alpha})$, se generan $M = 1000$ muestras mediante:

\begin{equation}
\tilde{Y}_{T+1}^{(m)} \sim \text{SN}(\hat{\mu}, \hat{\sigma}, \hat{\alpha}), \quad m = 1, \ldots, M
\end{equation}

Este conjunto de muestras $\{\tilde{Y}_{T+1}^{(m)}\}_{m=1}^{M}$ constituye una representación empírica de la distribución predictiva completa, permitiendo:

\begin{itemize}
\item Cálculo de cualquier cuantil predictivo mediante interpolación
\item Estimación de momentos (media, varianza, asimetría, curtosis)
\item Evaluación de métricas probabilísticas como CRPS
\item Visualización de la densidad predictiva mediante histogramas o KDE
\end{itemize}

\subsubsection{Parámetros e Hiperparámetros}

\paragraph{Hiperparámetros de Arquitectura LSTM}

\textbf{n\_lags ($N_x$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $N_x = 20$
\item \textit{Rango explorado:} $\{10, 20, 30, 40\}$
\item \textit{Función:} Longitud de la ventana temporal de entrada
\item \textit{Impacto:} Valores mayores permiten capturar dependencias de más largo plazo, pero reducen el número efectivo de muestras de entrenamiento a $T - N_x$ y aumentan la complejidad computacional
\end{itemize}

\textbf{units ($N_u$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $N_u = 32$
\item \textit{Rango explorado:} $\{16, 32, 64, 128\}$
\item \textit{Función:} Número de unidades (dimensión del estado oculto) en cada capa LSTM
\item \textit{Capacidad expresiva:} Determina la cantidad de memoria y capacidad de representación. Valores altos incrementan expresividad pero riesgo de sobreajuste
\end{itemize}

\textbf{n\_layers ($L$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $L = 2$
\item \textit{Rango explorado:} $\{1, 2, 3\}$
\item \textit{Función:} Número de capas LSTM apiladas
\item \textit{Interpretación:} Capas adicionales permiten representaciones jerárquicas a diferentes escalas temporales, pero incrementan significativamente el número de parámetros
\end{itemize}

\paragraph{Hiperparámetros de Entrenamiento}

\textbf{lr ($\eta$):}
\begin{itemize}
\item \textit{Tipo:} Real positivo
\item \textit{Valor por defecto:} $\eta = 0.005$
\item \textit{Rango explorado:} $[10^{-4}, 10^{-2}]$ en escala logarítmica
\item \textit{Función:} Tasa de aprendizaje del optimizador Adam
\item \textit{Trade-off:} Valores altos aceleran convergencia pero pueden causar inestabilidad; valores bajos requieren más épocas pero convergen más suavemente
\end{itemize}

\textbf{batch\_size ($B_{\text{train}}$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $B_{\text{train}} = 16$
\item \textit{Rango explorado:} $\{8, 16, 32, 64\}$
\item \textit{Función:} Tamaño de lotes para entrenamiento por descenso de gradiente estocástico
\item \textit{Impacto:} Lotes pequeños proporcionan actualizaciones ruidosas que pueden ayudar a escapar mínimos locales; lotes grandes dan estimaciones más estables pero requieren más memoria
\end{itemize}

\textbf{epochs:}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $20$ (con early stopping)
\item \textit{Función:} Número máximo de pasadas sobre el conjunto de entrenamiento
\item \textit{Early stopping:} El entrenamiento se detiene si la pérdida de validación no mejora durante 50 épocas consecutivas, previniendo sobreajuste
\end{itemize}

\paragraph{Hiperparámetros de Ensamble y Conformalización}

\textbf{B (número de modelos):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $B = 3$
\item \textit{Función:} Número de aprendices en el ensamble
\item \textit{Trade-off:} Valores mayores incrementan diversidad y robustez, pero reducen el tamaño de cada subconjunto de entrenamiento $T_b = T/B$ y aumentan el costo computacional linealmente
\item \textit{Justificación:} Para series de longitud $T \approx 200-500$, $B=3$ proporciona balance adecuado, generando subconjuntos de $\approx 65-165$ observaciones
\end{itemize}

\textbf{alpha ($\alpha$):}
\begin{itemize}
\item \textit{Tipo:} Real en $(0, 1)$
\item \textit{Valor por defecto:} $\alpha = 0.05$ (nivel de confianza 95\%)
\item \textit{Función:} Nivel de error nominal para los intervalos de predicción
\item \textit{Interpretación:} Determina el cuantil $(1-\alpha)$ de los scores de conformidad utilizados en la conformalización
\end{itemize}

\textbf{num\_samples ($M$):}
\begin{itemize}
\item \textit{Tipo:} Entero positivo
\item \textit{Valor por defecto:} $M = 1000$
\item \textit{Función:} Número de muestras generadas de la distribución Skew-Normal ajustada
\item \textit{Impacto:} Determina la resolución de la distribución predictiva empírica. Valores $M \geq 1000$ proporcionan representaciones suficientemente finas para la mayoría de métricas
\end{itemize}

\paragraph{Regularización}

\textbf{Lambda L2 ($\lambda_2$):}
\begin{itemize}
\item \textit{Rango explorado:} $[10^{-5}, 10^{-1}]$ en escala logarítmica
\item \textit{Función:} Penalización Ridge aplicada a todos los pesos de la red
\item \textit{Efecto:} Controla el trade-off sesgo-varianza, previniendo que los pesos crezcan excesivamente y causando sobreajuste
\end{itemize}

\textbf{Dropout:}
\begin{itemize}
\item \textit{Valor fijo:}
\item \textit{Valor fijo:} $p_{\text{drop}} = 0.1$
\item \textit{Función:} Probabilidad de desactivación aleatoria de unidades LSTM durante el entrenamiento
\item \textit{Efecto:} Regularización implícita que fuerza al modelo a aprender representaciones redundantes y robustas
\end{itemize}

\paragraph{Hiperparámetros de Cuantiles}

\textbf{Cuantiles objetivo:}
\begin{itemize}
\item \textit{Conjunto completo:} $\mathcal{T} = \{0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99\}$
\item \textit{Función:} Niveles cuantílicos estimados simultáneamente por cada LSTM
\item \textit{Justificación:} Esta granularidad captura adecuadamente la forma de la distribución predictiva, incluyendo colas extremas (0.01, 0.99) para detectar valores atípicos, cuartiles (0.25, 0.75) para caracterizar dispersión central, y mediana (0.50) para la tendencia central
\end{itemize}

\textbf{Niveles conformalizados optimizables:}
\begin{itemize}
\item $\tau_{\text{lo}} \in [0.01, 0.20]$ y $\tau_{\text{hi}} \in [0.70, 0.99]$
\item Estos niveles nominales del modelo LSTM subyacente pueden ajustarse como hiperparámetros adicionales para compensar sesgos sistemáticos antes de la conformalización
\item La conformalización posterior garantiza cobertura válida independientemente de estos valores
\end{itemize}

\paragraph{Configuración Típica y Número de Parámetros}

Para la configuración estándar con $N_x = 20$, $N_u = 32$, $L = 2$ capas LSTM, el número total de parámetros entrenables por modelo es aproximadamente:

\begin{equation}
\begin{aligned}
\text{\# params} &= \sum_{\ell=1}^{L} \left[4N_u(N_u + d_{\text{in}}^{(\ell)} + 1)\right] + (N_u \cdot |\mathcal{T}| + |\mathcal{T}|) \\
&\approx 4 \cdot 32 \cdot (32 + 1 + 1) + 4 \cdot 32 \cdot (32 + 32 + 1) + 32 \cdot 9 + 9 \\
&\approx 4{,}352 + 8{,}320 + 288 + 9 \approx 12{,}969 \text{ parámetros}
\end{aligned}
\end{equation}

donde $d_{\text{in}}^{(1)} = 1$ para la primera capa (entrada univariada) y $d_{\text{in}}^{(\ell)} = N_u$ para capas superiores. El factor 4 corresponde a las cuatro matrices de pesos de cada celda LSTM (forget, input, candidate, output gates).

Con $B = 3$ modelos en el ensamble, el sistema completo requiere entrenar $\approx 39{,}000$ parámetros totales, lo cual es manejable para series de longitud $T \geq 200$.

\subsubsection{Protocolo de Congelamiento y Evaluación}

\paragraph{Fase de Entrenamiento y Calibración}

El protocolo de congelamiento de EnCQR-LSTM sigue una estructura de dos etapas:

\begin{enumerate}
\item \textbf{Entrenamiento del ensamble:} Los $B = 3$ modelos LSTM se entrenan sobre sus respectivos subconjuntos disjuntos $S_1, S_2, S_3$ del conjunto de entrenamiento, minimizando la pérdida pinball \eqref{eq:encqr_pinball} mediante el optimizador Adam con los hiperparámetros seleccionados.

\item \textbf{Cálculo de scores de conformidad:} Se generan predicciones LOO para todas las observaciones de entrenamiento según \eqref{eq:encqr_loo}, y se calculan los scores asimétricos \eqref{eq:encqr_scores}. Las colecciones $\{E_i^{\text{lo}}\}_{i=1}^{T}$ y $\{E_i^{\text{hi}}\}_{i=1}^{T}$ se almacenan para uso posterior.

\item \textbf{Congelamiento completo:} Se fijan permanentemente:
\begin{itemize}
\item Los pesos $\{\theta_b\}_{b=1}^{B}$ de los $B$ modelos LSTM
\item Los parámetros de normalización $(\mu_{\text{train}}, \sigma_{\text{train}})$ o $(y_{\min}, y_{\max})$
\item Las distribuciones empíricas de scores conformales
\item El tamaño de ventana de actualización $s = 24$
\end{itemize}
\end{enumerate}

\paragraph{Evaluación Rolling Window}

Durante la fase de evaluación, para cada nuevo punto de prueba $t = T+1, T+2, \ldots, T+T'$:

\begin{enumerate}
\item Se extrae la ventana de entrada $\mathbf{x}_t = [y_{t-N_x}, \ldots, y_{t-1}]$

\item Se normaliza usando los parámetros congelados:
\begin{equation}
\tilde{\mathbf{x}}_t = \frac{\mathbf{x}_t - \mu_{\text{train}}}{\sigma_{\text{train}} + \epsilon}
\end{equation}

\item Cada modelo del ensamble genera predicciones cuantílicas:
\begin{equation}
\hat{\mathbf{q}}_t^{(b)} = \text{LSTM}_b(\tilde{\mathbf{x}}_t; \theta_b^{\text{frozen}}), \quad b = 1, \ldots, B
\end{equation}

\item Se agregan mediante media aritmética:
\begin{equation}
\hat{\mathbf{q}}_t = \frac{1}{B} \sum_{b=1}^{B} \hat{\mathbf{q}}_t^{(b)}
\end{equation}

\item Se conformalizan los cuantiles:
\begin{equation}
\begin{aligned}
\hat{q}_{\tau_i}^{\text{conf}}(t) &= \hat{q}_{\tau_i}(t) - \omega^{\text{lo}} \quad \text{si } \tau_i \leq 0.5 \\
\hat{q}_{\tau_i}^{\text{conf}}(t) &= \hat{q}_{\tau_i}(t) + \omega^{\text{hi}} \quad \text{si } \tau_i > 0.5
\end{aligned}
\end{equation}
donde $\omega^{\text{lo}} = Q_{1-\alpha}(\{E_i^{\text{lo}}\})$ y $\omega^{\text{hi}} = Q_{1-\alpha}(\{E_i^{\text{hi}}\})$ utilizan las distribuciones de scores actuales.

\item Se des-normalizan los cuantiles conformalizados:
\begin{equation}
\hat{q}_{\tau_i}^{\text{conf, orig}}(t) = \hat{q}_{\tau_i}^{\text{conf}}(t) \cdot \sigma_{\text{train}} + \mu_{\text{train}}
\end{equation}

\item Se ajusta la distribución Skew-Normal mediante \eqref{eq:encqr_skewnorm_fit} y se generan $M = 1000$ muestras.

\item \textbf{Actualización condicional:} Si $(t - T) \mod s = 0$:
\begin{itemize}
\item Se calculan los nuevos residuos LOO para las observaciones $\{t-s, \ldots, t-1\}$ usando los modelos congelados
\item Se eliminan los $s$ residuos más antiguos de $\{E_i^{\text{lo}}\}$ y $\{E_i^{\text{hi}}\}$
\item Se añaden los nuevos residuos, manteniendo el tamaño constante
\item Se recalculan $\omega^{\text{lo}}$ y $\omega^{\text{hi}}$ para las predicciones subsiguientes
\end{itemize}
\end{enumerate}

Este protocolo garantiza:
\begin{itemize}
\item \textbf{Ausencia de data leakage:} Los modelos nunca observan datos de prueba durante el entrenamiento
\item \textbf{Evaluación justa:} Todos los métodos operan bajo condiciones idénticas de congelamiento
\item \textbf{Adaptabilidad controlada:} La actualización de scores permite adaptación a cambios graduales sin reentrenamiento completo
\end{itemize}

\subsubsection{Posicionamiento Metodológico}

\paragraph{Comparación con Métodos Relacionados}

La Tabla \ref{tab:encqr_comparison} posiciona EnCQR-LSTM en el espacio de métodos probabilísticos para series temporales.

\begin{table}[htbp]
\centering
\caption{Comparación conceptual de EnCQR-LSTM con métodos relacionados}
\label{tab:encqr_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{2.8cm}%
                >{\centering\arraybackslash}p{2.2cm}%
                >{\centering\arraybackslash}p{2.2cm}%
                >{\centering\arraybackslash}p{3cm}%
                >{\centering\arraybackslash}p{3cm}}
\toprule
\textbf{Método} & \textbf{Intervalos adaptativos} & \textbf{Cobertura válida} & \textbf{Arquitectura} & \textbf{Complejidad computacional} \\
\midrule
QRNN-LSTM & Sí & No & LSTM con pinball loss & Media (entrenamiento NN) \\
\addlinespace
CP estándar & No & Sí & Agnóstico & Baja (post-procesamiento) \\
\addlinespace
CQR & Sí & Sí (i.i.d.) & Agnóstico + QR & Media \\
\addlinespace
EnbPI & No & Sí (TS) & Ensamble + CP & Media-Alta \\
\addlinespace
EnCQR-LSTM & Sí & Sí (TS) & Ensamble LSTM + QR + CP & Alta (B × entrenamiento NN) \\
\addlinespace
DeepAR & Sí & No & LSTM autorregresivo & Alta \\
\addlinespace
AREPD & Sí & No & Regresión polinomial Ridge & Baja \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Ventajas Distintivas}

\begin{enumerate}
\item \textbf{Síntesis metodológica óptima:} EnCQR combina tres paradigmas complementarios:
\begin{itemize}
\item \textit{Regresión cuantílica:} Permite intervalos adaptativos que varían con $x$ según la variabilidad local estimada
\item \textit{Predicción conformal:} Proporciona garantías de cobertura marginal válida sin suposiciones distribucionales paramétricas
\item \textit{Aprendizaje en ensamble:} Mejora robustez mediante agregación de modelos diversos, reduciendo varianza y sensibilidad a inicializaciones
\end{itemize}

\item \textbf{Aplicabilidad a series temporales heterocedásticas:} A diferencia de CP estándar (intervalos constantes) o EnbPI (intervalos simétricos), EnCQR genera intervalos que se expanden y contraen naturalmente según la volatilidad local, crucial para series con variabilidad cambiante.

\item \textbf{Robustez ante especificación incorrecta:} Incluso si el modelo LSTM subyacente captura imperfectamente la dinámica temporal, la conformalización garantiza cobertura válida. Los intervalos serán más amplios de lo necesario, pero mantendrán la propiedad de cobertura $(1-\alpha)$.

\item \textbf{Distribución predictiva completa:} El ajuste Skew-Normal permite generar muestras de toda la distribución predictiva, no solo intervalos puntuales. Esto facilita:
\begin{itemize}
\item Estimación de métricas probabilísticas completas (CRPS, Log-Score)
\item Análisis de asimetría y colas de la distribución
\item Toma de decisiones basada en riesgo mediante percentiles arbitrarios
\end{itemize}

\item \textbf{Actualización adaptativa sin reentrenamiento:} El mecanismo de ventana deslizante permite que los factores de conformalización se adapten a cambios graduales en $T$ pasos sin el costoso proceso de reentrenar los $B$ modelos LSTM.

\item \textbf{Escalabilidad a arquitecturas complejas:} El framework EnCQR es agnóstico a la arquitectura de red neuronal específica. Puede aplicarse sobre:
\begin{itemize}
\item LSTMs (implementado aquí)
\item Redes convolucionales temporales (TCN)
\item Transformers para series temporales
\item Modelos híbridos CNN-LSTM
\end{itemize}
\end{enumerate}

\paragraph{Limitaciones Fundamentales}

\begin{enumerate}
\item \textbf{Alto costo computacional:} Entrenar $B$ modelos LSTM completos es computacionalmente intensivo:
\begin{itemize}
\item Tiempo de entrenamiento: $O(B \cdot \text{épocas} \cdot T_b \cdot N_u^2)$ donde $T_b = T/B$
\item Para series largas ($T > 10{,}000$) y $B \geq 5$, el tiempo total puede ser prohibitivo sin aceleración GPU
\end{itemize}

\item \textbf{Requisitos de datos:} La partición en $B$ subconjuntos disjuntos reduce efectivamente el tamaño de datos para cada modelo:
\begin{itemize}
\item Cada LSTM se entrena con solo $T/B$ observaciones
\item Para series cortas ($T < 100$), la fragmentación puede ser excesiva, degradando la calidad de los aprendices individuales
\item Existe un trade-off fundamental: $B$ grande mejora diversidad del ensamble pero empobrece datos por modelo
\end{itemize}

\item \textbf{Cobertura marginal, no condicional:} EnCQR garantiza cobertura promedio sobre todo el conjunto de prueba, pero no garantiza cobertura para cada punto individual $x$ específico:
\begin{equation}
\mathbb{P}\left\{\frac{1}{T'}\sum_{t=T+1}^{T+T'} \mathbb{1}\{Y_t \in \hat{C}_{\alpha}(X_t)\} \geq 1-\alpha\right\} \approx 1
\end{equation}
pero no necesariamente $\mathbb{P}\{Y_t \in \hat{C}_{\alpha}(X_t) \mid X_t = x\} \geq 1-\alpha$ para todo $x$.

\item \textbf{Supuestos de estacionariedad débil:} La validez de las garantías de cobertura requiere que el proceso de error sea aproximadamente estacionario y mezclante. Para series con:
\begin{itemize}
\item Cambios estructurales abruptos (quiebres de régimen)
\item Tendencias determinísticas fuertes no removidas
\item Estacionalidad no capturada por el modelo
\end{itemize}
la cobertura puede degradarse significativamente.

\item \textbf{Ventana de actualización $s$ fija:} El parámetro $s$ debe especificarse a priori basándose en el conocimiento del dominio (e.g., $s=24$ para ciclos diarios). Una selección inadecuada puede resultar en:
\begin{itemize}
\item $s$ muy pequeño: Actualizaciones frecuentes con alta varianza en $\omega^{\text{lo}}, \omega^{\text{hi}}$, causando intervalos erráticos
\item $s$ muy grande: Adaptación lenta a cambios en volatilidad, intervalos potencialmente mal calibrados
\end{itemize}

\item \textbf{Sensibilidad al ajuste Skew-Normal:} Si los cuantiles conformalizados exhiben patrones multimodales o altamente irregulares (posible con muestras pequeñas), el ajuste unimodal Skew-Normal puede ser inadecuado, subestimando la verdadera complejidad de la distribución predictiva.

\item \textbf{Ausencia de interpretabilidad:} Las LSTMs son modelos de caja negra. A diferencia de AREPD (coeficientes polinomiales interpretables) o ARMA (coeficientes autorregresivos claros), es difícil extraer conocimiento sobre qué patrones temporales específicos captura el modelo.
\end{enumerate}

\paragraph{Casos de Uso Recomendados}

EnCQR-LSTM es particularmente apropiado para:

\begin{itemize}
\item \textbf{Series heterocedásticas con patrones complejos:} Donde la volatilidad cambia significativamente (e.g., demanda eléctrica con ciclos diarios/semanales fuertes, series financieras de alta frecuencia).

\item \textbf{Escenarios donde garantías de cobertura son críticas:} Aplicaciones en planificación energética, gestión de inventarios, o predicción de demanda donde subestimar incertidumbre tiene consecuencias costosas.

\item \textbf{Datos suficientemente largos:} Series con $T \geq 500$ observaciones permiten particiones en $B=3$ subconjuntos con $\approx 165$ observaciones cada uno, suficiente para entrenar LSTMs efectivos.

\item \textbf{Disponibilidad de recursos computacionales:} Entorno con GPUs disponibles para entrenamiento paralelo de los $B$ modelos.

\item \textbf{Necesidad de distribuciones predictivas completas:} Cuando se requiere más que intervalos puntuales, como análisis de riesgo en colas, optimización estocástica, o toma de decisiones bajo incertidumbre.
\end{itemize}

Y menos apropiado para:

\begin{itemize}
\item Series muy cortas ($T < 200$) donde la partición en $B$ subconjuntos resulta en datos insuficientes por modelo

\item Aplicaciones de tiempo real con restricciones computacionales severas donde el overhead de $B$ inferencias es prohibitivo

\item Series altamente no estacionarias con cambios estructurales frecuentes que violan el supuesto de estacionariedad débil

\item Escenarios donde interpretabilidad del modelo es un requisito regulatorio o de negocio

\item Procesos lineales simples donde métodos más parsimoniosos (ARMA, LSPM) serían igualmente efectivos
\end{itemize}

\paragraph{Contribución Metodológica en el Contexto del Estudio}

EnCQR-LSTM representa el enfoque más sofisticado metodológicamente entre los modelos evaluados en esta investigación, combinando tres paradigmas de frontera en aprendizaje estadístico. Su inclusión permite:

\begin{enumerate}
\item \textbf{Evaluar el valor de la complejidad arquitectónica:} Cuantificar si la inversión en complejidad computacional y conceptual se traduce en mejoras medibles de ECRPS versus métodos más simples.

\item \textbf{Establecer un límite superior de desempeño:} Como uno de los métodos más avanzados disponibles para pronóstico probabilístico de series temporales, EnCQR-LSTM proporciona un benchmark de referencia contra el cual métodos más parsimoniosos pueden compararse.

\item \textbf{Validar garantías teóricas en la práctica:} Verificar empíricamente si las garantías formales de cobertura se materializan en los escenarios de simulación considerados (ARMA, SETAR, GARCH).

\item \textbf{Caracterizar el trade-off complejidad-desempeño:} Determinar en qué tipos de procesos (lineales, no lineales, heterocedásticos) la complejidad adicional de EnCQR justifica su costo versus alternativas como AREPD o MCPS.
\end{enumerate}

Los resultados empíricos del Capítulo~\ref{cap:resultados} revelarán si EnCQR-LSTM logra su objetivo de sintetizar adaptabilidad heterocedástica con cobertura válida, y a qué costo relativo en términos de complejidad y recursos computacionales.