% !TeX root = ../main.tex
\chapter{Metodología de Simulaciones}
\label{cap:diseño_simulacion}

Este capítulo describe el diseño experimental desarrollado para evaluar el desempeño de los métodos de pronóstico probabilístico en series temporales. Se presenta la justificación de los escenarios de evaluación, la metodología de simulación empleada y las características específicas de los procesos generadores de datos utilizados.

\section{Introducción}
\label{sec:intro_simulacion}

La evaluación rigurosa de metodologías de pronóstico probabilístico requiere un marco experimental controlado que permita comparar el desempeño de diferentes técnicas bajo condiciones conocidas. A diferencia de los estudios con datos reales, donde la distribución verdadera es desconocida y la evaluación se limita a métricas indirectas, los estudios de simulación ofrecen la ventaja fundamental de conocer exactamente el proceso generador de datos (DGP, por sus siglas en inglés) \parencite{Hyndman2021FPP3}.

Este conocimiento del DGP permite evaluar directamente la calidad de las distribuciones predictivas mediante su comparación con la verdadera distribución teórica. En particular, el uso del ECRPS (Expected Continuous Ranked Probability Score) como métrica principal de evaluación se justifica porque permite cuantificar simultáneamente la calibración y la nitidez de los pronósticos probabilísticos, comparando las muestras generadas por cada método con muestras de la distribución teórica verdadera \parencite{Gneiting2014}.

El diseño experimental desarrollado considera tres dimensiones fundamentales de variación: (1) la estructura temporal del proceso (estacionariedad y linealidad), (2) la distribución del término de error, y (3) la magnitud de la varianza del ruido. Esta combinación genera un espacio de escenarios suficientemente amplio para evaluar la robustez y adaptabilidad de los métodos bajo diferentes condiciones operativas.

\section{Diseño de la Simulación}
\label{sec:diseño_experimental}

\subsection{Selección de Escenarios de Evaluación}
\label{subsec:escenarios}

El presente estudio considera tres escenarios fundamentales que caracterizan diferentes clases de comportamiento en series temporales. La selección de estos escenarios se fundamenta en la clasificación teórica de procesos estocásticos y en consideraciones de relevancia práctica.

\subsubsection{Escenario 1: Lineal Estacionario (ARMA)}

El primer escenario considera procesos autorregresivos de media móvil (ARMA), que representan la clase fundamental de modelos lineales estacionarios. Un proceso ARMA$(p,q)$ se caracteriza por su capacidad de capturar tanto la persistencia temporal (componente AR) como la dependencia de shocks pasados (componente MA), manteniendo propiedades estadísticas constantes en el tiempo \parencite{Arrieta2017}.

La estacionariedad de estos procesos garantiza que la media, varianza y estructura de autocorrelación permanezcan invariantes bajo traslaciones temporales, lo que facilita la modelación y el pronóstico \parencite{Hyndman2021FPP3}. Este escenario permite evaluar el desempeño de los métodos en condiciones ideales, donde los supuestos fundamentales de muchas técnicas estadísticas se cumplen.

\subsubsection{Escenario 2: Lineal No Estacionario (ARIMA)}

El segundo escenario aborda procesos autorregresivos integrados de media móvil (ARIMA), que extienden la clase ARMA para series con tendencias estocásticas. La presencia de raíces unitarias en el polinomio autorregresivo genera comportamientos de paseo aleatorio que son comunes en series económicas y financieras \parencite{Hyndman2021FPP3}.

La no estacionariedad introduce desafíos adicionales para el pronóstico probabilístico, ya que la incertidumbre crece sin límite conforme aumenta el horizonte de predicción. Este escenario permite evaluar la capacidad de los métodos para adaptarse a estructuras no estacionarias mediante diferenciación o técnicas adaptativas.

\subsubsection{Escenario 3: No Lineal Estacionario (SETAR)}

El tercer escenario considera modelos autorregresivos de umbral auto-excitados (SETAR), que permiten cambios estructurales endógenos en la dinámica del proceso. Estos modelos capturan no linealidades mediante el cambio de régimen determinado por valores pasados de la propia serie \parencite{Chen2023}.

La estacionariedad global de un proceso SETAR requiere condiciones específicas sobre los parámetros autorregresivos en cada régimen y la frecuencia de transición entre regímenes. Estas condiciones se discuten en detalle en la Sección~\ref{subsec:setar_stationarity}. Este escenario es particularmente relevante para evaluar la capacidad de los métodos conformales de capturar dinámicas asimétricas y dependientes del estado del sistema.

\subsection{Estructura del Diseño de Simulación base}
\label{subsec:diseño_factorial}

El diseño experimental implementa un esquema completo que combina sistemáticamente tres dimensiones de variación para cada uno de los tres escenarios considerados. Esta estructura genera un total de 420 configuraciones únicas de simulación, distribuidas equitativamente entre los escenarios.

\subsubsection{Dimensión 1: Configuraciones Paramétricas del Proceso}

Para cada clase de modelo (ARMA, ARIMA, SETAR), se consideran 7 configuraciones paramétricas distintas que representan diferentes grados de complejidad y características dinámicas. Las especificaciones detalladas de estas configuraciones se presentan en la Sección~\ref{sec:procesos_generadores}. Esta diversidad paramétrica permite evaluar la sensibilidad de los métodos a diferentes estructuras de dependencia temporal.

\subsubsection{Dimensión 2: Distribuciones del Término de Error}

Se consideran cinco familias de distribuciones para el término de innovación $\varepsilon_t$, seleccionadas para representar diferentes características de forma, simetría y comportamiento en las colas:

\begin{enumerate}
\item \textbf{Normal:} $\varepsilon_t \sim N(0, \sigma^2)$. Representa el caso base con colas ligeras y simetría perfecta.

\item \textbf{T-Student:} $\varepsilon_t \sim \sigma \cdot \frac{t_{18}}{\sqrt{18/16}}$, donde $t_{18}$ denota una distribución t de Student con 18 grados de libertad. Esta parametrización garantiza varianza unitaria y genera colas más pesadas que la normal, capturando eventos extremos más frecuentes.

\item \textbf{Exponencial:} $\varepsilon_t \sim \sigma(Y - 1)$, donde $Y \sim \text{Exp}(1)$. Produce asimetría positiva y es relevante para series que modelan variables intrínsecamente positivas o con shocks unidireccionales.

\item \textbf{Uniforme:} $\varepsilon_t \sim U(-\sqrt{3}\sigma, \sqrt{3}\sigma)$. Genera soporte acotado y ausencia de colas, representando un caso extremo de curtosis negativa.

\item \textbf{Mixtura de Normales:} $\varepsilon_t \sim 0.75 \cdot N(-\sigma/4, \sigma^2/16) + 0.25 \cdot N(3\sigma/4, \sigma^2/16)$. Produce bimodalidad y permite evaluar el desempeño bajo distribuciones predictivas complejas con múltiples modas.
\end{enumerate}

Esta selección permite evaluar la robustez de los métodos ante desviaciones del supuesto de normalidad que frecuentemente se asume en la literatura de pronóstico \parencite{Arrieta2017}.

\subsubsection{Dimensión 3: Niveles de Varianza del Error}

Se consideran cuatro niveles de varianza $\sigma^2 \in \{0.2, 0.5, 1.0, 3.0\}$ que representan diferentes razones señal-ruido. El nivel base $\sigma^2 = 1.0$ corresponde a la parametrización estándar, mientras que $\sigma^2 = 0.2$ representa un escenario de alta predictibilidad y $\sigma^2 = 3.0$ captura situaciones de alta volatilidad donde la incertidumbre inherente domina la dinámica del sistema.

\subsubsection{Combinatoria Total}

La combinación de estas tres dimensiones genera:
\begin{equation}
N_{\text{config}} = 7 \text{ modelos} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones por escenario}
\end{equation}

Con tres escenarios (ARMA, ARIMA, SETAR), el espacio experimental completo comprende:
\begin{equation}
N_{\text{total}} = 140 \times 3 = 420 \text{ configuraciones únicas}
\end{equation}

Adicionalmente, considerando que cada configuración se evalúa en un horizonte de predicción de 12 pasos usando ventana rodante para que se realice predicción a un paso adelante, el número total de combinaciones configuración-horizonte es de $420 \times 12 = 5040$.

\subsection{Protocolo de Simulación y Partición de Datos}
\label{subsec:protocolo_simulacion}

Para cada una de las 420 configuraciones, se implementa el siguiente protocolo de simulación:

\begin{enumerate}
\item \textbf{Generación de la Serie:} Se simulan $n_{\text{total}} = 302$ observaciones del proceso especificado, precedidas por un período de burn-in de 50 observaciones que se descartan para eliminar el efecto de las condiciones iniciales. Esto resulta en una serie efectiva de longitud $n = 252$ lo cual emula condiciones similares a las encontradas en algunas series de distintas frecuencias (21 años de una serie mensual, o aproximadamente 5 años de una serie semanal, o 9 meses de una serie diaria) .

\item \textbf{Partición Tripartita:} La serie se divide en tres conjuntos disjuntos:
\begin{itemize}
\item \textbf{Conjunto de Entrenamiento:} $n_{\text{train}} = 200$ observaciones iniciales utilizadas para la estimación inicial de parámetros.
\item \textbf{Conjunto de Calibración:} $n_{\text{cal}} = 40$ observaciones subsecuentes utilizadas para la calibración de los hiperparametros y la construcción de distribuciones conformales.
\item \textbf{Conjunto de Prueba:} $n_{\text{test}} = 12$ observaciones finales utilizadas para la evaluación del desempeño predictivo.
\end{itemize}

\item \textbf{Esquema de Ventana Rodante:} La evaluación se realiza mediante una ventana rodante (rolling window) donde:
\begin{itemize}
\item Para el primer paso de predicción, se utilizan las primeras 200 observaciones para entrenamiento y las siguientes 40 para calibración.
\item Para cada paso $h = 1, \ldots, 12$, la ventana de entrenamiento se extiende para incluir las observaciones anteriores, manteniendo fijo el conjunto de calibración de tamaño 40 inmediatamente anterior al punto de predicción.
\item Este esquema emula una situación operativa donde el analista actualiza periódicamente los modelos conforme nueva información se hace disponible.
\end{itemize}

\item \textbf{Generación de Distribuciones Predictivas:} Para cada método y cada paso de predicción $h$, se generan muestras de la distribución predictiva. Estas muestras se comparan con muestras de la distribución teórica verdadera del proceso (conocida por construcción del DGP) mediante el cálculo del ECRPS para ese paso específico.


\end{enumerate}

\section{Procesos Generadores de Datos}
\label{sec:procesos_generadores}

Esta sección describe formalmente los modelos utilizados como procesos generadores de datos en cada escenario, junto con las configuraciones paramétricas específicas consideradas. Para cada clase de modelo, se presentan las ecuaciones fundamentales, las condiciones de estacionariedad (cuando corresponda) y las parametrizaciones concretas evaluadas.
\subsection{Procesos ARMA: Escenario Lineal Estacionario}
\label{subsec:arma}

\subsubsection{Definición y Representación}

Un proceso autorregresivo de media móvil de órdenes $p$ y $q$, denotado ARMA$(p,q)$, se define mediante la ecuación en diferencias estocástica:
\begin{equation}
Y_t = c + \sum_{i=1}^p \phi_i Y_{t-i} + \varepsilon_t + \sum_{j=1}^q \theta_j \varepsilon_{t-j}
\label{eq:arma_general}
\end{equation}
donde $c$ es un término constante, $\{\phi_i\}_{i=1}^p$ son los coeficientes autorregresivos, $\{\theta_j\}_{j=1}^q$ son los coeficientes de media móvil, y $\{\varepsilon_t\}$ es un proceso de ruido blanco con media cero y varianza $\sigma^2$.

Utilizando el operador de rezagos $L$ definido por $L^k Y_t = Y_{t-k}$, el proceso puede expresarse en forma compacta:
\begin{equation}
\Phi(L) Y_t = c + \Theta(L) \varepsilon_t
\label{eq:arma_lag_operator}
\end{equation}
donde $\Phi(L) = 1 - \sum_{i=1}^p \phi_i L^i$ es el polinomio autorregresivo y $\Theta(L) = 1 + \sum_{j=1}^q \theta_j L^j$ es el polinomio de media móvil suponiendo que no hay raíces comunes entre el polinomio $\Phi$ y $\Theta$.

\subsubsection{Condiciones de Estacionariedad e Invertibilidad}

La estacionariedad y la invertibilidad de un proceso ARMA están determinadas por las raíces de sus polinomios característicos \parencite{Hyndman2021FPP3}:

\begin{itemize}
\item \textbf{Estacionariedad:} El proceso es estacionario en covarianza si y solo si todas las raíces del polinomio autorregresivo $\Phi(z) = 0$ se encuentran estrictamente fuera del círculo unitario complejo. Equivalentemente, las raíces del polinomio $\Phi(L)$ deben satisfacer $|z_i| > 1$ para todo $i$.

\item \textbf{Invertibilidad:} El proceso es invertible si y solo si todas las raíces del polinomio de media móvil $\Theta(z) = 0$ se encuentran estrictamente fuera del círculo unitario complejo.
\end{itemize}

Estas condiciones garantizan que el proceso admite representaciones de Wold (MA$(\infty)$) y autorregresiva (AR$(\infty)$) convergentes, lo que es fundamental para la teoría de pronóstico \parencite{Arrieta2017}.

\subsubsection{Distribución Predictiva Verdadera}

Para un proceso ARMA estacionario e invertible, la distribución del siguiente valor $Y_{n+1}$ condicionada a la historia observada $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$ tiene una forma analítica explícita. Dado que el modelo es lineal, la distribución condicional un paso adelante puede conocerse gracias a la ecuación en diferencias, por medio de la media condicional y la varianza condicional.

La media condicional se obtiene de la ecuación estructural del modelo:
\begin{equation}
\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n] = c + \sum_{i=1}^p \phi_i Y_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j}
\label{eq:arma_conditional_mean}
\end{equation}

donde todos los términos del lado derecho son conocidos. La varianza condicional es constante e igual a la varianza del ruido:
\begin{equation}
\text{Var}[Y_{n+1} \mid \mathcal{F}_n] = \sigma^2
\label{eq:arma_conditional_variance}
\end{equation}

Por lo tanto, si el ruido $\varepsilon_t$ sigue una distribución $F$ con media cero y varianza $\sigma^2$, la distribución predictiva verdadera es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F\left(\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\label{eq:arma_predictive_distribution}
\end{equation}

Esta distribución puede evaluarse numéricamente generando una muestra grande de errores futuros $\varepsilon_{n+1}^{(b)} \sim F(0, \sigma^2)$ y calculando:
\begin{equation}
Y_{n+1}^{(b)} = c + \sum_{i=1}^p \phi_i Y_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j} + \varepsilon_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:arma_monte_carlo_samples}
\end{equation}

donde $B$ es un número suficientemente grande (en esta investigación, $B = 1000$). Esta muestra empírica aproxima la distribución predictiva verdadera y sirve como referencia para el cálculo del ECRPS.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:arma_configs} presenta las siete configuraciones ARMA consideradas en este estudio. La selección incluye modelos puramente autorregresivos [AR(1), AR(2)], puramente de media móvil [MA(1), MA(2)], y mixtos [ARMA(1,1), ARMA(2,2), ARMA(2,1)], con diferentes grados de persistencia temporal y complejidad estructural.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos ARMA.}
\label{tab:arma_configs}
\begin{tabular}{lcccc}
\toprule
\textbf{Nombre} & \textbf{$p$} & \textbf{$q$} & \textbf{$\boldsymbol{\phi}$} & \textbf{$\boldsymbol{\theta}$} \\
\midrule
AR(1)     & 1 & 0 & $[0.9]$              & $[]$           \\
AR(2)     & 2 & 0 & $[0.5, -0.3]$        & $[]$           \\
MA(1)     & 0 & 1 & $[]$                 & $[0.7]$        \\
MA(2)     & 0 & 2 & $[]$                 & $[0.4, 0.2]$   \\
ARMA(1,1) & 1 & 1 & $[0.6]$              & $[0.3]$        \\
ARMA(2,2) & 2 & 2 & $[0.4, -0.2]$        & $[0.5, 0.1]$   \\
ARMA(2,1) & 2 & 1 & $[0.7, 0.2]$         & $[0.5]$        \\
\bottomrule
\end{tabular}
\end{table}

Todas las configuraciones fueron verificadas para satisfacer las condiciones de estacionariedad e invertibilidad mediante el cálculo numérico de las raíces de los polinomios característicos correspondientes.

\subsection{Procesos ARIMA: Escenario Lineal No Estacionario}
\label{subsec:arima}

\subsubsection{Definición y Operador de Diferenciación}

Un proceso autorregresivo integrado de media móvil de órdenes $(p,d,q)$, denotado ARIMA$(p,d,q)$, se construye aplicando el operador de diferenciación $\Delta = 1 - L$ un total de $d$ veces a una serie $Y_t$ y modelando la serie diferenciada resultante $W_t = \Delta^d Y_t$ mediante un proceso ARMA$(p,q)$ estacionario:
\begin{equation}
\Phi(L) W_t = c + \Theta(L) \varepsilon_t
\label{eq:arima_general}
\end{equation}
donde $W_t = (1-L)^d Y_t$.

Equivalentemente, en términos de la serie original:
\begin{equation}
\Phi(L)(1-L)^d Y_t = c + \Theta(L) \varepsilon_t
\label{eq:arima_original_scale}
\end{equation}

El orden de integración $d$ representa el número de raíces unitarias en el polinomio autorregresivo ampliado. En la gran mayoría de aplicaciones prácticas, $d \in \{0, 1, 2\}$, siendo $d=1$ el caso más frecuente \parencite{Hyndman2021FPP3}.

\subsubsection{Propiedades de Estacionariedad}

Un proceso ARIMA$(p,d,q)$ es no estacionario por construcción cuando $d > 0$, debido a la presencia de raíces unitarias. Sin embargo, la serie diferenciada $W_t = \Delta^d Y_t$ es estacionaria si el componente ARMA$(p,q)$ subyacente satisface las condiciones de estacionariedad e invertibilidad descritas en la Sección~\ref{subsec:arma}.

Esta propiedad de \textit{estacionariedad en diferencias} es fundamental para el pronóstico, ya que permite aplicar toda la teoría desarrollada para procesos estacionarios a la serie transformada $W_t$, recuperando posteriormente los pronósticos en la escala original mediante integración sucesiva \parencite{Hyndman2021FPP3}.

\subsubsection{Distribución Predictiva Verdadera}

Para un proceso ARIMA$(p,d,q)$, la distribución del siguiente valor $Y_{n+1}$ condicionada a la historia observada se obtiene mediante un procedimiento de dos etapas que explota la estructura de diferenciación del modelo.

Primero, se predice el siguiente valor de la serie diferenciada $W_{n+1} = \Delta^d Y_{n+1}$ usando la distribución ARMA subyacente. Para el caso más común $d=1$, la serie diferenciada es:
\begin{equation}
W_t = Y_t - Y_{t-1}
\end{equation}

y su predicción un paso adelante, condicionada a la historia $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$, sigue la distribución ARMA:
\begin{equation}
    \label{eq:ARIMA_densidad}
W_{n+1} \mid \mathcal{F}_n \sim F\left(\mathbb{E}[W_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\end{equation}
donde:
\begin{equation}
\mathbb{E}[W_{n+1} \mid \mathcal{F}_n] = c + \sum_{i=1}^p \phi_i W_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j}
\label{eq:arima_diff_conditional_mean}
\end{equation}
La ecuación \ref{eq:ARIMA_densidad} es un paso intermedio crucial, ya que la distribución predictiva verdadera para $Y_{n+1}$ se deriva de esta distribución por la recursividad.
Segundo, se recupera la predicción en la escala original mediante la relación de integración:
\begin{equation}
Y_{n+1} = Y_n + W_{n+1}
\label{eq:arima_integration}
\end{equation}

Por lo tanto, la distribución predictiva verdadera para $Y_{n+1}$ es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F\left(Y_n + \mathbb{E}[W_{n+1} \mid \mathcal{F}_n], \sigma^2\right)
\label{eq:arima_predictive_distribution}
\end{equation}

Esta distribución puede evaluarse numéricamente generando muestras del incremento futuro:
\begin{equation}
W_{n+1}^{(b)} = c + \sum_{i=1}^p \phi_i W_{n+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{n+1-j} + \varepsilon_{n+1}^{(b)}
\label{eq:arima_diff_samples}
\end{equation}

y aplicando la transformación:
\begin{equation}
Y_{n+1}^{(b)} = Y_n + W_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:arima_level_samples}
\end{equation}

donde $\varepsilon_{n+1}^{(b)} \sim F(0, \sigma^2)$ son errores futuros independientes. Esta muestra empírica representa la distribución predictiva verdadera que sirve como referencia para el ECRPS.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:arima_configs} presenta las siete configuraciones ARIMA$(p,1,q)$ consideradas en este estudio. Todas las configuraciones utilizan $d=1$, reflejando el caso más común en aplicaciones económicas y financieras. La selección incluye desde el paseo aleatorio puro [ARIMA(0,1,0)] hasta modelos con estructura autorregresiva y de media móvil en la serie diferenciada.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos ARIMA.}
\label{tab:arima_configs}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccc}
\toprule
\textbf{Nombre} & \boldmath$p$ & \boldmath$d$ & \boldmath$q$ & \boldmath$\phi$ & \boldmath$\theta$ \\
\midrule
ARIMA(0,1,0) & 0 & 1 & 0 & $[]$              & $[]$              \\
ARIMA(1,1,0) & 1 & 1 & 0 & $[0.6]$           & $[]$              \\
ARIMA(2,1,0) & 2 & 1 & 0 & $[0.5, -0.2]$     & $[]$              \\
ARIMA(0,1,1) & 0 & 1 & 1 & $[]$              & $[0.5]$           \\
ARIMA(0,1,2) & 0 & 1 & 2 & $[]$              & $[0.4, 0.25]$     \\
ARIMA(1,1,1) & 1 & 1 & 1 & $[0.7]$           & $[-0.3]$          \\
ARIMA(2,1,2) & 2 & 1 & 2 & $[0.6, 0.2]$      & $[0.4, -0.1]$     \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Procesos SETAR: Escenario No Lineal Estacionario}
\label{subsec:setar}

\subsubsection{Definición y Mecanismo de Cambio de Régimen}

Un modelo autorregresivo de umbral auto-excitado con dos regímenes, denotado SETAR(2; $p_1$, $p_2$), se define mediante una estructura de cambio de régimen determinado por valores pasados de la propia serie \parencite{Chen2023}:

\begin{equation}
Y_t = 
\begin{cases}
\phi_0^{(1)} + \sum_{i=1}^{p_1} \phi_i^{(1)} Y_{t-i} + \varepsilon_t^{(1)} & \text{si } Y_{t-d} \leq r \\
\phi_0^{(2)} + \sum_{i=1}^{p_2} \phi_i^{(2)} Y_{t-i} + \varepsilon_t^{(2)} & \text{si } Y_{t-d} > r
\end{cases}
\label{eq:setar_general}
\end{equation}

donde:
\begin{itemize}
\item $r$ es el \textit{valor umbral} (threshold value) que determina el cambio de régimen
\item $d$ es el \textit{rezago de umbral} (threshold delay) que especifica qué valor pasado de la serie se utiliza para determinar el régimen activo
\item $\phi_0^{(j)}$ y $\{\phi_i^{(j)}\}_{i=1}^{p_j}$ son los parámetros específicos del régimen $j$
\item $\varepsilon_t^{(j)} \sim WN(0, \sigma_j^2)$ son procesos de ruido blanco que pueden tener varianzas diferentes en cada régimen
\end{itemize}

La notación SETAR(2; $d$, $p$) denota un modelo de dos regímenes con rezago de umbral $d$ y orden autorregresivo común $p$ en ambos regímenes (aunque en general $p_1$ y $p_2$ pueden diferir).

\subsubsection{Estacionariedad en Procesos SETAR}
\label{subsec:setar_stationarity}

La estacionariedad de procesos SETAR es sustancialmente más compleja que en modelos lineales, ya que la dinámica cambia endógenamente según el estado del sistema. Las condiciones suficientes para la estacionariedad han sido objeto de extensa investigación \parencite{Chen2023}.

\paragraph{Caso SETAR(2; 1, 1):} Para el caso más simple de dos regímenes con orden autorregresivo 1, \cite{petruccelli1984consistent} demostraron que el proceso es ergódico si y solo si:
\begin{equation}
|\phi_1^{(1)}| < 1, \quad |\phi_1^{(2)}| < 1, \quad \text{y} \quad |\phi_1^{(1)} \phi_1^{(2)}| < 1
\label{eq:setar11_stationarity}
\end{equation}

Esta condición requiere que cada régimen sea individualmente estable y que el producto de los coeficientes autorregresivos sea menor que uno en valor absoluto. Esta última condición captura el efecto de la interacción entre regímenes.

\paragraph{Caso General SETAR(2; $p_1$, $p_2$):} Para órdenes autorregresivos mayores,\cite{chan1985testing} proporcionaron una condición suficiente basada en el radio espectral de las matrices compañeras:
\begin{equation}
\max_j \sum_{i=1}^{p_j} |\phi_i^{(j)}| < 1
\label{eq:setar_sufficient_condition}
\end{equation}

Sin embargo, esta condición es bastante conservadora. Un criterio más general y menos restrictivo se basa en el concepto de \textit{radio espectral conjunto} (joint spectral radius) de las matrices compañeras de ambos regímenes \parencite{Chen2023}. Sea $\boldsymbol{\Phi}^{(j)}$ la matriz compañera del régimen $j$:

\begin{equation}
\boldsymbol{\Phi}^{(j)} = 
\begin{pmatrix}
\phi_1^{(j)} & \phi_2^{(j)} & \cdots & \phi_{p-1}^{(j)} & \phi_p^{(j)} \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & 0
\end{pmatrix}
\end{equation}

El radio espectral conjunto se define como:
\begin{equation}
\rho(\{\boldsymbol{\Phi}^{(1)}, \boldsymbol{\Phi}^{(2)}\}) = \lim_{k \to \infty} \max \|\boldsymbol{\Phi}^{(i_1)} \cdots \boldsymbol{\Phi}^{(i_k)}\|^{1/k}
\end{equation}
donde el máximo se toma sobre todas las secuencias posibles de $k$ matrices.

El proceso SETAR es estacionario si $\rho(\{\boldsymbol{\Phi}^{(1)}, \boldsymbol{\Phi}^{(2)}\}) < 1$. Este criterio es menos restrictivo que \eqref{eq:setar_sufficient_condition} y permite que algunos regímenes individuales sean incluso explosivos, siempre que la dinámica global del sistema sea estabilizadora \parencite{Chen2023}.

\subsubsection{Distribución Predictiva Verdadera}

La distribución del siguiente valor $Y_{n+1}$ en un proceso SETAR condicionada a la historia observada $\mathcal{F}_n = \{Y_1, \ldots, Y_n, \varepsilon_1, \ldots, \varepsilon_n\}$ depende críticamente del régimen que será activado en el tiempo $n+1$. A diferencia de los modelos lineales, la predicción requiere determinar primero qué régimen gobernará la dinámica futura.

El régimen activo en el tiempo $n+1$ se determina comparando el valor retardado $Y_{n+1-d}$ con el umbral $r$:
\begin{equation}
\text{Régimen}_{n+1} = 
\begin{cases}
1 & \text{si } Y_{n+1-d} \leq r \\
2 & \text{si } Y_{n+1-d} > r
\end{cases}
\label{eq:setar_regime_determination}
\end{equation}

Dado que $Y_{n+1-d}$ ya es conocido en el tiempo $n$ (pues $n+1-d \leq n$ para $d \geq 1$), el régimen futuro es determinístico y no hay incertidumbre sobre cuál dinámica aplicar. Una vez identificado el régimen $j \in \{1, 2\}$, la media condicional se calcula mediante:
\begin{equation}
\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j] = \phi_0^{(j)} + \sum_{i=1}^{p_j} \phi_i^{(j)} Y_{n+1-i}
\label{eq:setar_conditional_mean}
\end{equation}

donde todos los valores $Y_{n+1-i}$ en el lado derecho son observados. La varianza condicional es constante dentro de cada régimen:
\begin{equation}
\text{Var}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j] = \sigma_j^2
\label{eq:setar_conditional_variance}
\end{equation}

Por lo tanto, la distribución predictiva verdadera es:
\begin{equation}
Y_{n+1} \mid \mathcal{F}_n \sim F_j\left(\mathbb{E}[Y_{n+1} \mid \mathcal{F}_n, \text{Régimen}_{n+1} = j], \sigma_j^2\right)
\label{eq:setar_predictive_distribution}
\end{equation}

donde $F_j$ es la distribución del ruido en el régimen $j$ y el subíndice $j$ se determina mediante \eqref{eq:setar_regime_determination}. Cabe aclarar la recursividad y la ecuación en diferencias son la base para la evaluación de esta distribución, ya que el valor futuro $Y_{n+1}$ depende de valores pasados que a su vez dependen de regímenes anteriores. 

Esta distribución puede evaluarse numéricamente generando una muestra grande de errores futuros específicos del régimen activo:
\begin{equation}
Y_{n+1}^{(b)} = \phi_0^{(j)} + \sum_{i=1}^{p_j} \phi_i^{(j)} Y_{n+1-i} + \varepsilon_{n+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:setar_monte_carlo_samples}
\end{equation}

donde $\varepsilon_{n+1}^{(b)} \sim F_j(0, \sigma_j^2)$ son errores independientes del régimen determinado. A diferencia de los modelos ARMA, aquí no existe incertidumbre sobre el régimen en predicciones un paso adelante, lo que simplifica considerablemente la evaluación de la distribución predictiva verdadera.

\subsubsection{Configuraciones Paramétricas Evaluadas}

La Tabla~\ref{tab:setar_configs} presenta las siete configuraciones SETAR consideradas en este estudio. Las configuraciones incluyen diferentes órdenes autorregresivos, rezagos de umbral y valores de umbral, representando una amplia gama de comportamientos no lineales.

\begin{table}[htbp]
\centering
\caption{Configuraciones paramétricas para procesos SETAR.}
\label{tab:setar_configs}
\small
\begin{tabular}{lccccl}
\toprule
\textbf{Nombre} & \textbf{$\boldsymbol{\phi}^{(1)}$} & \textbf{$\boldsymbol{\phi}^{(2)}$} & \textbf{$r$} & \textbf{$d$}  \\
\midrule
SETAR-1 & [0.6] & [-0.5] & 0.0 & 1 \\
SETAR-2 & [0.7] & [-0.7] & 0.0 & 2 \\
SETAR-3 & [0.5, -0.2] & [-0.3, 0.1] & 0.5 & 1 \\
SETAR-4 & [0.8, -0.15] & [-0.6, 0.2] & 1.0 & 2 \\
SETAR-5 & [0.4, -0.1, 0.05] & [-0.3, 0.1, -0.05] & 0.0 & 1 \\
SETAR-6 & [0.5, -0.3, 0.1] & [-0.4, 0.2, -0.05] & 0.5 & 2 \\
SETAR-7 & [0.3, 0.1] & [-0.2, -0.1] & 0.8 & 3 \\
\bottomrule
\end{tabular}
\end{table}

Finalmente, es importante destacar que todas las configuraciones detalladas en la Tabla~\ref{tab:setar_configs} fueron seleccionadas bajo un estricto criterio de estabilidad. Para garantizar el rigor estadístico de las comparaciones en este escenario, se realizó un análisis de estacionariedad basado en el cálculo numérico del radio espectral conjunto ($\rho$) para cada par de matrices compañeras. Se verificó que en la totalidad de los casos empleados en la simulación se cumple la condición $\rho < 1$, asegurando que los procesos SETAR generados son globalmente estacionarios.

% ===========================================================================
% Modelos predictivos
% ===========================================================================


\section{Modelos predictivos}
\label{sec:modelos_predictivos}
Para evaluar la capacidad de cuantificación de la incertidumbre en diversos entornos estocásticos, esta investigación emplea un conjunto heterogéneo de nueve modelos predictivos. Esta selección abarca desde métodos de remuestreo clásicos y propuestas de predicción conformal, hasta arquitecturas de aprendizaje profundo y modelos híbridos. El uso de esta diversidad de enfoques permite contrastar cómo las garantías teóricas de cada familia de modelos se traducen en un rendimiento práctico bajo la métrica ECRPS, especialmente cuando se enfrentan a la ruptura de los supuestos de intercambiabilidad y linealidad.

\subsection{Modelos tomados de la literatura}

\subsubsection{Circular Block Bootstrap (CBB)}
\label{subsec:CCB}
\paragraph{Explicación Teórica del Modelo}

El método \textit{Circular Block Bootstrap} (CBB), introducido por \textcite{politis1992circular}, representa una evolución metodológica del remuestreo por bloques que aborda una limitación fundamental de los esquemas no circulares como el Moving Block Bootstrap (MBB). Según \textcite{Lahiri2003}, el problema radica en que las observaciones ubicadas en los extremos de la serie temporal $\{Y_1, \dots, Y_n\}$ aparecen con menor frecuencia en los bloques remuestreados, generando una infra-representación sistemática de los bordes y sesgo en la estimación de varianza.

\subparagraph{Fundamento Teórico}
El CBB resuelve esta asimetría mediante la \textit{circunscripción} de los datos: la serie temporal se conceptualiza como una estructura circular donde $Y_n$ es seguido inmediatamente por $Y_1$, permitiendo la continuidad periódica. Formalmente, para una serie de longitud $n$ y bloques de tamaño $l$, se definen exactamente $n$ bloques posibles:
\begin{equation}
B_i = \{Y_i, Y_{i+1 \bmod n}, \dots, Y_{i+l-1 \bmod n}\}, \quad i = 1,\dots,n
\end{equation}

donde el operador módulo ($\bmod$) implementa la extensión circular. Esta construcción garantiza que cada observación histórica tiene probabilidad idéntica $1/n$ de ser seleccionada como punto de inicio de un bloque, eliminando el sesgo de borde.

\subparagraph{Algoritmo de Remuestreo}
El procedimiento de generación de muestras bootstrap opera en dos etapas:

\begin{enumerate}
    \item \textbf{Muestreo de puntos de inicio}: Se seleccionan $B$ índices $\{i_1, \dots, i_B\}$ uniformemente de $\{1,\dots,n\}$, donde $B$ es el número de réplicas bootstrap deseadas.
    \item \textbf{Construcción de bloques circulares}: Cada réplica $Y^*_b$ se forma extrayendo el bloque circular iniciado en $i_b$:
    \begin{equation}
    Y^*_b = Y_{(i_b + r) \bmod n}, \quad r \in \{0, 1, \dots, l-1\}
    \end{equation}
\end{enumerate}

Para pronóstico un paso adelante, la distribución predictiva se aproxima mediante el conjunto de valores finales de cada bloque: $\{\hat{y}_{t+1}^{(1)}, \dots, \hat{y}_{t+1}^{(B)}\}$, donde $\hat{y}_{t+1}^{(b)} = Y^*_b$.

\subparagraph{Propiedades Estadísticas}
\textcite{Lahiri2003} establece que bajo condiciones de mixing (dependencia que decae con el tiempo), el estimador CBB de la varianza es consistente cuando $l \to \infty$ y $l/n \to 0$ conforme $n \to \infty$. La equiprobabilidad de selección garantiza distribuciones predictivas mejor calibradas en contextos de dependencia temporal, haciendo al CBB apropiado para series financieras y económicas donde la estructura de autocorrelación es relevante.

\subparagraph{Clasificación del Modelo}
El CBB es un método \textbf{no paramétrico} puro: no asume ninguna forma funcional para la distribución subyacente de los datos ni estima parámetros poblacionales. La distribución predictiva emerge directamente del remuestreo empírico de la historia observada, preservando las características distribucionales y de dependencia presentes en la muestra sin imponer supuestos estructurales.

\paragraph{De la Teoría a la Práctica}

La implementación desarrollada introduce tres adaptaciones principales respecto a la formulación teórica estándar:

\subparagraph{Adaptación 1: Simplificación del Esquema de Remuestreo}
La teoría clásica del CBB \parencite{politis1992circular} genera bloques completos de longitud $l$ que luego se concatenan para formar series bootstrap de longitud $n$. En contraste, la implementación para pronóstico un paso adelante simplifica el proceso: dado que solo se requiere predecir $\hat{y}_{t+1}$, se muestrea directamente el valor en la posición $(i_b + r) \bmod n$ donde $r = n \bmod l$ representa la posición relativa dentro del último bloque histórico. Esta simplificación reduce la complejidad computacional de $O(Bl)$ a $O(B)$ operaciones de indexación.

\subparagraph{Adaptación 2: Selección Automática de $l$}
Mientras que la teoría requiere especificación manual del tamaño de bloque basado en análisis del proceso estocástico subyacente, la implementación incorpora la heurística automática de \textcite{politis2004automatic}: $l \approx 1.5 \times n^{1/3}$, ademas de otros valores de refencia para balancear eficiencia y captura de dependencia.

\subparagraph{Adaptación 3: Congelamiento Post-Optimización}
A diferencia de implementaciones estándar que podrían recalcular $l$ en cada paso, el diseño experimental congela el hiperparámetro tras la fase de validación. Este congelamiento es crítico para prevenir \textit{data leakage}: re-optimizar en ventanas rolling introduciría información futura en la selección del modelo, violando la evaluación predictiva rigurosa.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del CBB}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría Clásica} & \textbf{Implementación} \\
\midrule
Remuestreo & Bloques completos concatenados & Valor directo $(i + n \bmod l) \bmod n$ \\
Longitud de bloque & Manual / dependiente del contexto & Optimización\\
Actualización de $l$ & No especificada & Congelada post-optimización \\
Complejidad & $O(Bl)$ para serie completa & $O(B)$ para un pronóstico \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimización, Parámetros e Hiperparámetros}
\subparagraph{Hiperparámetro Principal: \texttt{block\_length} ($l$)}
Controla la cantidad de dependencia temporal preservada en las réplicas bootstrap. Su configuración óptima se determina mediante una estrategia de optimización que emplea una búsqueda en grilla. La grilla de valores candidatos para $l$ está definida por las siguientes cuatro opciones, que representan diferentes escalas de dependencia temporal en función del tamaño de muestra $n$:

\begin{enumerate}
    \item $l = 5$, para modelar estructuras de dependencia de corto plazo.
    \item $l = \lfloor 1.5 \cdot n^{1/3} \rfloor$, una aproximación heurística común en métodos de \textit{block bootstrap}.
    \item $l = \lfloor \sqrt{n} \rfloor$, que ofrece un balance entre corto y mediano plazo.
    \item $l = \lfloor n / 5 \rfloor$, diseñada para capturar posibles estructuras de dependencia de largo plazo.
\end{enumerate}

La selección del valor óptimo de $l$ de entre estas cuatro opciones se realiza minimizando el \textbf{CRPS promedio} (ECRPS) sobre un conjunto de validación, conforme a la métrica detallada en la subsección \ref{subsec:ecrps}.

\subparagraph{Protocolo de Congelamiento}
Una vez identificado el valor óptimo mediante validación, este se congela y se utiliza de manera fija durante la fase de prueba. Esta lógica garantiza que no haya contaminación de información y que el modelo evaluado sea idéntico al seleccionado durante la validación.


\subsubsection{Sieve Bootstrap (SB)}
\label{subsec:SB}
\paragraph{Explicación Teórica del Modelo}

El \textit{Sieve Bootstrap}, introducido por \textcite{Buhlmann1997} y analizado en profundidad por \textcite{Lahiri2003}, representa un enfoque alternativo al remuestreo por bloques para series temporales dependientes. En lugar de preservar la dependencia mediante partición física de la serie, el método emplea una aproximación paramétrica para filtrar la estructura de autocorrelación.

\paragraph{Fundamento Teórico}
El Sieve Bootstrap se fundamenta en el teorema de Wold, que establece que cualquier proceso estocástico estacionario linealmente regular admite una representación autorregresiva de orden infinito, AR($\infty$). Formalmente, para una serie temporal $\{Y_t\}$ estacionaria con media $\mu$, existe una representación:
\begin{equation}
Y_t - \mu = \sum_{j=1}^{\infty} \phi_j (Y_{t-j} - \mu) + \epsilon_t
\end{equation}
donde $\{\epsilon_t\}$ es un proceso de innovaciones i.i.d. con media cero y varianza $\sigma^2$.

En la práctica, esta representación infinita se aproxima mediante un modelo autorregresivo finito AR($p$) donde $p$ crece con el tamaño muestral $n$:
\begin{equation}
Y_t = \phi_0 + \sum_{j=1}^{p} \phi_j Y_{t-j} + \epsilon_t
\end{equation}

\paragraph{Algoritmo de Remuestreo}
El procedimiento del Sieve Bootstrap opera en tres etapas secuenciales:

\begin{enumerate}
    \item \textbf{Ajuste del tamiz autorregresivo}: Se estima el modelo AR($p$) mediante mínimos cuadrados ordinarios sobre la serie histórica, obteniendo coeficientes $\hat{\phi} = (\hat{\phi}_0, \hat{\phi}_1, \dots, \hat{\phi}_p)$.
    
    \item \textbf{Extracción de residuos}: Se calculan los residuos del modelo ajustado:
    \begin{equation}
    \hat{\epsilon}_t = Y_t - \hat{\phi}_0 - \sum_{j=1}^{p} \hat{\phi}_j Y_{t-j}, \quad t = p+1, \dots, n
    \end{equation}
    que idealmente deben comportarse como realizaciones i.i.d. Estos residuos se centran: $\tilde{\epsilon}_t = \hat{\epsilon}_t - \bar{\epsilon}$.
    
    \item \textbf{Generación de muestras bootstrap}: Para cada réplica $b = 1, \dots, B$:
    \begin{itemize}
        \item Se remuestrean con reemplazo los residuos centrados: $\epsilon^*_b \sim \{\tilde{\epsilon}_{p+1}, \dots, \tilde{\epsilon}_n\}$
        \item Se genera la predicción un paso adelante:
        \begin{equation}
        \hat{Y}_{n+1}^{(b)} = \hat{\phi}_0 + \sum_{j=1}^{p} \hat{\phi}_j Y_{n+1-j} + \epsilon^*_b
        \end{equation}
    \end{itemize}
\end{enumerate}

La distribución predictiva empírica está dada por el conjunto $\{\hat{Y}_{n+1}^{(1)}, \dots, \hat{Y}_{n+1}^{(B)}\}$.

\paragraph{Propiedades de Consistencia}
\textcite{Buhlmann1997} demuestra que bajo condiciones de regularidad (estacionaridad, ergodicidad, y $p = p_n \to \infty$ con $p_n^3/n \to 0$), el Sieve Bootstrap aproxima consistentemente la distribución del estimador de interés. La clave es que el orden $p$ debe crecer suficientemente para capturar la dependencia, pero no tan rápido como para introducir varianza excesiva por sobreparametrización.

\paragraph{Clasificación del Modelo}
El Sieve Bootstrap es un método \textbf{semiparamétrico}: utiliza una estructura paramétrica (el modelo AR) para filtrar la dependencia temporal, pero trata la distribución de los residuos de forma no paramétrica mediante bootstrap empírico. No asume una forma distribucional específica para las innovaciones, solo que sean aproximadamente i.i.d. después del filtrado AR.

\paragraph{De la Teoría a la Práctica}

La implementación desarrollada incorpora tres adaptaciones clave para el contexto de pronóstico rolling:

\subparagraph{Adaptación 1: Selección de Orden Basada en Validación}
Mientras la teoría asintótica sugiere $p \to \infty$, la implementación emplea una grilla discreta de órdenes candidatos $p \in \{5, 10, 20\}$ evaluados mediante ECRPS en validación. Esta discretización responde a dos consideraciones prácticas: (i) evitar sobreparametrización en muestras finitas ($n = 200$), y (ii) reducir tiempo computacional frente a búsquedas exhaustivas.

\subparagraph{Adaptación 2: Congelamiento de Parámetros AR}
La implementación introduce un mecanismo de congelamiento crítico: durante la fase de calibración, se ajusta el modelo AR($p^*$) con orden óptimo $p^*$ sobre los datos de entrenamiento+calibración combinados, almacenando permanentemente:
\begin{itemize}
    \item Coeficientes autorregresivos: $\hat{\phi}^* = (\hat{\phi}_0^*, \dots, \hat{\phi}_{p^*}^*)$
    \item Residuos centrados: $\tilde{\epsilon}^* = \{\tilde{\epsilon}_{p^*+1}, \dots, \tilde{\epsilon}_{n_{\text{calib}}}\}$
\end{itemize}

En cada ventana rolling subsecuente, se reutilizan $\hat{\phi}^*$ y $\tilde{\epsilon}^*$ sin re-estimación, aplicando solo los últimos $p^*$ valores observados para generar la predicción. Esta estrategia previene data leakage y reduce variabilidad numérica.

\subparagraph{Adaptación 3: Predicción Secuencial Eficiente}
En lugar de generar series bootstrap completas de longitud $n$ (complejidad $O(Bn)$), la implementación genera directamente predicciones un paso adelante (complejidad $O(B)$): dado el vector de historia reciente $\mathbf{X}_{n-p^*:n} = (X_{n-p^*+1}, \dots, X_n)$, cada predicción se calcula como:
\begin{equation}
\hat{X}_{n+1}^{(b)} = \hat{\phi}_0^* + \sum_{j=1}^{p^*} \hat{\phi}_j^* X_{n+1-j} + \epsilon^*_b
\end{equation}
donde $\epsilon^*_b$ se muestrea de $\tilde{\epsilon}^*$ con reemplazo.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del Sieve Bootstrap}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría Clásica} & \textbf{Implementación} \\
\midrule
Orden AR & $p \to \infty$  & Grilla discreta $\{5, 10, 20\}$\\
Selección de $p$ & Criterios asintóticos & Validación cruzada (ECRPS) \\
Parámetros $\hat{\phi}$ & Re-estimados en cada muestra & Congelados post-calibración \\
Residuos & Recalculados dinámicamente & Pool fijo $\tilde{\epsilon}^*$ \\
Complejidad & $O(Bn)$ & $O(B)$ \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Hiperparámetro Principal: \texttt{order} ($p$)}
Define la profundidad del tamiz autorregresivo, controlando cuánta memoria del proceso se captura. Durante la fase de validación se evalúan tres configuraciones sobre $n_{\text{train}} = 200$:
\begin{itemize}
    \item $p = 5$ (dependencias de corto plazo, hasta una semana)
    \item $p = 10$ (memoria intermedia, aproximadamente dos semanas)
    \item $p = 20$ (dependencias extendidas, un mes)
\end{itemize}

\subparagraph{Métrica de Optimización}
La selección del orden óptimo $p^*$ se realiza minimizando el \textbf{ECRPS} (véase \ref{subsec:ecrps}) sobre el conjunto de validación. El orden seleccionado es aquel que produce las distribuciones predictivas más calibradas durante la fase de validación.

\subparagraph{Protocolo de Congelamiento}
Una vez identificado $p^*$ mediante validación, el método ajusta el modelo AR($p^*$) sobre los datos de entrenamiento+calibración y almacena permanentemente los coeficientes $\hat{\phi}^*$ y residuos centrados $\tilde{\epsilon}^*$. Estos parámetros se reutilizan sin re-estimación en toda la fase de prueba rolling, previniendo data leakage y garantizando evaluación rigurosa.


\subsubsection{Least Squares Prediction Machine (LSPM)}
\label{subsec:LSPM}
\paragraph{Explicación Teórica del Modelo}

El \textit{Least Squares Prediction Machine} (LSPM), introducido por \textcite{Vovk2022}, representa una evolución de la predicción conformal que trasciende la generación de intervalos de confianza para construir distribuciones predictivas completas. A diferencia de los predictores conformales estándar, el LSPM se define como un Sistema Predictivo Conformal (CPS), cuya salida es una Función de Distribución Predictiva Conformal (CPD).

\subparagraph{Fundamento Teórico: Predicción Conformal}
La predicción conformal se fundamenta en el principio de intercambiabilidad: dada una secuencia de pares $(x_1, y_1), \dots, (x_{n-1}, y_{n-1})$ y un nuevo objeto $x_n$, se asume que para cualquier etiqueta candidata $y$, la secuencia aumentada $(x_1, y_1), \dots, (x_{n-1}, y_{n-1}), (x_n, y)$ es intercambiable. Bajo este supuesto, se puede construir una distribución predictiva válida sin asumir una forma paramétrica específica para los errores.

El LSPM utiliza mínimos cuadrados ordinarios (OLS) como algoritmo subyacente. La versión más robusta es el \textbf{LSPM Studentizado}, que emplea los elementos diagonales de la matriz de proyección (matriz hat) $\bar{H}$ para normalizar los residuos. Según \textcite{Vovk2022}, esta normalización garantiza que la distribución predictiva sea monótonamente creciente, incluso cuando el nuevo objeto posee alto apalancamiento (\textit{leverage}).

\subparagraph{Construcción de la Distribución Predictiva}
Para un conjunto de datos aumentado que incluye $n-1$ ejemplos de entrenamiento $(x_1, y_1), \dots, (x_{n-1}, y_{n-1})$ y un nuevo objeto $x_n$ con etiqueta hipotética $y$, se construye la matriz de diseño aumentada:
\begin{equation}
\bar{X} = \begin{pmatrix}
1 & x_1^T \\
\vdots & \vdots \\
1 & x_{n-1}^T \\
1 & x_n^T
\end{pmatrix}
\end{equation}

La matriz hat se define como:
\begin{equation}
\bar{H} = \bar{X}(\bar{X}^T\bar{X})^{-1}\bar{X}^T
\end{equation}

donde $h_{i,j}$ denota el elemento en la fila $i$ y columna $j$ de $\bar{H}$.

\subparagraph{Valores Críticos Studentizados}
La distribución predictiva se construye mediante valores críticos $C_i$ que actúan como puntos de salto de una función escalonada. Para la versión studentizada, según las ecuaciones 7.15 y 7.16 de \textcite{Vovk2022}:

\begin{equation}
C_i = \frac{A_i}{B_i}, \quad i = 1, \dots, n-1
\end{equation}

donde:
\begin{equation}
B_i = \sqrt{1 - h_{n,n}} + \frac{h_{i,n}}{\sqrt{1 - h_{i,i}}}
\end{equation}

\begin{equation}
A_i = \frac{\sum_{j=1}^{n-1} h_{j,n} y_j}{\sqrt{1 - h_{n,n}}} + \frac{y_i - \sum_{j=1}^{n-1} h_{i,j} y_j}{\sqrt{1 - h_{i,i}}}
\end{equation}

El término $A_i$ combina dos componentes: (i) la predicción OLS estándar sobre el punto nuevo, normalizada por su palanca (leverage), y (ii) el residuo studentizado del punto $i$ en un ajuste leave-one-out. El término $B_i$ normaliza esta combinación considerando el leverage tanto del punto nuevo ($h_{n,n}$) como del punto histórico ($h_{i,i}$) y su covarianza ($h_{i,n}$).

Ordenando la secuencia de conformidad $C_1, \ldots, C_{n-1}$ en orden ascendente como $C_{(1)} \leq \cdots \leq C_{(n-1)}$, y definiendo $C_{(0)} := -\infty$ y $C_{(n)} := \infty$, la distribución predictiva se expresa como:

\begin{equation}
    \bar{\Pi}_n(y) := 
    \begin{cases}
        \left[\frac{i'}{n}, \frac{i'+1}{n}\right] & \text{si } y \in (C_{(i)}, C_{(i+1)}) \text{ para } i \in \{0, 1, \ldots, n-1\} \\
        \left[\frac{i'}{n}, \frac{i''}{n}\right] & \text{si } y = C_{(i)} \text{ para } i \in \{1, \ldots, n-1\}
    \end{cases}
\end{equation}

donde $i' := \min\{j : C_{(j)} = C_{(i)}\}$ y $i'' := \max\{j : C_{(j)} = C_{(i)}\}$.

Esta representación explícita revela que la distribución predictiva de la LSPM estudiantizada es una función escalonada con a lo sumo $n-1$ puntos de discontinuidad, cada uno correspondiente a un valor único de los puntajes de conformidad ordenados. El grosor de cada intervalo es igual a $1/n$, excepto en los puntos de empate donde varios puntajes de conformidad coinciden.


\subparagraph{Propiedades Estadísticas}
\textcite{Vovk2022} demuestra que bajo intercambiabilidad, la función de distribución construida mediante estos valores críticos es estadísticamente válida: para cualquier nivel de confianza $\alpha$, el intervalo de predicción conformal tiene cobertura exacta $1-\alpha$ en expectativa sobre la secuencia intercambiable. La studentización es crucial para mantener esta validez incluso cuando $h_{n,n} \to 1$ (leverage extremo del punto de prueba).

\subparagraph{Clasificación del Modelo}
El LSPM es un método \textbf{no paramétrico} basado en distribución-libre (\textit{distribution-free}). Aunque utiliza OLS como algoritmo subyacente, no asume ninguna forma distribucional para los errores $\epsilon_i$. La validez estadística proviene únicamente del supuesto de intercambiabilidad, no de supuestos gaussianos o paramétricos sobre los residuos.

\paragraph{De la Teoría a la Práctica}

La implementación desarrollada adapta el marco teórico de Vovk al contexto específico de pronóstico en series temporales:

\subparagraph{Adaptación 1: Construcción Autorregresiva}
La teoría original asume vectores de características $X_i$ independientes. En series temporales, se construyen objetos dinámicamente mediante retardos: dado $p$ lags, cada observación se transforma en un vector autorregresivo:
\begin{equation}
X_t = (Y_{t-1}, Y_{t-2}, \dots, Y_{t-p})^T
\end{equation}

Esto convierte al LSPM en un predictor conformal autorregresivo AR($p$), donde la matriz de diseño se construye dinámicamente en cada ventana rolling usando los últimos $n$ valores disponibles.

\subparagraph{Adaptación 2: Estabilidad Numérica}
Para calcular $\bar{H} = \bar{X}(\bar{X}^T\bar{X})^{-1}\bar{X}^T$, la implementación emplea la pseudoinversa de Moore-Penrose en lugar de la inversa estándar. Esta decisión es crítica: series temporales con alta autocorrelación generan matrices $\bar{X}^T\bar{X}$ casi singulares (número de condición alto), causando inestabilidad numérica en la inversión directa. La pseudoinversa provee una solución regularizada que previene errores computacionales.

\subparagraph{Adaptación 3: Filtrado de Singularidades}
La teoría requiere que $h_{i,i} < 1$ y $h_{n,n} < 1$ para que los denominadores en $A_i$ y $B_i$ sean no nulos. La implementación incorpora dos mecanismos de seguridad:
\begin{itemize}
    \item Umbral de tolerancia: Se filtran observaciones con $|1 - h_{i,i}| < 10^{-10}$ o $|1 - h_{n,n}| < 10^{-10}$.
    \item Filtrado de divisores: Se descartan valores críticos donde $|B_i| < 10^{-10}$.
\end{itemize}

Estos filtros previenen divisiones por cero cuando un punto es tan influyente que el modelo lo ajusta sin residuo.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del LSPM}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría Clásica} & \textbf{Implementación} \\
\midrule
Vectores $x_i$ & Características independientes & Retardos AR: $(y_{t-1}, \dots, y_{t-p})$ \\
Cálculo de $\bar{H}$ & Inversa $(\bar{X}^T\bar{X})^{-1}$ & Pseudoinversa (Moore-Penrose) \\
Singularidades & Supuesto $h_{i,i}, h_{n,n} < 1$ & Filtrado explícito ($10^{-10}$) \\
Valores críticos & Todos los $n-1$ puntos & Solo puntos con $B_i$ válido\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Parámetro Principal: \texttt{n\_lags} ($p$)}
Define el orden autorregresivo del modelo, controlando cuántos retardos se usan para construir la matriz de diseño. La implementación emplea la heurística:
\begin{equation}
p = \max\left\{1, \lfloor n^{1/3} \rfloor\right\}
\end{equation}

Esta regla está motivada por consideraciones teóricas de métodos de tamiz (sieve methods) en estadística no paramétrica, donde el número de parámetros $p$ debe crecer con el tamaño muestral $n$ pero a una tasa controlada que preserve consistencia. La tasa $n^{1/3}$ es estándar en la literatura de bootstrap para series temporales \parencite{Buhlmann1997, politis2004automatic}, garantizando que $p \to \infty$ conforme $n \to \infty$, pero manteniendo $p^3/n \to 0$ para evitar sobreajuste. Esta misma tasa aparece en la heurística de longitud de bloque del CBB, reflejando un principio unificado: el número de parámetros debe escalar subcuadráticamente con la muestra. Para $n = 200$, resulta $p \approx 5$ lags, suficiente para capturar autocorrelaciones de corto plazo sin saturar los grados de libertad del modelo OLS subyacente.

\subparagraph{Métrica de Optimización}
Dado que el LSPM es un método conformal teóricamente válido sin hiperparámetros libres (la versión studentizada es la única apropiada según \textcite{Vovk2022}), la optimización se limita a seleccionar $p$ mediante la heurística anterior.

\subparagraph{Protocolo de Congelamiento}
El método ejecuta una operación crítica: congela $p$ basado en el tamaño del conjunto de entrenamiento+calibración. Este valor congelado $p^*$ se almacena  y se reutiliza en todas las ventanas rolling subsecuentes, garantizando que la dimensionalidad de la matriz de diseño permanezca constante y previniendo data leakage.


\subsubsection{DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks}
\label{subsec:DeepAR}
\paragraph{Explicación Teórica del Modelo}

DeepAR, introducido por \textcite{Salinas2020}, representa un cambio de paradigma en el pronóstico probabilístico de series temporales al trasladar el enfoque desde el modelado individual de cada serie hacia el \textbf{aprendizaje de un modelo global} a partir de múltiples series relacionadas mediante una arquitectura de red neuronal recurrente autorregresiva.

\subparagraph{Fundamento Arquitectónico}

El modelo emplea redes LSTM (Long Short-Term Memory) para procesar secuencias temporales de forma autorregresiva. Para una serie temporal $i$ con valores $z_{i,t}$, DeepAR modela la distribución condicional del futuro dado el pasado:

\begin{equation}
P(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T})
\end{equation}

donde $t_0$ denota el punto de inicio del horizonte de predicción, $z_{i,1:t_0-1}$ representa el rango de condicionamiento, $z_{i,t_0:T}$ los valores futuros, y $x_{i,1:T}$ son covariables conocidas.

La arquitectura factoriza esta distribución mediante el producto de verosimilitudes condicionales:

\begin{equation}
Q_{\Theta}(z_{i,t_0:T} \mid z_{i,1:t_0-1}, x_{i,1:T}) = \prod_{t=t_0}^{T} Q_{\Theta}(z_{i,t} \mid z_{i,1:t-1}, x_{i,1:T})
\end{equation}

donde cada factor está parametrizado por la salida de la red recurrente:

\begin{equation}
Q_{\Theta}(z_{i,t} \mid z_{i,1:t-1}, x_{i,1:T}) = \ell(z_{i,t} \mid \theta(h_{i,t}, \Theta))
\end{equation}

El estado oculto $h_{i,t}$ se actualiza recursivamente mediante:

\begin{equation}
h_{i,t} = h(h_{i,t-1}, z_{i,t-1}, x_{i,t}, \Theta)
\end{equation}

donde $h(\cdot)$ es una función implementada por una red LSTM multicapa con parámetros $\Theta$.

\subparagraph{Naturaleza Autorregresiva}

En cada paso temporal $t$, la red consume como entrada el valor observado del paso anterior $z_{i,t-1}$ junto con las covariables $x_{i,t}$ y el estado oculto previo $h_{i,t-1}$. Durante el entrenamiento, todos los valores $z_{i,t}$ en el rango de predicción son conocidos. Durante la predicción, para $t \geq t_0$, los valores futuros se reemplazan por muestras $\tilde{z}_{i,t} \sim \ell(\cdot \mid \theta(h_{i,t}, \Theta))$ generadas por el propio modelo, que se retroalimentan para calcular el siguiente estado oculto mediante \textit{muestreo ancestral}.

\subparagraph{Modelado Probabilístico}

DeepAR no predice directamente valores futuros, sino los \textbf{parámetros de una distribución de probabilidad} $\theta(h_{i,t})$ sobre valores posibles. Para datos de valores reales, se emplea \textbf{verosimilitud Gaussiana}:

\begin{equation}
\ell_G(z \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)
\end{equation}

donde la media $\mu(h_{i,t})$ y desviación estándar $\sigma(h_{i,t})$ se obtienen mediante transformaciones de la salida de la red:

\begin{equation}
\mu(h_{i,t}) = w_{\mu}^T h_{i,t} + b_{\mu}, \quad \sigma(h_{i,t}) = \log(1 + \exp(w_{\sigma}^T h_{i,t} + b_{\sigma}))
\end{equation}

Los parámetros $\Theta$ del modelo se aprenden maximizando la log-verosimilitud:

\begin{equation}
\mathcal{L}(\Theta) = \sum_{i=1}^{N} \sum_{t=1}^{T} \log \ell(z_{i,t} \mid \theta(h_{i,t}, \Theta))
\end{equation}

Una ventaja fundamental es que el modelo es completamente observable durante el entrenamiento, no requiriendo técnicas de inferencia variacional o métodos de Monte Carlo para aproximar la función objetivo.

\subparagraph{Manejo de Escalas Heterogéneas}

DeepAR introduce un mecanismo de escalamiento que normaliza las entradas y salidas autorregresivas por un factor de escala específico de cada serie $\nu_i$:

\begin{equation}
\tilde{z}_{i,t} = \frac{z_{i,t}}{\nu_i}, \quad \nu_i = 1 + \frac{1}{t_0} \sum_{t=1}^{t_0} z_{i,t}
\end{equation}

Los parámetros de la verosimilitud se ajustan correspondientemente:

\begin{equation}
\mu_{\text{escalado}} = \nu_i \cdot \mu(h_{i,t}), \quad \sigma_{\text{escalado}} = \nu_i \cdot \sigma(h_{i,t})
\end{equation}

\subparagraph{Generación de Pronósticos Probabilísticos}

La generación de pronósticos se realiza mediante muestreo ancestral, produciendo $B$ trayectorias completas $\{\tilde{z}_{i,t_0:T}^{(b)}\}_{b=1}^{B}$. El conjunto de trayectorias representa una muestra empírica de la distribución predictiva conjunta, preservando las correlaciones temporales aprendidas por el modelo.

\subparagraph{Clasificación del Modelo}

DeepAR es un \textbf{modelo semi-paramétrico}. La componente paramétrica reside en la arquitectura LSTM con parámetros $\Theta$ que deben estimarse, y en la elección de la familia de distribuciones de verosimilitud (Gaussiana, Binomial Negativa). La componente no paramétrica emerge del muestreo ancestral, que genera distribuciones predictivas empíricas sin asumir formas funcionales rígidas para la distribución conjunta multi-paso.

\paragraph{De la Teoría a la Práctica}

La implementación de DeepAR para series temporales univariadas desarrollada en esta investigación traduce el marco teórico autorregresivo a un sistema predictivo concreto mediante adaptaciones específicas al contexto de este estudio.

\subparagraph{Adaptaciones Principales}

\textbf{(1) Simplificación de Covariables:} A diferencia del trabajo original de \textcite{Salinas2020} que asume múltiples covariables $x_{i,t}$, esta implementación \textbf{no utiliza covariables externas}. La arquitectura se reduce a su forma puramente autorregresiva:

\begin{equation}
h_{i,t} = h(h_{i,t-1}, z_{i,t-1}, \Theta)
\end{equation}

Esta simplificación elimina la dependencia de información auxiliar, centrando la comparación en la capacidad de extraer patrones de la historia temporal intrínseca.

\textbf{(2) Construcción de Ventanas:} Dado que cada configuración genera una única serie de longitud $n=252$, se emplea \textit{windowing} deslizante para generar múltiples instancias de entrenamiento. Se construyen pares $(X^{(k)}, y^{(k)})$ mediante:

\begin{equation}
X^{(k)} = [y_k, \ldots, y_{k+p-1}], \quad y^{(k)} = y_{k+p}
\end{equation}

para $k = 1, \ldots, n_{\text{train}} - p$, generando aproximadamente $n_{\text{train}} - p$ instancias de entrenamiento.

\textbf{(3) Normalización Z-Score:} En lugar del escalamiento por $\nu_i$ del original, se emplea normalización Z-score completa:

\begin{equation}
\tilde{z}_t = \frac{z_t - \mu_{\text{train}}}{\sigma_{\text{train}} + \epsilon}
\end{equation}

garantizando que las entradas a la LSTM tengan media cero y varianza unitaria. Las predicciones se des-normalizan mediante la transformación inversa.

\textbf{(4) Early Stopping:} Se reserva el 20\% final de las instancias generadas como conjunto de validación interno. El entrenamiento se detiene si la pérdida de validación no mejora durante un número de épocas de paciencia (típicamente 5), reteniendo los parámetros correspondientes a la época con menor pérdida.

\textbf{(5) Protocolo de Congelamiento:} Para evitar \textit{data leakage} en la evaluación rolling window, el modelo se entrena una única vez sobre $n_{\text{train}} = 200$ observaciones. Los parámetros de normalización $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$ y los pesos de la red $\Theta_{\text{frozen}}$ se congelan completamente. Durante la fase de evaluación rolling, para cada paso $t = 1, \ldots, 12$, se normalizan los últimos $p$ valores observados usando parámetros congelados, se calcula la predicción con $\Theta_{\text{frozen}}$ sin re-entrenamiento, y se des-normalizan las muestras predictivas. Este protocolo previene contaminación de información futura y garantiza validez estadística.

La Tabla \ref{tab:deepar_comparison} resume las diferencias metodológicas entre la implementación original y la adaptación desarrollada.

\begin{table}[htbp]
\centering
\caption{Comparativa: Teoría vs. Implementación de DeepAR}
\label{tab:deepar_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{DeepAR Original} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Contexto de aplicación & Miles de series relacionadas & Serie temporal única \\
Covariables & Múltiples features temporales & Sin covariables externas \\
Verosimilitud & Gaussiana y Binomial Negativa & Gaussiana únicamente \\
Normalización & Escalamiento por $\nu_i$ & Z-score completo \\
Muestreo de entrenamiento & Ponderado por velocidad & Uniforme sobre ventanas \\
Regularización & No especificada & Early stopping con validación interna \\
Protocolo de evaluación & Modelo único para todas las series & Congelamiento total para rolling forecast \\
Framework & MXNet & PyTorch \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Hiperparámetros Arquitectónicos}

\textbf{hidden\_size ($h$):} Dimensionalidad del estado oculto de cada celda LSTM. Controla la capacidad representacional del modelo.

\textbf{n\_lags ($p$):} Número de valores pasados utilizados como entrada autorregresiva. Similar al orden en modelos AR$(p)$.

\textbf{num\_layers ($L$):} Número de capas LSTM apiladas. Valores mayores incrementan el número de parámetros: $\Theta \propto L \times h^2$.

\textbf{epochs ($E$):} Número máximo de pasadas completas sobre el conjunto de entrenamiento. El early stopping típicamente detiene antes de alcanzar este máximo.

\textbf{lr (learning rate):} Controla el tamaño del paso en la actualización de parámetros mediante el optimizador Adam \parencite{Kingma2015}.

\subparagraph{Espacio de Búsqueda}

La optimización de hiperparámetros explora dos configuraciones estratégicamente diseñadas:

\textbf{Configuración Ligera:}
\begin{itemize}
    \item \texttt{hidden\_size}=20, \texttt{n\_lags}=10, \texttt{num\_layers}=1, \texttt{epochs}=25, \texttt{lr}=0.01
\end{itemize}

\textbf{Configuración Profunda:}
\begin{itemize}
    \item \texttt{hidden\_size}=32, \texttt{n\_lags}=15, \texttt{num\_layers}=2, \texttt{epochs}=30, \texttt{lr}=0.005
\end{itemize}

\subparagraph{Protocolo de Optimización}

La selección entre ambas configuraciones se realiza minimizando \textbf{ECRPS} sobre un conjunto de calibración temporal de 40 observaciones posteriores al entrenamiento. Formalmente:

\begin{equation}
(h^*, \text{n}_\text{lags}^*, \text{n}_\text{layers}^*, \text{epochs}^*, \text{lr}^*) = \arg\min_{(h, \text{n}_\text{lags}, \text{n}_\text{layers}, \text{epochs}, \text{lr}) \in \mathcal{H}} \frac{1}{N_{\text{cal}}} \sum_{i=1}^{N_{\text{cal}}} \text{CRPS}(\hat{F}_i, y_i)
\end{equation}

donde $\mathcal{H}$ es el conjunto de configuraciones candidatas y $\hat{F}_i$ es la distribución predictiva empírica generada por muestreo ancestral.

\subparagraph{Congelamiento de Hiperparámetros}

Una vez optimizada, la configuración seleccionada se congela:

\begin{enumerate}
    \item Estima los parámetros de normalización: $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$
    \item Entrena la red LSTM hasta convergencia (early stopping)
    \item Almacena los pesos de la red: $\Theta_{\text{frozen}}$
    \item Establece el flag 
\end{enumerate}

Durante toda la evaluación rolling window posterior, el método verifica este flag: si es \texttt{True}, omite completamente el entrenamiento y procede directamente a generar predicciones con el modelo existente. Este protocolo garantiza que no haya fuga de información futura y que las métricas de desempeño reflejen la capacidad predictiva genuina del modelo.


\subsection{Modelos adaptados de la literatura}

\subsubsection{Least Squares Prediction Machine with Weighted Residuals (LSPMW)}
\label{subsec:LSPMW}
\paragraph{Explicación Teórica del Modelo}

El \textit{Least Squares Prediction Machine with Weighted Residuals} (LSPMW) constituye una extensión del LSPM diseñada para contextos donde el supuesto de intercambiabilidad se viola por deriva distributiva (\textit{distribution drift}) o no estacionaridad. Esta variante se fundamenta en los desarrollos de \textcite{Barber2023} sobre predicción conformal no intercambiable.

\subparagraph{Fundamento Teórico: Cuantiles Ponderados}
\textcite{Barber2023} demuestran que cuando la intercambiabilidad falla, la pérdida de cobertura (\textit{coverage gap}) de un predictor conformal puede acotarse mediante la distancia de variación total entre distribuciones. Para mitigar este problema en presencia de deriva temporal, proponen sustituir la distribución empírica uniforme por una distribución empírica ponderada.

Formalmente, dado un conjunto de valores críticos (residuos conformales) $\{C_1, \dots, C_{n-1}\}$ ordenados cronológicamente, se asignan pesos temporales no uniformes $w_i$ que priorizan observaciones recientes. La función de distribución empírica ponderada se define como:
\begin{equation}
\hat{F}_n(y) = \sum_{i=1}^{n-1} \tilde{w}_i \cdot \mathbbm{1}(C_i \leq y)
\end{equation}

donde $\tilde{w}_i = w_i / \sum_{j=1}^{n-1} w_j$ son los pesos normalizados.

\subparagraph{Esquema de Decaimiento Geométrico}
Para deriva gradual, \textcite{Barber2023} recomiendan un esquema de decaimiento geométrico:
\begin{equation}
w_i = \rho^{n-1-i}, \quad \rho \in (0, 1)
\end{equation}

donde $i$ indexa los residuos en orden cronológico (de más antiguo a más reciente). El hiperparámetro $\rho$ controla la tasa de olvido:
\begin{itemize}
    \item $\rho \to 1$: Convergencia al LSPM uniforme (memoria larga)
    \item $\rho \to 0$: Concentración extrema en el pasado inmediato (memoria corta)
\end{itemize}

El peso efectivo del $i$-ésimo residuo decrece exponencialmente conforme retrocedemos en el tiempo, otorgando a la observación más reciente ($i = n-1$) peso $\rho^0 = 1$, y a la más antigua ($i = 1$) peso $\rho^{n-2}$.


\subparagraph{Construcción de la Distribución Predictiva Ponderada}

El LSPMW modifica la distribución predictiva del LSPM mediante la introducción de pesos temporales que priorizan observaciones recientes. Partiendo de los valores críticos studentizados $\{C_1, \ldots, C_{n-1}\}$ calculados según las ecuaciones del LSPM base, se asignan pesos geométricos decrecientes:

\begin{equation}
w_i = \rho^{n-1-i}, \quad i = 1, \ldots, n-1
\label{eq:lspmw_raw_weights}
\end{equation}

donde $\rho \in (0, 1]$ es el parámetro de decaimiento temporal. El índice $i$ representa el orden cronológico, con $i=1$ correspondiendo a la observación más antigua y $i=n-1$ a la más reciente. Así, $w_{n-1} = \rho^0 = 1$ (peso máximo para el residuo más reciente) y $w_1 = \rho^{n-2}$ (peso mínimo para el residuo más antiguo).

Para construir una distribución de probabilidad válida, los pesos se normalizan:

\begin{equation}
\tilde{w}_i = \frac{w_i}{\sum_{j=1}^{n-1} w_j} = \frac{\rho^{n-1-i}}{\sum_{j=1}^{n-1} \rho^{n-1-j}}
\label{eq:lspmw_normalized_weights}
\end{equation}

de manera que $\sum_{i=1}^{n-1} \tilde{w}_i = 1$.

La distribución predictiva conformal ponderada se define mediante la función de distribución acumulada empírica ponderada. Ordenando los valores críticos en orden ascendente como $C_{(1)} \leq \cdots \leq C_{(n-1)}$, y definiendo $C_{(0)} := -\infty$ y $C_{(n)} := \infty$, la distribución predictiva se expresa como:

\begin{equation}
    \bar{\Pi}_n^{\text{LSPMW}}(y) := 
    \begin{cases}
        \left[\sum_{j=1}^{i} \tilde{w}_{(j)}, \sum_{j=1}^{i+1} \tilde{w}_{(j)}\right] & \text{si } y \in (C_{(i)}, C_{(i+1)}) \text{ para } i \in \{0, 1, \ldots, n-1\} \\
        \left[\sum_{j=1}^{i'} \tilde{w}_{(j)}, \sum_{j=1}^{i''} \tilde{w}_{(j)}\right] & \text{si } y = C_{(i)} \text{ para } i \in \{1, \ldots, n-1\}
    \end{cases}
    \label{eq:lspmw_predictive_dist}
\end{equation}

donde $\tilde{w}_{(j)}$ denota el peso normalizado correspondiente al valor crítico ordenado $C_{(j)}$, y en caso de empates, $i' := \min\{j : C_{(j)} = C_{(i)}\}$ y $i'' := \max\{j : C_{(j)} = C_{(i)}\}$.

Esta representación revela que, a diferencia del LSPM uniforme donde cada salto tiene masa $1/n$, el LSPMW asigna saltos de masa variable $\tilde{w}_{(i)}$ dependiendo de la antigüedad temporal del valor crítico correspondiente. El efecto práctico es que valores críticos recientes contribuyen más significativamente a la distribución predictiva, adaptándola a derivas distributivas graduales.

Equivalentemente, la función de distribución acumulada puede expresarse como:

\begin{equation}
F_{Y_{n+1}}^{\text{LSPMW}}(y) = \sum_{i=1}^{n-1} \tilde{w}_i \cdot \mathbbm{1}\{C_i \leq y\}
\label{eq:lspmw_cdf}
\end{equation}

Esta formulación explicita que la distribución es una mezcla finita de distribuciones degeneradas (deltas de Dirac) en los valores críticos, con pesos no uniformes determinados por el esquema de decaimiento geométrico.

\subparagraph{Cuantiles Ponderados}
Para un nivel de cobertura $\alpha$, el cuantil $(1-\alpha)$ de la distribución ponderada se obtiene mediante:
\begin{equation}
q_{1-\alpha} = \inf\left\{y : \sum_{i: C_i \leq y} \tilde{w}_i \geq 1-\alpha\right\}
\end{equation}

Este cuantil ponderado adapta la región de predicción conforme la distribución subyacente cambia en el tiempo.

\subparagraph{Clasificación del Modelo}
El LSPMW es un método \textbf{no paramétrico adaptativo}. Mantiene la propiedad distribution-free del LSPM (no asume forma distribucional de errores) pero introduce un mecanismo de ponderación que adapta la distribución predictiva a cambios temporales sin modelar explícitamente la deriva.

\paragraph{De la Teoría a la Práctica}

La implementación desarrollada traduce la teoría de \textcite{Barber2023} mediante un esquema de \textbf{muestreo ponderado adaptativo} que recalcula dinámicamente los residuos conformales:

\subparagraph{Adaptación 1: Muestreo Ponderado vs. Expansión por Replicación}
A diferencia de métodos que pre-computan una distribución expandida, la implementación emplea \textit{muestreo estratificado} en tiempo real. Para generar una distribución predictiva de tamaño $N = 1000$, se ejecuta:
\begin{equation}
\tilde{R}_j \sim \text{Categorical}(\{C_1, \dots, C_n\}, \{\tilde{w}_1, \dots, \tilde{w}_n\}), \quad j = 1, \dots, N
\end{equation}

donde cada muestra $\tilde{R}_j$ se extrae de los valores críticos $\{C_i\}$ con probabilidades proporcionales a los pesos normalizados $\tilde{w}_i = w_i / \sum_k w_k$. Este enfoque es matemáticamente equivalente a muestrear de la distribución empírica ponderada $\hat{F}_n(y)$.

\subparagraph{Adaptación 2: Recálculo Dinámico de Residuos}
Contrario a esquemas de congelamiento completo, la implementación mantiene únicamente los hiperparámetros ($\rho^*$, $p^*$) fijos tras calibración. En cada paso del rolling forecast:
\begin{enumerate}
    \item Se recalculan valores críticos $\{C_1, \dots, C_m\}$ usando la ventana temporal actual.
    \item Se actualizan pesos $w_i = (\rho^*)^{m-1-i}$ para $i = 1, \dots, m$ (ajustados al tamaño $m$ de la ventana).
    \item Se normalizan: $\tilde{w}_i = w_i / \sum_j w_j$.
    \item Se genera la distribución predictiva mediante muestreo con reemplazo usando estos pesos actualizados.
\end{enumerate}

Este diseño permite que el modelo se adapte continuamente a derivas distributivas, pues los residuos más recientes (calculados con datos actuales) reciben mayor ponderación automáticamente.

\subparagraph{Adaptación 3: Protocolo de Congelamiento Parcial}
El método almacena únicamente:
\begin{itemize}
    \item \texttt{\_frozen\_rho}: Valor óptimo $\rho^*$ seleccionado por validación
    \item \texttt{\_frozen\_n\_lags}: Orden autorregresivo $p^*$ heredado de LSPM
\end{itemize}

\textit{No} se congelan los residuos conformales ni sus pesos asociados, permitiendo que la distribución predictiva refleje el estado más reciente de la serie temporal. Esta filosofía difiere del LSPM estándar, donde congelar la distribución completa es apropiado bajo intercambiabilidad, pero resulta inadecuado bajo deriva continua.

\begin{table}[h]
\centering
\caption{Comparativa: Teoría vs. Implementación del LSPMW}
\begin{tabular}{lll}
\toprule
\textbf{Aspecto} & \textbf{Teoría (Barber et al.)} & \textbf{Implementación} \\
\midrule
Salida & Cuantiles ponderados específicos & Distribución completa (1000 muestras) \\
Método de generación & Definición abstracta $\hat{F}_n(y)$ & Muestreo ponderado con reemplazo \\
Actualización & Marco teórico general & Recálculo dinámico de residuos \\
Residuos & Valores críticos conceptuales & LSPM studentizados recalculados \\
Optimización & $\rho$ teórico o fijo & Búsqueda por validación (ECRPS) \\
Congelamiento & No especificado & Solo hiperparámetros ($\rho^*$, $p^*$) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Hiperparámetro Principal: \texttt{rho} ($\rho$)}
Controla la tasa de decaimiento temporal de los pesos. Durante validación se evalúan valores en el rango $[0.90, 0.99]$:
\begin{itemize}
    \item $\rho = 0.90$: Adaptación rápida, memoria efectiva $\approx 10$ observaciones
    \item $\rho = 0.95$: Balance intermedio, memoria efectiva $\approx 20$ observaciones  
    \item $\rho = 0.99$: Adaptación lenta, cercano al LSPM uniforme
\end{itemize}

El tamaño efectivo de muestra bajo decaimiento geométrico es:
\begin{equation}
n_{\text{eff}} = \frac{1}{\sum_{i=1}^{n-1} \tilde{w}_i^2} \approx \frac{1-\rho}{1-\rho^n}
\end{equation}

\subparagraph{Métrica de Optimización}
La selección de $\rho^*$ se realiza minimizando el \textbf{ECRPS} (véase \ref{subsec:ecrps}) sobre el conjunto de validación. Para cada candidato $\rho$, se genera la distribución ponderada en cada tiempo de validación mediante muestreo estratificado y se evalúa la calibración predictiva.

\subparagraph{Protocolo de Congelamiento Parcial}
El método  ejecuta:
\begin{enumerate}
    \item Congela $p^*$ (número de lags) heredado de la clase base LSPM.
    \item Identifica y almacena $\rho^*$ óptimo del proceso de validación.
    \item Activa la bandera para uso de hiperparámetros congelados.
    \item \textit{No congela} valores críticos ni pesos, preservando adaptabilidad temporal.
\end{enumerate}

Durante la fase de prueba rolling, el método \texttt{fit\_predict()}:
\begin{enumerate}
    \item Recalcula residuos conformales $\{C_1, \dots, C_m\}$ usando $p^*$ sobre la ventana actual.
    \item Computa pesos actualizados: $w_i = (\rho^*)^{m-1-i}$ para $i = 1, \dots, m$.
    \item Normaliza: $\tilde{w}_i = w_i / \sum_j w_j$.
    \item Genera 1000 muestras mediante \texttt{np.random.choice} con probabilidades $\{\tilde{w}_i\}$.
\end{enumerate}


\subsubsection{Mondrian Conformal Predictive System (MCPS)}
\label{subsec:MCPS}
\paragraph{Explicación Teórica del Modelo}

El \textit{Mondrian Conformal Predictive System} (MCPS), formalizado por \textcite{Bostrom2021}, constituye una extensión localmente adaptativa del Sistema Predictivo Conformal estándar (SCPS). A diferencia del SCPS, que asume homogeneidad en la distribución de errores sobre todo el espacio de entrada, el MCPS reconoce que la incertidumbre predictiva varía significativamente según el régimen operativo del modelo. Este enfoque resulta especialmente relevante en aplicaciones reales donde, por ejemplo, predicciones en rangos bajos del modelo pueden exhibir patrones de error distintos a predicciones en rangos altos.

\subparagraph{Fundamento: Particionamiento Mondrian}

La innovación central radica en la estrategia de \textbf{particionamiento Mondrian} \parencite{Vovk2005}, que divide el conjunto de calibración $\mathcal{D}_c$ en subconjuntos disjuntos basándose en características compartidas de las predicciones. Sea $h: \mathcal{X} \rightarrow \mathbb{R}$ un modelo de regresión entrenado, y $B \in \mathbb{N}$ el número de bins. Se define una partición:

\begin{equation}
\mathcal{D}_c = \bigcup_{\kappa=1}^{B} \mathcal{D}_c^{\kappa}, \quad \mathcal{D}_c^{\kappa} \cap \mathcal{D}_c^{\kappa'} = \emptyset \text{ para } \kappa \neq \kappa'
\end{equation}

Cada subconjunto $\mathcal{D}_c^{\kappa}$ agrupa instancias $(x_j, y_j)$ cuyas predicciones $h(x_j)$ pertenecen al mismo rango, construido mediante cuantiles empíricos:

\begin{equation}
\mathcal{D}_c^{\kappa} = \left\{ (x_j, y_j) \in \mathcal{D}_c \,:\, q_{\frac{\kappa-1}{B}} \leq h(x_j) < q_{\frac{\kappa}{B}} \right\}
\end{equation}

donde $q_p$ denota el cuantil $p$ de las predicciones $\{h(x_j)\}_{j=1}^{N_c}$. Esta estrategia captura automáticamente heterogeneidad: instancias con predicciones similares comparten probablemente patrones de error similares.

\subparagraph{Cálculo Localizado de Scores Conformales}

Para una instancia de prueba $x$ con predicción $h(x)$, se determina su bin correspondiente $\kappa^*$:

\begin{equation}
\kappa^* = \arg\min_{\kappa} \left\{ \kappa \,:\, h(x) < q_{\frac{\kappa}{B}} \right\}
\end{equation}

Los \textbf{calibration scores} se calculan únicamente sobre el subconjunto local $\mathcal{D}_c^{\kappa^*}$:

\begin{equation}
C_j^{\kappa^*} = h(x) + (y_j - h(x_j)), \quad \forall (x_j, y_j) \in \mathcal{D}_c^{\kappa^*}
\end{equation}

Esta formulación es algebraicamente idéntica al SCPS, pero la diferencia crítica reside en que los residuos históricos provienen exclusivamente de casos con comportamiento predictivo similar. Matemáticamente, se estima la distribución condicional $P(\epsilon \mid h(x) \in \text{Bin}_{\kappa})$ localmente, en lugar de globalmente como en SCPS donde se asume $P(\epsilon \mid h(x)) = P(\epsilon)$.

\subparagraph{Construcción de la Distribución Predictiva Localizada}

El MCPS construye una distribución predictiva mediante particionamiento adaptativo del espacio de calibración, permitiendo que la incertidumbre varíe según el régimen operativo del modelo. El proceso se articula en tres etapas: particionamiento Mondrian, localización del bin, y generación de scores conformales.

\textbf{Etapa 1: Particionamiento Mondrian del Conjunto de Calibración}

Dado un conjunto de calibración $\mathcal{D}_c = \{(x_1, y_1), \ldots, (x_{N_c}, y_{N_c})\}$ y un modelo base entrenado $h: \mathcal{X} \rightarrow \mathbb{R}$, se calculan las predicciones de calibración:

\begin{equation}
\hat{y}_j^{\text{cal}} = h(x_j), \quad j = 1, \ldots, N_c
\end{equation}

El espacio de predicciones se particiona en $B$ bins mediante cuantiles empíricos. Los bordes de los bins se definen como:

\begin{equation}
\{\theta_0, \theta_1, \ldots, \theta_B\} \quad \text{donde} \quad \theta_{\kappa} = Q_{\kappa/B}\left(\{\hat{y}_1^{\text{cal}}, \ldots, \hat{y}_{N_c}^{\text{cal}}\}\right)
\label{eq:mcps_bin_edges}
\end{equation}

con $\theta_0 = -\infty$ y $\theta_B = +\infty$. Cada bin $\kappa \in \{1, \ldots, B\}$ contiene las observaciones de calibración cuyas predicciones satisfacen:

\begin{equation}
\mathcal{D}_c^{\kappa} = \left\{(x_j, y_j) \in \mathcal{D}_c : \theta_{\kappa-1} \leq \hat{y}_j^{\text{cal}} < \theta_{\kappa}\right\}
\end{equation}

Esta partición es exhaustiva y mutuamente excluyente: $\bigcup_{\kappa=1}^B \mathcal{D}_c^{\kappa} = \mathcal{D}_c$ y $\mathcal{D}_c^{\kappa} \cap \mathcal{D}_c^{\kappa'} = \emptyset$ para $\kappa \neq \kappa'$.

\textbf{Etapa 2: Localización del Punto de Prueba}

Para un nuevo punto de prueba $x_{\text{test}}$, se obtiene su predicción puntual:

\begin{equation}
\hat{y}_{\text{test}} = h(x_{\text{test}})
\end{equation}

El bin correspondiente se determina mediante:

\begin{equation}
\kappa^* = \min\left\{\kappa \in \{1, \ldots, B\} : \hat{y}_{\text{test}} < \theta_{\kappa}\right\}
\label{eq:mcps_bin_assignment}
\end{equation}

Este índice identifica el subconjunto de calibración $\mathcal{D}_c^{\kappa^*}$ que contiene observaciones con comportamiento predictivo similar al punto de prueba.

\textbf{Etapa 3: Generación de Scores Conformales Localizados}

Los scores conformales se calculan exclusivamente sobre el bin localizado $\mathcal{D}_c^{\kappa^*}$. Para cada observación $(x_j, y_j) \in \mathcal{D}_c^{\kappa^*}$, el score conformal es:

\begin{equation}
C_j^{\kappa^*} = \hat{y}_{\text{test}} + \left(y_j - \hat{y}_j^{\text{cal}}\right), \quad \forall (x_j, y_j) \in \mathcal{D}_c^{\kappa^*}
\label{eq:mcps_calibration_scores}
\end{equation}

Esta ecuación translada los residuos de calibración $\epsilon_j^{\text{cal}} = y_j - \hat{y}_j^{\text{cal}}$ hacia el nivel de predicción actual $\hat{y}_{\text{test}}$. La distribución predictiva resultante es:

\begin{equation}
Y_{\text{test}} \sim \sum_{j: (x_j, y_j) \in \mathcal{D}_c^{\kappa^*}} \frac{1}{N_c^{\kappa^*}} \cdot \delta_{C_j^{\kappa^*}}
\label{eq:mcps_predictive_dist}
\end{equation}

donde $N_c^{\kappa^*} = |\mathcal{D}_c^{\kappa^*}|$ es el tamaño del bin localizado, y $\delta_{C_j^{\kappa^*}}$ denota la distribución degenerada (delta de Dirac) en el score conformal $C_j^{\kappa^*}$.

Equivalentemente, la función de distribución acumulada empírica es:

\begin{equation}
F_{Y_{\text{test}}}^{\text{MCPS}}(y) = \frac{1}{N_c^{\kappa^*}} \sum_{j: (x_j, y_j) \in \mathcal{D}_c^{\kappa^*}} \mathbbm{1}\{C_j^{\kappa^*} \leq y\}
\label{eq:mcps_cdf}
\end{equation}

Esta formulación explicita que la distribución predictiva del MCPS es una distribución empírica uniforme sobre el conjunto de scores $\{C_1^{\kappa^*}, \ldots, C_{N_c^{\kappa^*}}^{\kappa^*}\}$, donde cada score tiene probabilidad $1/N_c^{\kappa^*}$ de ser seleccionado.


\subparagraph{Naturaleza No Paramétrica}

El MCPS es un modelo \textbf{no paramétrico}. No asume forma funcional específica para la distribución de errores ni para la relación entre predictores y respuesta. La distribución predictiva se construye enteramente a partir de datos empíricos mediante el conjunto de scores conformales, sin parámetros poblacionales a estimar. El único modelo subyacente es el regressor base $h(x)$ (que puede ser paramétrico o no), pero la construcción de intervalos conformales es completamente libre de distribución.

\paragraph{De la Teoría a la Práctica}

La implementación del MCPS para series temporales autorregresivas representa una contribución metodológica poco explorada, traduciendo el framework teórico (originalmente diseñado para datos independientes en logística \parencite{Ye2025}) hacia contextos con dependencias temporales.

\subparagraph{Adaptaciones Principales}

\textbf{(1) Construcción Dinámica de Features:} Mientras que \textcite{Ye2025} asumen features pre-existentes $x_i$ observables (ubicación, peso, transportista), en series temporales los ``objetos'' se construyen dinámicamente como ventanas deslizantes de $p$ rezagos:

\begin{equation}
x_t = [y_{t-p}, y_{t-p+1}, \ldots, y_{t-1}] \in \mathbb{R}^p
\end{equation}

Esta transformación convierte la serie univariada en una matriz autoregresiva, introduciendo dependencias inherentes entre filas. La validez se preserva bajo condiciones de \textit{mixing débil} \parencite{Yu1994}, donde observaciones suficientemente separadas son ``casi independientes''.

\textbf{(2) Binning Adaptativo:} En series con baja variabilidad, las predicciones pueden concentrarse en rangos estrechos. Se emplea binning tolerante a duplicados, donde el número efectivo de bins es:

\begin{equation}
B_{\text{efectivo}} = \left|\left\{q_{\frac{\kappa}{B}} : \kappa = 1, \ldots, B-1\right\}\right| \leq B
\end{equation}

Si el binning falla completamente (predicciones idénticas), el sistema degrada a SCPS global automáticamente.

\textbf{(3) Representación Discreta:} En lugar de generar una CDF continua con suavizado $\tau$ (como en \texttt{Crepes} \parencite{Bostrom2022}), se retornan directamente los scores $\{C_j^{\kappa^*}\}$. Esta representación es computacionalmente eficiente para calcular ECRPS:


\textbf{(4) Fallback Jerárquico:} Si el bin localizado $\kappa^*$ contiene menos de 5 observaciones, se usa el conjunto completo:

\begin{equation}
\mathcal{D}_c^{\text{efectivo}} = 
\begin{cases}
\mathcal{D}_c^{\kappa^*} & \text{si } N_c^{\kappa^*} \geq 5 \\
\mathcal{D}_c & \text{si } N_c^{\kappa^*} < 5
\end{cases}
\end{equation}

Esta heurística, no especificada en \textcite{Bostrom2021}, previene intervalos erráticamente anchos por tamaño de muestra insuficiente.

\begin{table}[htbp]
\centering
\caption{Comparativa: Teoría vs. Implementación del MCPS}
\label{tab:mcps_comparison}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5.5cm}}
\toprule
\textbf{Aspecto} & \textbf{Teoría (Ye et al., 2025)} & \textbf{Implementación (Esta Tesis)} \\
\midrule
Dominio & Logística (órdenes independientes) & Series temporales autorregresivas \\
Features & Pre-existentes observables & Dinámicas (ventanas deslizantes) \\
Librería conformal & \texttt{Crepes} \parencite{Bostrom2022} & Implementación directa de ecuaciones \\
Representación CPD & CDF continua con suavizado $\tau$ & Distribución empírica discreta exacta \\
Binning robusto & Cuantiles fijos & Fusión automática de bins colapsados \\
Fallback a SCPS & No mencionado & Automático si $N_c^{\kappa} < 5$ \\
Horizonte & Batch (predicciones simultáneas) & Secuencial (rolling forecast) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Hiperparámetros Primarios}

Dado el costo computacional de la validación cruzada completa y las restricciones del conjunto de datos, se consideran únicamente dos configuraciones de hiperparámetros:

\begin{enumerate}
    \item \textbf{Configuración conservadora:} $p = 10$ rezagos, $B = 8$ bins
    \item \textbf{Configuración flexible:} $p = 15$ rezagos, $B = 15$ bins
\end{enumerate}

Estos hiperparámetros controlan aspectos fundamentales del modelo:

\textbf{n\_lags ($p$):} Define el orden del modelo autorregresivo, determinando cuánta memoria temporal incorpora el predictor. La configuración conservadora utiliza $p=10$ mientras que la flexible extiende a $p=15$ para capturar dinámicas de más largo plazo.

\textbf{n\_bins ($B$):} Número de particiones Mondrian que controla la adaptabilidad local del ajuste de conformidad. Según \textcite{Bostrom2021}, valores muy pequeños ($B \leq 3$) degeneran el método a SCPS (sin adaptación local), mientras que valores excesivos ($B \geq 20$) fragmentan la calibración por insuficiencia muestral. La configuración conservadora emplea $B=8$ bins (tamaño esperado por bin: $\mathbb{E}[N_c^{\kappa}] \approx N_c/8$), mientras que la flexible utiliza $B=15$ para mayor adaptabilidad espacial.


\subparagraph{Optimización de Hiperparámetros}

\textbf{Estrategia de congelamiento:} El método definido entrena el modelo base XGBoost una sola vez sobre los datos de entrenamiento completos, calculando los artefactos necesarios:

\begin{itemize}
    \item Predicciones de calibración $\{h(x_j)\}_{j \in \mathcal{D}_c}$
    \item Valores observados de calibración $\{y_j\}_{j \in \mathcal{D}_c}$
    \item Bordes de bins $\{q_{\kappa/B}\}_{\kappa=0}^{B}$
\end{itemize}

Estos artefactos se reutilizan en todas las predicciones posteriores sin reentrenamiento, garantizando eficiencia computacional en rolling forecasts.

\textbf{Métrica de optimización:} Los hiperparámetros $(p, B)$ se optimizan minimizando el ECRPS sobre un conjunto de validación:

\begin{equation}
(p^*, B^*) = \arg\min_{(p,B)} \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{CRPS}(\mathcal{C}_i^{\kappa^*}, y_i)
\end{equation}

Esta optimización ocurre en la fase inicial; una vez congelados, los hiperparámetros permanecen fijos durante todo el horizonte de predicción.

\subparagraph{Parámetros del Modelo Base}

El regressor autorregresivo $h: \mathbb{R}^p \rightarrow \mathbb{R}$ se implementa mediante XGBoost \parencite{Chen2016}:

\begin{itemize}
    \item \textbf{n\_estimators:} 50 (número de árboles)
    \item \textbf{max\_depth:} 3 (previene sobreajuste, actúa como GAM)
    \item \textbf{learning\_rate:} 0.1 (convergencia estable)
    \item \textbf{objective:} \texttt{reg:squarederror} (regresión por mínimos cuadrados)
\end{itemize}

Esta parametrización conservadora previene captura de ruido, crítico cuando $p$ es grande relativo a $N_{\text{train}}$.

\subparagraph{Balance Fundamental}

La selección conjunta define un trade-off:

\begin{equation}
\text{Calidad de } h(x) \propto (1 - \text{test\_size}) \cdot f(p)
\end{equation}

\begin{equation}
\text{Precisión de CPD} \propto \text{test\_size} \cdot \frac{N_c}{B}
\end{equation}

Para series típicas con $T \approx 1000$, la configuración $(p=10, B=10, \text{test\_size}=0.25)$ resulta en $\sim$25 scores por bin, suficiente para estimación robusta según \textcite{Hyndman1996}.


\subsubsection{Adaptive Volatility Mondrian Conformal Predictive System (AV-MCPS)}
\label{subsec:AV-MCPS}
\paragraph{Explicación Teórica del Modelo}

El \textit{Adaptive Volatility Mondrian Conformal Predictive System} (AV-MCPS) constituye una extensión metodológica del MCPS estándar desarrollada específicamente para esta investigación, representando una de las contribuciones originales más significativas de la tesis. Mientras que el MCPS de \textcite{Ye2025} particiona el espacio de calibración únicamente según predicciones puntuales $h(x)$, el AV-MCPS introduce una \textbf{estratificación bidimensional} que incorpora explícitamente la volatilidad local como segunda dimensión de heterogeneidad.

\subparagraph{Motivación: Límites del Particionamiento Unidimensional}

El MCPS estándar asume implícitamente homogeneidad de varianza dentro de cada bin de predicción. Formalmente, si $\mathcal{D}_c^{\kappa}$ denota el subconjunto con predicciones en $[q_{\kappa/B}, q_{(\kappa+1)/B})$:

\begin{equation}
\text{Var}(\epsilon_i \mid h(x_i) \in \mathcal{D}_c^{\kappa}) \approx \text{constante} \quad \forall i \in \mathcal{D}_c^{\kappa}
\end{equation}

Esta suposición se viola frecuentemente en series temporales con \textbf{heterocedasticidad condicional}, donde la volatilidad de errores varía sistemáticamente: $\text{Var}(\epsilon_t) = \sigma_t^2 \neq \text{constante}$. Dos observaciones con predicciones similares $h(x_i) \approx h(x_j)$ pueden experimentar errores de magnitudes radicalmente diferentes si provienen de regímenes de volatilidad distintos.

\subparagraph{Construcción de la Distribución Predictiva Bidimensional}

El AV-MCPS construye una distribución predictiva mediante particionamiento bidimensional del espacio de calibración, permitiendo que la incertidumbre varíe simultáneamente según el nivel de predicción y el régimen de volatilidad. El proceso se articula en cuatro etapas: particionamiento Mondrian bidimensional, estimación de volatilidad local, localización de la celda, y generación de scores conformales localizados.

\textbf{Etapa 1: Particionamiento Mondrian Bidimensional del Conjunto de Calibración}

Dado un conjunto de calibración $\mathcal{D}_c = \{(x_1, y_1), \ldots, (x_{N_c}, y_{N_c})\}$ y un modelo base entrenado $h: \mathcal{X} \rightarrow \mathbb{R}$, se calculan las predicciones de calibración:

\begin{equation}
\hat{y}_j^{\text{cal}} = h(x_j), \quad j = 1, \ldots, N_c
\end{equation}

Paralelamente, se estima la volatilidad local para cada punto de calibración mediante ventana rodante de longitud $w$:

\begin{equation}
\hat{\sigma}_j = \sqrt{\frac{1}{w-1} \sum_{k=j-w}^{j-1} (y_k - \bar{y}_{j,w})^2}, \quad j = w, \ldots, N_c
\end{equation}

donde $\bar{y}_{j,w} = \frac{1}{w}\sum_{k=j-w}^{j-1} y_k$ es la media local. Para las primeras observaciones ($j < w$), se aplica backfilling usando toda la historia disponible.

El espacio de calibración se particiona según dos dimensiones. Los bordes de bins para la dimensión de predicción se definen como:

\begin{equation}
\{\theta_0^{\text{pred}}, \theta_1^{\text{pred}}, \ldots, \theta_{B_{\text{pred}}}^{\text{pred}}\} \quad \text{donde} \quad \theta_{\kappa}^{\text{pred}} = Q_{\kappa/B_{\text{pred}}}\left(\{\hat{y}_1^{\text{cal}}, \ldots, \hat{y}_{N_c}^{\text{cal}}\}\right)
\label{eq:avmcps_pred_edges}
\end{equation}

Análogamente, los bordes de bins para la dimensión de volatilidad son:

\begin{equation}
\{\theta_0^{\text{vol}}, \theta_1^{\text{vol}}, \ldots, \theta_{B_{\text{vol}}}^{\text{vol}}\} \quad \text{donde} \quad \theta_{\lambda}^{\text{vol}} = Q_{\lambda/B_{\text{vol}}}\left(\{\hat{\sigma}_1, \ldots, \hat{\sigma}_{N_c}\}\right)
\label{eq:avmcps_vol_edges}
\end{equation}

con $\theta_0^{\text{pred}} = \theta_0^{\text{vol}} = -\infty$ y $\theta_{B_{\text{pred}}}^{\text{pred}} = \theta_{B_{\text{vol}}}^{\text{vol}} = +\infty$. 

Cada celda bidimensional $(\kappa, \lambda) \in \{1, \ldots, B_{\text{pred}}\} \times \{1, \ldots, B_{\text{vol}}\}$ contiene las observaciones de calibración que satisfacen simultáneamente:

\begin{equation}
\mathcal{D}_c^{(\kappa,\lambda)} = \left\{(x_j, y_j) \in \mathcal{D}_c : 
\begin{aligned}
&\theta_{\kappa-1}^{\text{pred}} \leq \hat{y}_j^{\text{cal}} < \theta_{\kappa}^{\text{pred}} \\
&\text{y } \theta_{\lambda-1}^{\text{vol}} \leq \hat{\sigma}_j < \theta_{\lambda}^{\text{vol}}
\end{aligned}
\right\}
\label{eq:avmcps_cell_definition}
\end{equation}

Esta partición genera $B_{\text{pred}} \times B_{\text{vol}}$ celdas mutuamente excluyentes y exhaustivas: $\bigcup_{\kappa=1}^{B_{\text{pred}}} \bigcup_{\lambda=1}^{B_{\text{vol}}} \mathcal{D}_c^{(\kappa,\lambda)} = \mathcal{D}_c$ y $\mathcal{D}_c^{(\kappa,\lambda)} \cap \mathcal{D}_c^{(\kappa',\lambda')} = \emptyset$ para $(\kappa,\lambda) \neq (\kappa',\lambda')$.

\textbf{Etapa 2: Estimación de Volatilidad del Punto de Prueba}

Para un nuevo punto de prueba $x_{\text{test}}$, se obtiene su predicción puntual:

\begin{equation}
\hat{y}_{\text{test}} = h(x_{\text{test}})
\end{equation}

Simultáneamente, se estima su volatilidad local usando la ventana temporal más reciente disponible antes de la predicción:

\begin{equation}
\hat{\sigma}_{\text{test}} = \sqrt{\frac{1}{w-1} \sum_{k=t-w}^{t-1} (y_k - \bar{y}_{t,w})^2}
\end{equation}

donde $t$ denota el índice temporal del punto de prueba.

\textbf{Etapa 3: Localización de la Celda Bidimensional}

La celda correspondiente al punto de prueba se determina mediante doble asignación:

\begin{equation}
\kappa^* = \min\left\{\kappa \in \{1, \ldots, B_{\text{pred}}\} : \hat{y}_{\text{test}} < \theta_{\kappa}^{\text{pred}}\right\}
\label{eq:avmcps_pred_bin_assignment}
\end{equation}

\begin{equation}
\lambda^* = \min\left\{\lambda \in \{1, \ldots, B_{\text{vol}}\} : \hat{\sigma}_{\text{test}} < \theta_{\lambda}^{\text{vol}}\right\}
\label{eq:avmcps_vol_bin_assignment}
\end{equation}

El par $(\kappa^*, \lambda^*)$ identifica el subconjunto de calibración $\mathcal{D}_c^{(\kappa^*,\lambda^*)}$ que contiene observaciones con comportamiento predictivo y régimen de volatilidad similares al punto de prueba.

Dado que la partición bidimensional puede generar celdas con pocas observaciones, se implementa una estrategia de fallback jerárquica:

\begin{equation}
\mathcal{D}_c^{\text{efectivo}} = 
\begin{cases}
\mathcal{D}_c^{(\kappa^*,\lambda^*)} & \text{si } N_c^{(\kappa^*,\lambda^*)} \geq n_{\text{min}} \\
\mathcal{D}_c^{(\kappa^*, \cdot)} & \text{si } N_c^{(\kappa^*,\lambda^*)} < n_{\text{min}} \text{ y } N_c^{(\kappa^*, \cdot)} \geq n_{\text{min}} \\
\mathcal{D}_c & \text{en otro caso}
\end{cases}
\label{eq:avmcps_fallback}
\end{equation}

donde $\mathcal{D}_c^{(\kappa^*, \cdot)} = \bigcup_{\lambda=1}^{B_{\text{vol}}} \mathcal{D}_c^{(\kappa^*,\lambda)}$ representa el bin marginal basado únicamente en predicción, y $n_{\text{min}} = 5$ es el umbral mínimo de observaciones. Este mecanismo degrada gradualmente la resolución espacial para preservar estabilidad estadística.

\textbf{Etapa 4: Generación de Scores Conformales Localizados}

Los scores conformales se calculan exclusivamente sobre el conjunto efectivo $\mathcal{D}_c^{\text{efectivo}}$. Para cada observación $(x_j, y_j) \in \mathcal{D}_c^{\text{efectivo}}$, el score conformal es:

\begin{equation}
C_j^{(\kappa^*,\lambda^*)} = \hat{y}_{\text{test}} + \left(y_j - \hat{y}_j^{\text{cal}}\right), \quad \forall (x_j, y_j) \in \mathcal{D}_c^{\text{efectivo}}
\label{eq:avmcps_calibration_scores}
\end{equation}

Esta ecuación translada los residuos de calibración $\epsilon_j^{\text{cal}} = y_j - \hat{y}_j^{\text{cal}}$ hacia el nivel de predicción actual $\hat{y}_{\text{test}}$, pero restringiendo exclusivamente a observaciones del mismo régimen bidimensional (nivel, volatilidad). La distribución predictiva resultante es:

\begin{equation}
Y_{\text{test}} \sim \sum_{j: (x_j, y_j) \in \mathcal{D}_c^{\text{efectivo}}} \frac{1}{N_c^{\text{efectivo}}} \cdot \delta_{C_j^{(\kappa^*,\lambda^*)}}
\label{eq:avmcps_predictive_dist}
\end{equation}

donde $N_c^{\text{efectivo}} = |\mathcal{D}_c^{\text{efectivo}}|$ es el tamaño del conjunto efectivo, y $\delta_{C_j^{(\kappa^*,\lambda^*)}}$ denota la distribución degenerada (delta de Dirac) en el score conformal $C_j^{(\kappa^*,\lambda^*)}$.

Equivalentemente, la función de distribución acumulada empírica es:

\begin{equation}
F_{Y_{\text{test}}}^{\text{AV-MCPS}}(y) = \frac{1}{N_c^{\text{efectivo}}} \sum_{j: (x_j, y_j) \in \mathcal{D}_c^{\text{efectivo}}} \mathbbm{1}\{C_j^{(\kappa^*,\lambda^*)} \leq y\}
\label{eq:avmcps_cdf}
\end{equation}

Esta formulación explicita que la distribución predictiva del AV-MCPS es una distribución empírica uniforme sobre el conjunto de scores $\{C_1^{(\kappa^*,\lambda^*)}, \ldots, C_{N_c^{\text{efectivo}}}^{(\kappa^*,\lambda^*)}\}$, donde cada score tiene probabilidad $1/N_c^{\text{efectivo}}$ de ser seleccionado. La diferencia fundamental respecto al MCPS estándar radica en que este conjunto de scores proviene exclusivamente de observaciones que comparten tanto el régimen de nivel como el régimen de volatilidad con el punto de prueba, permitiendo capturar explícitamente la heterocedasticidad condicional de la serie temporal.
\subparagraph{Naturaleza No Paramétrica}

El AV-MCPS es un modelo \textbf{no paramétrico}. No asume forma funcional para la distribución de errores ni para la relación entre predictores y respuesta. La estratificación bidimensional es completamente libre de distribución, construyéndose enteramente a partir de cuantiles empíricos. La distribución predictiva se genera directamente desde scores conformales sin parámetros poblacionales.

\paragraph{De la Teoría a la Práctica}

La implementación del AV-MCPS para series temporales traduce el marco teórico bidimensional en un sistema operativo mediante decisiones de diseño específicas.

\subparagraph{Adaptaciones Principales}

\textbf{(1) Estimación Rolling de Volatilidad:} Se emplea desviación estándar rolling con ventana fija:

\begin{equation}
\hat{\sigma}_t = \sqrt{\frac{1}{w-1} \sum_{k=t-w}^{t-1} (y_k - \bar{y}_t)^2}
\end{equation}

Justificación: simplicidad computacional, robustez a shocks transitorios, e interpretabilidad directa. Para las primeras $w-1$ observaciones se aplica \textit{backfilling}:

\begin{equation}
\hat{\sigma}_t = \sqrt{\frac{1}{t-2} \sum_{k=1}^{t-1} (y_k - \bar{y}_{1:t-1})^2} \quad \text{para } t < w
\end{equation}

\textbf{(2) Binning Adaptativo Robusto:} Las series de volatilidad exhiben distribuciones asimétricas con colas pesadas. Se fusionan automáticamente bins con fronteras colapsadas:

\begin{equation}
B_{\text{vol}}^{\text{efectivo}} = \left|\left\{q_{\text{vol}}^{\lambda} : \lambda = 1, \ldots, B_{\text{vol}}-1\right\}\right| \leq B_{\text{vol}}
\end{equation}

Si el binning falla en alguna dimensión, el sistema degrada a MCPS unidimensional o SCPS global.

\textbf{(3) Representación Discreta de Scores:} Consistente con LSPM y MCPS, se retornan directamente los scores sin transformación:

\begin{equation}
\mathcal{C}^{(\kappa^*,\lambda^*)} = \left\{C_i^{(\kappa^*,\lambda^*)}\right\}_{i=1}^{N_c^{(\kappa^*,\lambda^*)}}
\end{equation}

Esta representación empírica permite cálculo eficiente de CRPS sin reconstruir CDF continua.

\textbf{(4) Protocolo de Congelamiento Bidimensional:} El método  fija simultáneamente:

\begin{itemize}
\item Parámetros del regressor XGBoost (entrenado una sola vez)
\item Bordes de bins de predicción $\{q_{\text{pred}}^{\kappa}\}_{\kappa=0}^{B_{\text{pred}}}$
\item Bordes de bins de volatilidad $\{q_{\text{vol}}^{\lambda}\}_{\lambda=0}^{B_{\text{vol}}}$
\item Artefactos de calibración: $\{h(x_i)\}$, $\{y_i\}$, $\{\sigma_i\}$
\end{itemize}

Durante evaluación rolling, para cada punto $t$ se extraen rezagos, se calcula $h(x_t)$ con modelo congelado, se estima $\sigma_t$ con ventana actual, y se asigna a celda usando bordes congelados. Este protocolo previene data leakage y garantiza validez estadística.

\begin{table}[htbp]
\centering
\caption{Comparativa: MCPS vs. AV-MCPS}
\label{tab:avmcps_vs_mcps}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{MCPS} & \textbf{AV-MCPS} \\
\midrule
Dimensiones de partición & Unidimensional (predicción) & Bidimensional (predicción + volatilidad) \\
Número de bins & $B$ & $B_{\text{pred}} \times B_{\text{vol}}$ \\
Tamaño esperado de celda & $N_c / B$ & $N_c / (B_{\text{pred}} \times B_{\text{vol}})$ \\
Captura de heterocedasticidad & Indirecta (via niveles) & Explícita (via volatilidad local) \\
Complejidad computacional & $O(\log B)$ & $O(\log B_{\text{pred}} + \log B_{\text{vol}})$ \\
Estrategia de fallback & Degradar a SCPS si $N_c^{\kappa} < 5$ & Jerárquica: 2D $\to$ 1D $\to$ SCPS \\
Hiperparámetros adicionales & Ninguno & \texttt{volatility\_window}, $B_{\text{vol}}$ \\
Casos de uso óptimos & Heterogeneidad por nivel & Series con volatilidad cambiante \\
\bottomrule
\end{tabular}
\end{table}

\subparagraph{Innovación Metodológica}

La contribución fundamental es reconocer que \textbf{la volatilidad local predice el error independientemente del nivel}. Si $e_i = |y_i - h(x_i)|$, la hipótesis subyacente es:

\begin{equation}
\mathbb{E}[e_i \mid h(x_i), \sigma_i] \neq \mathbb{E}[e_i \mid h(x_i)]
\end{equation}

El AV-MCPS explota esta estructura mediante estratificación bidimensional, logrando distribuciones predictivas que se adaptan simultáneamente al \textit{nivel} y al \textit{régimen de incertidumbre}. Esta adaptabilidad dual representa una ventaja teórica significativa, especialmente en series con volatilidad time-varying como procesos financieros, climáticos o epidemiológicos.

\paragraph{Optimización, Parámetros e Hiperparámetros}
\subparagraph{Hiperparámetros Primarios}

\textbf{n\_lags ($p$):} Orden del modelo autorregresivo.


\textbf{n\_pred\_bins ($B_{\text{pred}}$):} Resolución en dimensión de predicción.


\textbf{n\_vol\_bins ($B_{\text{vol}}$):} Resolución en dimensión de volatilidad.





\subparagraph{Configuraciones Evaluadas}

El espacio de hiperparámetros se limita a dos configuraciones estratégicamente diseñadas:

\textbf{Configuración Conservadora:} $(p=10, B_{\text{pred}}=8, B_{\text{vol}}=3)$

\textbf{Configuración Agresiva:} $(p=15, B_{\text{pred}}=10, B_{\text{vol}}=5)$

\subparagraph{Optimización de Hiperparámetros}

La selección entre ambas configuraciones se realiza minimizando \textbf{ECRPS} sobre un conjunto de validación temporal. Formalmente:

\begin{equation}
(p^*, B_{\text{pred}}^*, B_{\text{vol}}^*) = \arg\min_{(p, B_p, B_v) \in \mathcal{H}} \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{CRPS}(\mathcal{C}_i^{(\kappa^*,\lambda^*)}, y_i)
\end{equation}

donde $\mathcal{H} = \{(10,8,3), (15,10,5)\}$ es el conjunto de configuraciones candidatas. Una vez optimizada, la configuración seleccionada se congela fijando simultáneamente el modelo base XGBoost, los bordes de bins bidimensionales, y los artefactos de calibración.

\subparagraph{Parámetros del Modelo Base}

Idénticos a MCPS: XGBoost con 50 árboles, profundidad 3, learning rate 0.1, objetivo \texttt{reg:squarederror}. Esta parametrización conservadora es particularmente crítica en AV-MCPS, donde el conjunto de entrenamiento puede ser más pequeño debido a la fragmentación bidimensional del conjunto de calibración.


\subsubsection{Autoregressive Exponentially-weighted Polynomial Distribution (AREPD)}
\label{subsec:AREPD}
\paragraph{Explicación Teórica del Modelo}

El modelo \textit{Autoregressive Exponentially-weighted Polynomial Distribution} (AREPD) representa una contribución metodológica original de esta investigación, desarrollada como una extensión híbrida que combina predicción ponderada temporalmente con expansión polinomial de características autorregresivas. A diferencia de métodos puramente conformales como LSPM que utilizan regresión lineal, AREPD introduce \textbf{transformaciones no lineales polinomiales} de las entradas autorregresivas.

\subparagraph{Fundamento Matemático}

Para una serie temporal $\{Y_t\}_{t=1}^{n}$ con $p$ rezagos y grado polinomial $d$, AREPD construye una matriz de diseño expandida $\mathbf{X} \in \mathbb{R}^{(n-p) \times (1+pd)}$:

\begin{equation}
\mathbf{X}_i = \left[1, Y_{i}, Y_{i}^2, \ldots, Y_{i}^d, Y_{i+1}, Y_{i+1}^2, \ldots, Y_{i+1}^d, \ldots, Y_{i+p-1}^d\right]
\end{equation}

para $i = 1, \ldots, n-p$. Esta expansión permite capturar relaciones cuadráticas, cúbicas o de orden superior entre valores pasados y futuros sin recurrir a arquitecturas de aprendizaje profundo.

\subparagraph{Ponderación Exponencial Temporal}

AREPD asigna pesos exponencialmente decrecientes a observaciones históricas:

\begin{equation}
w_i = \rho^{n-p-i}, \quad i = 1, \ldots, n-p
\end{equation}

donde $\rho \in (0, 1)$ es el parámetro de decaimiento. Los pesos se normalizan: $\tilde{w}_i = w_i / \sum_{j=1}^{n-p} w_j$. La observación más reciente recibe peso máximo, mientras que observaciones antiguas reciben pesos progresivamente menores. La vida media efectiva es:

\begin{equation}
\tau_{1/2} = \frac{\log(2)}{\log(1/\rho)}
\end{equation}

Por ejemplo, con $\rho = 0.95$, $\tau_{1/2} \approx 13.5$ observaciones.

\subparagraph{Estimación mediante Regresión Ridge}

Los coeficientes se estiman resolviendo el problema de regresión Ridge ponderada:

\begin{equation}
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \left\{\sum_{i=1}^{n-p} \tilde{w}_i \left(y_i - \mathbf{X}_i^T \boldsymbol{\beta}\right)^2 + \lambda \|\boldsymbol{\beta}\|_2^2\right\}
\end{equation}

donde $\lambda > 0$ es el parámetro de regularización Ridge. La penalización Ridge es crítica por dos razones:

\begin{enumerate}
\item \textbf{Estabilidad numérica:} La expansión polinomial genera matrices casi singulares debido a alta correlación entre términos como $Y_{t-1}$ y $Y_{t-1}^2$
\item \textbf{Prevención de sobreajuste:} Con $p \cdot d$ características, el modelo tiene alta capacidad expresiva que requiere regularización
\end{enumerate}

La solución tiene forma cerrada:

\begin{equation}
\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^T\mathbf{W}\mathbf{X} + \lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{W}\mathbf{y}
\end{equation}

donde $\mathbf{W} = \text{diag}(\tilde{w}_1, \ldots, \tilde{w}_{n-p})$ es la matriz diagonal de pesos.

\subparagraph{Generación de Distribuciones Predictivas}

AREPD genera distribuciones predictivas mediante un enfoque \textbf{histórico-empírico}. Para predecir $Y_{n+1}$:

\begin{enumerate}
\item Construye vectores de características expandidos para todos los puntos históricos
\item Calcula predicciones puntuales históricas: $\hat{Y}_{\text{hist}} = \mathbf{X}_{\text{hist}} \hat{\boldsymbol{\beta}}$
\item Transforma a escala original: $\hat{Y}_{\text{hist}}^{\text{original}} = \hat{Y}_{\text{hist}} \cdot \sigma_{\text{train}} + \mu_{\text{train}}$
\item Utiliza $\{\hat{Y}_{\text{hist}}^{\text{original}}\}$ como muestra empírica de la distribución predictiva
\end{enumerate}

Este enfoque difiere fundamentalmente de la predicción conformal estándar. En lugar de ajustar mediante residuos de calibración, AREPD construye una distribución empírica directamente desde predicciones históricas del modelo ajustado, asumiendo que bajo estacionariedad local, estas predicciones son representativas del futuro.

\subparagraph{Clasificación del Modelo}

AREPD es un \textbf{modelo semi-paramétrico}. La componente paramétrica reside en la estructura de regresión Ridge con coeficientes $\hat{\boldsymbol{\beta}}$ que deben estimarse. La componente no paramétrica emerge del uso de la distribución empírica de predicciones históricas sin asumir formas funcionales rígidas para la distribución predictiva.

\begin{table}[htbp]
\centering
\caption{Comparativa Conceptual:AREPD vs Modelos Relacionados}
\label{tab:arepd_conceptual}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{3.5cm}%
                >{\raggedright\arraybackslash}p{3cm}%
                >{\raggedright\arraybackslash}p{4cm}}
\toprule
\textbf{Método} & \textbf{Espacio de características} & \textbf{Ponderación temporal} & \textbf{Distribución predictiva} \\
\midrule
LSPM & Lineal (rezagos) & Uniforme & Conformal (scores ajustados) \\
LSPMW & Lineal (rezagos) & Exponencial ($\rho$) & Conformal ponderada \\
AREPD & Polinomial (grado $d$) & Exponencial ($\rho$) & Histórico-empírica \\
DeepAR & No lineal (LSTM) & Uniforme & Muestreo ancestral \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Hiperparámetros Estructurales}

\textbf{n\_lags ($p$):} Número de rezagos incluidos en la matriz de diseño. Con grado $d$, la dimensión del espacio de características es $1 + p \cdot d$. Valores mayores permiten capturar dependencias de largo plazo pero reducen el tamaño del conjunto de entrenamiento e incrementan riesgo de sobreajuste.

\textbf{poly\_degree ($d$):} Grado máximo de la expansión polinomial.
\begin{itemize}
\item $d=1$: Modelo puramente lineal (regresión autorregresiva Ridge estándar)
\item $d=2$: Incluye términos cuadráticos, captura efectos de amplificación moderados
\item $d=3$: Incluye términos cúbicos, alta expresividad pero riesgo significativo de sobreajuste
\end{itemize}

Número de parámetros: $1 + p \cdot d$. Para $(p,d)=(5,2)$: 11 parámetros; para $(p,d)=(10,3)$: 31 parámetros.

\textbf{rho ($\rho$):} Parámetro de decaimiento exponencial. Vida media efectiva:
\begin{itemize}
\item $\rho = 0.90$: $\tau_{1/2} \approx 6.6$ observaciones (memoria corta, adaptación rápida)
\item $\rho = 0.95$: $\tau_{1/2} \approx 13.5$ observaciones (balance intermedio)
\item $\rho = 0.98$: $\tau_{1/2} \approx 34.3$ observaciones (memoria larga, estabilidad)
\end{itemize}

\subparagraph{Parámetro Fijo}

\textbf{alpha ($\lambda$):} Parámetro de regularización Ridge, fijo en $\lambda=0.1$. Controla el trade-off sesgo-varianza: valores cercanos a cero aproximan mínimos cuadrados no regularizados, valores grandes contraen coeficientes hacia cero. El valor $\lambda=0.1$ proporciona regularización suave sin sesgo excesivo.

\subparagraph{Espacio de Búsqueda}

La optimización explora tres configuraciones estratégicamente diseñadas:

\textbf{Configuración 1 - Estándar:}
\begin{itemize}
\item \texttt{n\_lags}=5, \texttt{rho}=0.95, \texttt{poly\_degree}=2
\end{itemize}

\textbf{Configuración 2 - Memoria Corta:}
\begin{itemize}
\item \texttt{n\_lags}=10, \texttt{rho}=0.90, \texttt{poly\_degree}=2
\end{itemize}

\textbf{Configuración 3 - Alta No Linealidad:}
\begin{itemize}
\item \texttt{n\_lags}=5, \texttt{rho}=0.98, \texttt{poly\_degree}=3
\end{itemize}

\subparagraph{Protocolo de Optimización}

La selección entre configuraciones minimiza \textbf{ECRPS} sobre el conjunto de calibración de 40 observaciones:

\begin{equation}
(p^*, \rho^*, d^*) = \arg\min_{(p, \rho, d) \in \mathcal{H}} \frac{1}{N_{\text{cal}}} \sum_{i=1}^{N_{\text{cal}}} \text{CRPS}(\hat{F}_i, y_i)
\end{equation}

donde $\mathcal{H} = \{(5,0.95,2), (10,0.90,2), (5,0.98,3)\}$ es el conjunto de configuraciones candidatas y $\hat{F}_i$ es la distribución predictiva empírica generada por el método histórico.

\subparagraph{Congelamiento de Hiperparámetros}

Una vez optimizada, la configuración se congela mediante :

\begin{enumerate}
\item Estima $\mu_{\text{frozen}}, \sigma_{\text{frozen}}$ sobre datos de entrenamiento
\item Entrena el modelo Ridge ponderado hasta convergencia
\item Almacena coeficientes: $\hat{\boldsymbol{\beta}}_{\text{frozen}}$
\item Establece flag \texttt{\_is\_frozen = True}
\end{enumerate}

Durante toda la evaluación rolling, \texttt{fit\_predict} verifica este flag: si es \texttt{True}, omite entrenamiento y procede directamente a predecir con el modelo existente, garantizando validez estadística sin fuga de información futura.


\subsubsection{Ensemble Conformalized Quantile Regression (EnCQR-LSTM)}
\label{subsec:EnCQR-LSTM}
\paragraph{Explicación Teórica del Modelo}

El \textit{Ensemble Conformalized Quantile Regression} (EnCQR-LSTM) constituye una síntesis metodológica avanzada que integra tres paradigmas complementarios del aprendizaje estadístico: regresión cuantílica (QR), predicción conformal (CP) y aprendizaje en ensamble. Esta arquitectura híbrida busca heredar simultáneamente la \textbf{adaptabilidad heterocedástica} de QR y las \textbf{garantías formales de cobertura} de CP, superando las limitaciones inherentes de cada enfoque por separado.

\subparagraph{Motivación y Limitaciones de Métodos Puros}

Los métodos basados únicamente en regresión cuantílica pueden generar intervalos adaptativos cuya amplitud varía localmente con la volatilidad de los datos, pero carecen de garantías formales de cobertura. En la práctica, los intervalos de predicción generados por QR tienden a ser excesivamente confiados (demasiado estrechos), resultando en coberturas empíricas significativamente inferiores al nivel nominal $(1-\alpha)$. \parencite{Jensen2022}

Por otra parte, los métodos de predicción conformal estándar garantizan cobertura marginal válida bajo intercambiabilidad, pero construyen intervalos de amplitud constante o levemente variable. Para series temporales con heterocedasticidad, donde la incertidumbre fluctúa considerablemente, estos intervalos resultan excesivamente conservadores en períodos de baja volatilidad e insuficientes en períodos de alta volatilidad.

EnCQR-LSTM aborda ambas limitaciones mediante una arquitectura de ensamble que combina estimadores LSTM de regresión cuantílica con un mecanismo de conformalización posterior.

\subparagraph{Arquitectura de Ensamble y Regresión Cuantílica}

El método presentado por \cite{Jensen2022} construye un ensamble homogéneo de $B$ modelos LSTM, cada uno entrenado sobre subconjuntos \textbf{disjuntos} del conjunto de entrenamiento $\{(x_i, y_i)\}_{i=1}^{T}$. La partición se define como:

\begin{equation}
S_b = \{(x_i, y_i) : i \in [(b-1)T_b + 1, bT_b]\}, \quad b = 1, \ldots, B
\end{equation}

donde $T_b = \lfloor T/B \rfloor$. Esta fragmentación disjunta es fundamental para construir residuos fuera de muestra válidos, garantizando que cada observación está excluida de al menos un modelo del ensamble.

Cada modelo LSTM estima simultáneamente múltiples funciones cuantílicas condicionales mediante la minimización de la pérdida pinball agregada:

\begin{equation}
\mathcal{L}_{\text{pinball}}(\theta_b) = \frac{1}{|S_b| \cdot |\mathcal{T}|} \sum_{(x_i, y_i) \in S_b} \sum_{\tau \in \mathcal{T}} \rho_{\tau}(y_i - \hat{q}_{\tau}^{(b)}(x_i))
\end{equation}

donde $\mathcal{T} = \{0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99\}$ es el conjunto de cuantiles objetivo, y $\rho_{\tau}(u) = \max(\tau \cdot u, (\tau - 1) \cdot u)$ es la función de pérdida pinball que penaliza asimétricamente los errores según el cuantil objetivo.

\subparagraph{Predicción Leave-One-Out y Scores de Conformidad}

Una vez entrenados los $B$ modelos, EnCQR construye predicciones leave-one-out (LOO) para cada observación de entrenamiento. Para una observación $i$ en el subconjunto $S_b$, se agregan las predicciones de todos los modelos que no incluyeron esa observación:

\begin{equation}
\hat{q}_{\tau}^{(-i)}(x_i) = \frac{1}{B-1}\sum_{b': i \notin S_{b'}} \hat{q}_{\tau}^{(b')}(x_i)
\end{equation}

Este procedimiento LOO reemplaza el requisito de intercambiabilidad por un esquema de validación cruzada que genera residuos genuinamente fuera de muestra, esencial para series temporales.

EnCQR introduce scores de conformidad \textbf{asimétricos} que cuantifican separadamente el error de cobertura en las colas inferior y superior:

\begin{equation}
\begin{aligned}
E_i^{\text{lo}} &= \hat{q}_{\tau_{\text{lo}}}^{(-i)}(x_i) - y_i \\
E_i^{\text{hi}} &= y_i - \hat{q}_{\tau_{\text{hi}}}^{(-i)}(x_i)
\end{aligned}
\end{equation}

Esta formulación asimétrica permite que las distribuciones de errores para los cuantiles inferior y superior tengan formas diferentes, crucial en presencia de asimetría sistemática.

\subparagraph{Conformalización y Distribución Predictiva}

Para una nueva observación $x_{T+1}$, el ensamble completo genera predicciones agregadas que se conformalizan mediante:

\begin{equation}
\hat{C}_{\alpha}(x_{T+1}) = \left[\hat{q}_{\tau_{\text{lo}}}(x_{T+1}) - \omega^{\text{lo}}, \hat{q}_{\tau_{\text{hi}}}(x_{T+1}) + \omega^{\text{hi}}\right]
\end{equation}

donde $\omega^{\text{lo}} = Q_{1-\alpha}(\{E_i^{\text{lo}}\}_{i=1}^{T})$ y $\omega^{\text{hi}} = Q_{1-\alpha}(\{E_i^{\text{hi}}\}_{i=1}^{T})$ son los cuantiles $(1-\alpha)$ empíricos de las distribuciones de scores.

Una innovación clave de la implementación es el ajuste de una distribución \textbf{Skew-Normal} paramétrica a los cuantiles conformalizados. La función de densidad es:

\begin{equation}
f(x; \mu, \sigma, \alpha) = \frac{2}{\sigma} \phi\left(\frac{x - \mu}{\sigma}\right) \Phi\left(\alpha \frac{x - \mu}{\sigma}\right)
\end{equation}

donde $\phi(\cdot)$ y $\Phi(\cdot)$ son la densidad y distribución acumulada Normal estándar. Los parámetros $(\mu, \sigma, \alpha)$ se estiman minimizando:

\begin{equation}
(\hat{\mu}, \hat{\sigma}, \hat{\alpha}) = \arg\min_{(\mu, \sigma, \alpha)} \sum_{i=1}^{|\mathcal{T}|} \left(\hat{q}_{\tau_i}^{\text{conf}} - F^{-1}_{\text{SN}}(\tau_i; \mu, \sigma, \alpha)\right)^2
\end{equation}

Este ajuste garantiza unimodalidad y permite generar $M = 1000$ muestras de la distribución predictiva completa.

\subparagraph{Naturaleza del Modelo}

EnCQR-LSTM es un modelo \textbf{semi-paramétrico}. La arquitectura LSTM y la función de pérdida pinball son completamente no paramétricas respecto a la distribución de $Y|X$. Sin embargo, la fase final de ajuste Skew-Normal introduce una componente paramétrica para suavizar la distribución predictiva. No obstante, las garantías de cobertura provienen del mecanismo conformal no paramétrico, no del ajuste distribucional.

\subparagraph{Propiedades Teóricas}

Bajo el supuesto de que el proceso de error es estacionario y fuertemente mezclante, EnCQR garantiza \parencite{Jensen2022}:

\begin{enumerate}
\item \textbf{Cobertura marginal válida:} $\mathbb{P}\{Y_{T+1} \in \hat{C}_{\alpha}(X_{T+1})\} \geq 1 - \alpha + O(1/T)$
\item \textbf{Adaptabilidad heterocedástica:} La amplitud varía localmente con $x_{T+1}$ a través de $\hat{q}_{\tau}(x_{T+1})$
\item \textbf{Robustez ante especificación incorrecta:} Incluso con modelo LSTM mal especificado, la conformalización mantiene cobertura válida
\end{enumerate}

\paragraph{De la Teoría a la Práctica}

La implementación de EnCQR-LSTM para esta investigación introduce adaptaciones específicas respecto al marco teórico original, priorizando factibilidad computacional y robustez empírica.

\subparagraph{Adaptaciones Principales}

\textbf{(1) Tamaño de Ensamble Reducido:} Se utiliza $B=3$ modelos en lugar de $B \geq 5$ como sugiere la literatura. Esta reducción responde al trade-off entre diversidad del ensamble y tamaño de subconjuntos de entrenamiento. Para series de longitud $T \approx 200-500$, $B=3$ genera subconjuntos de $65-165$ observaciones, suficientes para entrenar LSTMs efectivos sin fragmentación excesiva.

\textbf{(2) Conjunto de Cuantiles Reducido:} En lugar de estimar 19+ cuantiles, se limita a $\mathcal{T} = \{0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99\}$ (9 cuantiles). Esta reducción disminuye la complejidad de la capa de salida del LSTM y acelera el entrenamiento, manteniendo resolución suficiente para caracterizar la distribución predictiva.

\textbf{(3) Ajuste Skew-Normal como Suavizado:} El paper original no especifica cómo generar distribuciones continuas desde cuantiles discretos. La implementación introduce el ajuste Skew-Normal como mecanismo de suavizado, evitando interpolación lineal que puede generar distribuciones bimodales artificiales. Si la optimización falla, se emplea fallback a distribución Normal estándar.

\textbf{(4) Actualización de Scores Deshabilitada:} El mecanismo de ventana deslizante para actualizar scores conformales ($s=24$) no se implementa en la versión evaluada. Todos los scores se calculan una sola vez durante la fase de calibración y permanecen congelados. Esta simplificación prioriza reproducibilidad y reduce complejidad computacional.

\textbf{(5) Arquitectura LSTM Simplificada:} Se emplea $L=2$ capas LSTM con 32 unidades cada una, sin mecanismos avanzados como attention o skip connections. La regularización se limita a dropout ($p=0.1$) y penalización L2 implícita en Adam.

\begin{table}[htbp]
\centering
\caption{Comparativa: Teoría vs. Implementación de EnCQR-LSTM}
\label{tab:encqr_theory_vs_practice}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{4cm}%
                >{\raggedright\arraybackslash}p{5cm}%
                >{\raggedright\arraybackslash}p{5cm}}
\toprule
\textbf{Aspecto} & \textbf{Teoría Original} & \textbf{Implementación} \\
\midrule
Tamaño de ensamble ($B$) & $B \geq 5$ recomendado & $B = 3$ fijo \\
Cuantiles estimados & 19+ cuantiles para alta resolución & 9 cuantiles estratégicos \\
Distribución predictiva & No especificada & Ajuste Skew-Normal paramétrico \\
Actualización de scores & Ventana deslizante cada $s$ observaciones & Deshabilitada (scores congelados) \\
Arquitectura LSTM & No especificada & 2 capas, 32 unidades, dropout 0.1 \\
Protocolo de congelamiento & Scores adaptativos & Congelamiento total (modelos + scores) \\
Complejidad computacional & Alta (múltiples entrenamientos + updates) & Reducida (sin actualizaciones) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimización, Parámetros e Hiperparámetros}

\subparagraph{Hiperparámetros del Modelo}

\textbf{n\_lags ($N_x$):} Longitud de ventana temporal de entrada. Define cuántos valores históricos se usan para predecir el siguiente.

\textbf{units ($N_u$):} Número de unidades (dimensión del estado oculto) en cada capa LSTM. Controla la capacidad expresiva del modelo.

\textbf{epochs:} Número de pasadas sobre el conjunto de entrenamiento. Se implementa early stopping con paciencia de 50 épocas para prevenir sobreajuste.

\textbf{Parámetros Fijos:}
\begin{itemize}
\item $B = 3$ (número de modelos en ensamble)
\item $L = 2$ (número de capas LSTM)
\item $\text{lr} = 0.005$ (tasa de aprendizaje Adam)
\item $\text{batch\_size} = 16$
\item $\text{dropout} = 0.1$
\item $\alpha = 0.05$ (nivel de error nominal, cobertura 95\%)
\item $\text{num\_samples} = 1000$ (muestras de distribución Skew-Normal)
\end{itemize}

\subparagraph{Espacio de Búsqueda Restringido}

Dada la alta complejidad computacional de entrenar ensambles de LSTMs, el espacio de hiperparámetros se limita estratégicamente a dos configuraciones:

\textbf{Configuración 1 (Conservadora):} $(N_x=10, N_u=24, \text{epochs}=20)$


\textbf{Configuración 2 (Estándar):} $(N_x=20, N_u=32, \text{epochs}=25)$


\subparagraph{Optimización y Congelamiento}

La selección entre ambas configuraciones se realiza minimizando \textbf{ECRPS} sobre un conjunto de validación temporal separado. Formalmente:

\begin{equation}
(N_x^*, N_u^*, \text{epochs}^*) = \arg\min_{(N_x, N_u, e) \in \mathcal{H}} \frac{1}{N_{\text{val}}} \sum_{i=1}^{N_{\text{val}}} \text{CRPS}(\mathcal{F}_i, y_i)
\end{equation}

donde $\mathcal{H} = \{(10, 24, 20), (20, 32, 25)\}$ y $\mathcal{F}_i$ es la distribución predictiva para la observación $i$.

Una vez optimizada la configuración, se ejecuta el protocolo de congelamiento completo mediante , que fija permanentemente:

\begin{itemize}
\item Los pesos $\{\theta_b\}_{b=1}^{B}$ de los $B$ modelos LSTM
\item Los parámetros de normalización MinMax: $(y_{\min}, y_{\max})$
\item Las distribuciones empíricas de scores conformales: $\{E_i^{\text{lo}}\}_{i=1}^{T}$ y $\{E_i^{\text{hi}}\}_{i=1}^{T}$
\end{itemize}

Durante la evaluación rolling window, para cada punto de prueba $t = T+1, \ldots, T+T'$, se extrae la ventana de entrada, se normalizan los datos con parámetros congelados, se generan predicciones de los $B$ modelos congelados, se agregan mediante media aritmética, se conformalizan usando scores congelados, se des-normalizan, y se ajusta la distribución Skew-Normal final. Este protocolo garantiza ausencia total de data leakage y evaluación justa comparable con otros métodos.

\subparagraph{Justificación del Espacio Reducido}

La decisión de limitar el espacio de búsqueda a solo 2 configuraciones se fundamenta en:

\begin{enumerate}
\item \textbf{Costo computacional prohibitivo:} Cada configuración requiere entrenar $B=3$ LSTMs completos. Una búsqueda exhaustiva sobre $\{10,15,20\} \times \{24,32,64\} \times \{20,25,30\}$ implicaría 27 entrenamientos por serie, inviable para el benchmark de 100 series.

\item \textbf{Evidencia de rendimientos decrecientes:} Experimentos piloto mostraron que configuraciones intermedias rara vez superaban los extremos, sugiriendo un comportamiento bimodal del desempeño.

\item \textbf{Priorización de diversidad metodológica:} Los recursos computacionales se asignan a evaluar múltiples arquitecturas fundamentalmente diferentes (LSPM, MCPS, AREPD, DeepAR, EnCQR) en lugar de explorar exhaustivamente el espacio de un solo método.
\end{enumerate}

Esta estrategia de optimización restringida permite evaluar el potencial de EnCQR-LSTM manteniendo el estudio computacionalmente viable, reconociendo que representaciones más sofisticadas (búsqueda bayesiana, algoritmos genéticos) quedan fuera del alcance de esta investigación.

% ============================================================================
% SIMULACIONES COMPLEMENTARIAS
% ============================================================================




\subsection{Contribuciones de este trabajo en modelación}

El desarrollo de esta investigación ha dado lugar a contribuciones metodológicas que representan extensiones en el dominio de la predicción distribucional en series temporales. Estas contribuciones abarcan desde adaptaciones directas de técnicas establecidas hasta propuestas completamente originales que combinan múltiples paradigmas teóricos.

\subsubsection{Aplicación Directa de LSPM a Series Temporales}

La primera contribución consiste en la aplicación directa del Least Squares Prediction Machine (LSPM), descrito en la Sección~\ref{subsec:LSPM}, al contexto de series temporales autorregresivas. Mientras que el marco teórico original de Vovk se desarrolló para datos intercambiables con vectores de características independientes, este trabajo implementa una transformación dinámica que convierte observaciones temporales consecutivas en objetos autorregresivos mediante ventanas deslizantes de rezagos.

Este enfoque se adoptó para contrastar el desempeño de la LSPM frente al modelo LSPMW, verificando así la eficacia de la alternativa con ponderaciones aplicada, en este caso, a la generación de distribuciones de probabilidad completas.


\subsubsection{Desarrollo de Métodos Conformales Adaptativos: MCPS y AV-MCPS}

La segunda contribución consiste en el desarrollo de dos métodos completamente originales inspirados en el paradigma de particionamiento Mondrian: el Mondrian Conformal Predictive System (MCPS) y el Adaptive Volatility Mondrian Conformal Predictive System (AV-MCPS), descritos en las Secciones~\ref{subsec:MCPS} y~\ref{subsec:AV-MCPS} respectivamente. Tomando inspiración del trabajo de Ye et al. sobre aplicaciones logísticas, estos métodos trasladan la lógica de estratificación espacial desde la predicción de intervalos hacia la generación de distribuciones predictivas completas en series temporales.

El MCPS implementa particionamiento unidimensional basado en predicciones puntuales del modelo base, permitiendo que las distribuciones conformales capturen heterocedasticidad local sin asumir formas paramétricas. El AV-MCPS representa una innovación metodológica más profunda al introducir estratificación bidimensional que incorpora volatilidad local como segunda dimensión de heterogeneidad, reconociendo que observaciones con niveles predichos similares pueden exhibir errores de magnitudes radicalmente diferentes bajo regímenes de incertidumbre distintos.


\subsubsection{Método Híbrido AREPD: Expansión Polinomial Ponderada}

La tercera contribución consiste en el desarrollo del Autoregressive Exponentially-weighted Polynomial Distribution (AREPD), descrito en la Sección~\ref{subsec:AREPD}, un método completamente original que sintetiza tres paradigmas: expansión polinomial de características autorregresivas, ponderación exponencial temporal y generación de distribuciones histórico-empíricas. Este enfoque híbrido aborda simultáneamente la necesidad de capturar relaciones no lineales entre valores pasados y futuros, adaptarse a derivas distributivas graduales, y generar distribuciones predictivas sin recurrir a arquitecturas de aprendizaje profundo.

La innovación fundamental radica en la filosofía de "distribución histórico-empírica", donde las predicciones pasadas del modelo ponderado se interpretan directamente como muestras de la distribución predictiva futura. Esta estrategia contrasta con la lógica conformal estándar basada en residuos de calibración, sacrificando garantías teóricas formales a cambio de mayor adaptabilidad y eficiencia computacional bajo el supuesto de estacionariedad local inducida por ponderación exponencial.


\subsubsection{Extensión de Métodos de Intervalos hacia Distribuciones Completas}

La cuarta contribución consiste en la extensión sistemática de dos métodos concebidos originalmente para predicción de intervalos hacia la generación de distribuciones predictivas integrales: LSPMW y EnCQR-LSTM, descritos en las Secciones~\ref{subsec:LSPMW} y~\ref{subsec:EnCQR-LSTM} respectivamente. Mientras que sus formulaciones teóricas originales se limitan a caracterizar regiones de predicción con niveles de confianza específicos, este trabajo los reinterpreta como caracterizaciones parciales de distribuciones subyacentes que pueden completarse mediante técnicas de reconstrucción.



\section{Metodología de Comparación}
\label{sec:metodologia_comparacion}

La evaluación de los métodos de pronóstico probabilístico propuestos se realiza mediante dos enfoques complementarios: experimentos de simulación controlada y aplicación a datos reales. Cada enfoque requiere adaptaciones específicas tanto en las métricas de evaluación como en los procedimientos de comparación estadística.

\subsection{Evaluación en Escenarios Simulados}

En los experimentos de simulación, se generan series temporales a partir de modelos conocidos (ARMA, ARIMA, SETAR) con parámetros y distribuciones de ruido especificados. Esta configuración controlada permite acceder a la distribución verdadera del siguiente valor $Y_{t+1}$ dado el historial observado mediante muestreo directo del proceso generador.

Para cada escenario de simulación y en cada paso temporal $t$ de la ventana de evaluación, se comparan las distribuciones predictivas generadas por los métodos evaluados contra la distribución teórica verdadera mediante el ECRPS. Específicamente, si $F_A^{(t)}$ denota la distribución predictiva del método $A$ en el paso $t$ y $G^{(t)}$ la distribución verdadera, se calcula $\text{ECRPS}(F_A^{(t)}, G^{(t)})$ según la ecuación \eqref{eq:ecrps_expectation}.

El conjunto completo de escenarios simulados abarca múltiples configuraciones de modelos, distribuciones de ruido y niveles de varianza. Dado que diferentes escenarios presentan escalas heterogéneas, se aplica la normalización mediante Z-score (ecuación \eqref{eq:zscore}) calculando para cada método su desviación respecto a la media y desviación estándar de todos los métodos en ese escenario particular. Esta transformación permite identificar fortalezas y debilidades específicas de cada método según las características del proceso generador.

Para establecer la significancia estadística de las diferencias observadas entre pares de métodos, se emplea el test de Diebold-Mariano modificado (ecuación \eqref{eq:dm_modified}) con corrección para muestras pequeñas propuesta por \cite{Harvey1997}. Los diferenciales de pérdida $d_t = \text{ECRPS}(F_A^{(t)}, G^{(t)}) - \text{ECRPS}(F_B^{(t)}, G^{(t)})$ se calculan para cada paso $t$, y el estadístico resultante se compara contra la distribución $t$ de Student con $(n-1)$ grados de libertad. Dado que se realizan múltiples comparaciones entre los nueve métodos evaluados, se aplica la corrección de Bonferroni al nivel de significancia: si se contrastan $K$ pares de métodos al nivel global $\alpha = 0.05$, cada test individual se realiza al nivel $\alpha/K$ para controlar la tasa de error familiar.

\subsection{Evaluación con Datos Reales}

En aplicaciones con datos reales, como el conjunto de datos Electricity, no se dispone de la distribución verdadera. La evaluación se fundamenta en el CRPS, que compara cada distribución predictiva $F_t$ contra el valor puntual observado $y_t$ según la ecuación \eqref{eq:crps_integral}.

Para cada método evaluado, se calculan los valores $\text{CRPS}(F_t, y_t)$ a lo largo de la ventana de evaluación de $T$ pasos. Las comparaciones entre métodos se realizan mediante el test de Diebold-Mariano, construyendo los diferenciales de pérdida $d_t = \text{CRPS}(F_A^{(t)}, y_t) - \text{CRPS}(F_B^{(t)}, y_t)$ y aplicando la misma corrección de Bonferroni utilizada en escenarios simulados.

El análisis se complementa con dos herramientas diagnósticas aplicadas individualmente a cada método:

\begin{enumerate}
\item \textbf{Análisis PIT:} Para cada método, se calculan los valores $p_t = F_t(y_t)$ según la ecuación \eqref{eq:pit} y se construye un histograma con los $T$ valores obtenidos. La proximidad del histograma a la distribución uniforme se evalúa visualmente, identificando patrones de descalibración (forma de U indica subdispersión, forma de $\cap$ indica sobredispersión, sesgos indican errores sistemáticos).

\item \textbf{Curvas de Confiabilidad:} Se selecciona un conjunto de umbrales $\{u_1, \ldots, u_K\}$ cubriendo el rango de valores observados. Para cada método y cada umbral $u_k$, se calcula la probabilidad pronosticada promedio $\hat{p}_k = T^{-1}\sum_{t=1}^T F_t(u_k)$ y la frecuencia observada $\bar{o}_k = T^{-1}\sum_{t=1}^T \mathbbm{1}\{y_t \leq u_k\}$. La curva resultante se contrasta visualmente con la diagonal de 45 grados, cuantificando desviaciones mediante el error cuadrático medio entre $\hat{p}_k$ y $\bar{o}_k$.
\end{enumerate}

Esta metodología integrada proporciona una evaluación comprehensiva que combina precisión puntual (vía CRPS), calibración probabilística (vía PIT y curvas de confiabilidad) y comparaciones estadísticamente rigurosas (vía tests de Diebold-Mariano con corrección de Bonferroni).





\section{Simulaciones Complementarias}
\label{sec:simulaciones_complementarias}

Además del diseño principal descrito en la Sección~\ref{sec:diseño_experimental}, se implementaron cinco conjuntos de simulaciones complementarias diseñadas para resolver dilemas metodológicos específicos sobre el preprocesamiento, la persistencia de los datos, la arquitectura de la muestra y la propagación de la incertidumbre en horizontes lejanos. Estas simulaciones abordan preguntas fundamentales que no pueden responderse mediante el diseño principal debido a su estructura particular de ventana rodante a un paso.

% ----------------------------------------------------------------------------
\subsection{Simulación 1: Impacto de la Diferenciación en ARIMA ($d=1$)}
\label{subsec:sim1_diferenciacion}

\subsubsection{Motivación}

Esta simulación evalúa una pregunta metodológica fundamental: ¿los métodos conformales capturan mejor la variabilidad cuando operan sobre la serie diferenciada estacionaria ($\Delta Y_t$) o sobre los niveles integrados ($Y_t$)? Esta cuestión es relevante porque, aunque la diferenciación elimina la no estacionariedad y simplifica el modelado, también puede introducir pérdida de información sobre el nivel de la serie y afectar la estructura de autocorrelación \parencite{Hyndman2021FPP3}.

\subsubsection{Diseño Experimental}

Se utilizan las 7 configuraciones ARIMA$(p,1,q)$ del diseño principal (Sección~\ref{sec:procesos_generadores}), combinadas con las 5 distribuciones de ruido y 4 niveles de varianza, generando:

\begin{equation}
N_{\text{config}} = 7 \text{ procesos} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones base}
\end{equation}

Cada configuración se ejecuta bajo dos modalidades:

\begin{enumerate}
\item \textbf{SIN\_DIFF:} El modelo recibe la serie integrada $Y_t$ directamente y genera una distribución predictiva $\hat{F}_{Y_{t+1}}$ para el siguiente valor en niveles.

\item \textbf{CON\_DIFF:} El modelo recibe la serie diferenciada $\Delta Y_t = Y_t - Y_{t-1}$. La predicción generada $\widehat{\Delta Y}_{t+1}$ se integra mediante:
\begin{equation}
\hat{Y}_{t+1} = Y_t + \widehat{\Delta Y}_{t+1}
\end{equation}
para calcular el ECRPS en el espacio de niveles, permitiendo una comparación directa con la modalidad SIN\_DIFF.
\end{enumerate}

\subsubsection{Protocolo de Evaluación}

Siguiendo el esquema de la simulación principal:
\begin{itemize}
\item Serie simulada: $n_{\text{total}} = 252$ observaciones efectivas (200 entrenamiento + 40 calibración + 12 prueba)
\item Esquema de ventana rodante con 12 pasos de predicción
\item Evaluación de los 9 modelos conformales mediante ECRPS contra la distribución teórica
\end{itemize}

Esto genera:
\begin{equation}
N_{\text{filas}} = 140 \times 2 \text{ modalidades} \times 12 \text{ pasos} = 3{,}360 \text{ evaluaciones}
\end{equation}

La distribución predictiva verdadera utilizada como referencia para calcular el ECRPS en esta simulación es idéntica a la descrita en la Sección~\ref{subsec:arima} para procesos ARIMA generales. Independientemente de si el modelo conformal recibe la serie en niveles ($Y_t$) o diferenciada ($\Delta Y_t$), la métrica ECRPS siempre se evalúa comparando la distribución predictiva estimada $\hat{F}_{Y_{t+1}}$ contra la distribución teórica verdadera $F_{Y_{t+1}}$ en el espacio de niveles.


% ----------------------------------------------------------------------------


\subsection{Simulación 2: Límites de Integración y Persistencia (Multi-D)}
\label{subsec:sim2_multi_d}

\subsubsection{Motivación}

Se investiga la estabilidad numérica de los métodos conformales ante órdenes de integración elevados $d \in \{1, 2, 3, 4, 5, 6, 7, 10\}$. A medida que $d$ aumenta, la serie integrada $Y_t$ exhibe una persistencia extrema y rangos de valores cada vez más amplios, lo que puede desestabilizar modelos que no utilizan diferenciación previa adecuada.

\subsubsection{Diseño Experimental}

A partir de las 7 configuraciones ARMA$(p,q)$ base, se genera el proceso ARIMA$(p,d,q)$ mediante:

\begin{equation}
Y_t = \sum_{j=0}^{d-1} \nabla^j W_t, \quad \text{donde } W_t \sim \text{ARMA}(p,q)
\end{equation}

Con la estructura completa:
\begin{equation}
N_{\text{config}} = 7 \text{ ARMA} \times 8 \text{ valores de } d \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 1{,}120 \text{ configuraciones}
\end{equation}

Cada configuración se evalúa bajo las dos modalidades (SIN\_DIFF y CON\_DIFF) en 12 pasos:

\begin{equation}
N_{\text{filas}} = 1{,}120 \times 2 \times 12 = 26{,}880 \text{ evaluaciones}
\end{equation}

\subsubsection{Distribución Predictiva Verdadera}

La distribución predictiva verdadera para esta simulación se construye mediante un procedimiento de integración múltiple que generaliza el caso $d=1$ descrito en la Sección~\ref{subsec:arima}. Para un proceso ARIMA$(p,d,q)$ con orden de integración arbitrario $d \geq 1$, la distribución de referencia $F_{Y_{t+1}}$ se obtiene en dos etapas fundamentales.

\textbf{Etapa 1: Generación de la distribución base ARMA.} Dado que la serie diferenciada $d$ veces sigue un proceso ARMA estacionario, $W_t = \nabla^d Y_t \sim \text{ARMA}(p,q)$, primero se genera la distribución del siguiente incremento $W_{t+1}$ mediante la estructura ARMA subyacente. Utilizando la historia observada $\mathcal{F}_t$, se calculan muestras:

\begin{equation}
W_{t+1}^{(b)} = \sum_{i=1}^p \phi_i W_{t+1-i} + \sum_{j=1}^q \theta_j \varepsilon_{t+1-j} + \varepsilon_{t+1}^{(b)}, \quad b = 1, \ldots, B
\label{eq:multid_arma_samples}
\end{equation}

donde $\varepsilon_{t+1}^{(b)} \sim F(0, \sigma^2)$ son errores futuros independientes generados según la distribución de ruido especificada.

\textbf{Etapa 2: Integración múltiple hacia el espacio de niveles.} Las muestras $\{W_{t+1}^{(b)}\}$ corresponden a la serie diferenciada $d$ veces. Para recuperar la distribución de $Y_{t+1}$ en el espacio de niveles originales, se aplica un proceso de integración recursiva que invierte las $d$ diferenciaciones sucesivas:

\begin{equation}
Y_{t+1}^{(b)} = \sum_{k=0}^{d-1} \nabla^k Y_t + W_{t+1}^{(b)}
\label{eq:multid_integration}
\end{equation}

Esta fórmula puede expresarse de forma recursiva almacenando los últimos valores en cada nivel de diferenciación $\{\nabla^0 Y_t, \nabla^1 Y_t, \ldots, \nabla^{d-1} Y_t\}$ y aplicando:

\begin{equation}
\begin{aligned}
Z^{(0)} &= W_{t+1}^{(b)} \\
Z^{(k)} &= \nabla^{d-k} Y_t + Z^{(k-1)}, \quad k = 1, \ldots, d \\
Y_{t+1}^{(b)} &= Z^{(d)}
\end{aligned}
\label{eq:multid_recursive_integration}
\end{equation}

Para el caso particular $d=1$, esta formulación se reduce a $Y_{t+1}^{(b)} = Y_t + W_{t+1}^{(b)}$, recuperando exactamente la ecuación~\eqref{eq:arima_level_samples}. Para $d > 1$, el procedimiento mantiene la misma estructura conceptual pero requiere integración múltiple para capturar la persistencia acumulada en todos los niveles de diferenciación.

La distribución predictiva verdadera resultante es:

\begin{equation}
F_{Y_{t+1}}^{\text{Multi-D}}(y) = \frac{1}{B} \sum_{b=1}^B \mathbbm{1}\{Y_{t+1}^{(b)} \leq y\}
\label{eq:multid_true_cdf}
\end{equation}


\subsubsection{Hipótesis}

Se hipotetiza que para $d \geq 4$, el rango explosivo de $Y_t$ desestabilizará a los modelos en modalidad SIN\_DIFF, mientras que la modalidad CON\_DIFF mantendrá estabilidad numérica al operar en el espacio diferenciado acotado.

% ----------------------------------------------------------------------------


\subsection{Simulación 3: Efectos del Tamaño Muestral Absoluto}
\label{subsec:sim3_tamano_muestral}

\subsubsection{Motivación}

Esta simulación caracteriza la tasa de convergencia de las distribuciones predictivas empíricas hacia la densidad teórica a medida que el volumen de datos aumenta. Permite cuantificar el trade-off entre calidad de estimación (que mejora con más datos de entrenamiento) y precisión de calibración (que mejora con más datos de calibración). A diferencia del diseño principal que mantiene fijos los tamaños $n_{\text{train}} = 200$ y $n_{\text{calib}} = 40$, aquí se explora sistemáticamente el espacio de tamaños absolutos manteniendo una proporción fija entre entrenamiento y calibración.

\subsubsection{Diseño Experimental}

Se evalúan los tres tipos de procesos (ARMA, ARIMA, SETAR) con sus 7 configuraciones cada uno, bajo cinco tamaños muestrales totales manteniendo una proporción fija de aproximadamente 83\% para entrenamiento y 17\% para calibración:

\begin{table}[htbp]
\centering
\caption{Tamaños muestrales evaluados con proporción fija}
\label{tab:combinaciones_muestrales}
\begin{tabular}{lcccc}
\toprule
\textbf{Etiqueta} & $n_{\text{train}}$ & $n_{\text{calib}}$ & $n_{\text{total}}$ & \textbf{Proporción} \\
\midrule
N=120 & 100 & 20 & 120 & 83:17 \\
N=240 & 199 & 41 & 240 & 83:17 \\
N=360 & 299 & 61 & 360 & 83:17 \\
N=600 & 498 & 102 & 600 & 83:17 \\
N=1200 & 996 & 204 & 1{,}200 & 83:17 \\
\bottomrule
\end{tabular}
\end{table}

Esta estructura permite evaluar el efecto del incremento simultáneo de datos de entrenamiento y calibración, manteniendo constante su proporción relativa. Esto aísla el efecto puro del tamaño muestral total sobre la calidad de las distribuciones predictivas.

La estructura completa genera:
\begin{equation}
\begin{split}
N_{\text{config}} = & \ 3 \text{ tipos} \times 7 \text{ configs} \times 5 \text{ tamaños} \\
& \times 5 \text{ distribuciones} \times 4 \text{ varianzas} \\
= & \ 2{,}100 \text{ configuraciones}
\end{split}
\end{equation}

Con 12 pasos de evaluación por configuración:
\begin{equation}
N_{\text{filas}} = 2{,}100 \times 12 = 25{,}200 \text{ evaluaciones}
\end{equation}

% ----------------------------------------------------------------------------


\subsection{Simulación 4: Proporciones de Calibración con Tamaño Fijo}
\label{subsec:sim4_proporciones}

\subsubsection{Motivación}

En escenarios de datos limitados, existe un conflicto fundamental entre usar datos para mejorar el ajuste del modelo (\textit{training}) o para mejorar la precisión de los intervalos conformales (\textit{calibration}). Esta simulación busca determinar si existe una ``proporción óptima'' que minimice el ECRPS promedio cuando el presupuesto total de datos es fijo.

\subsubsection{Diseño Experimental}

Se fija un presupuesto total de $n_{\text{total}} = 240$ observaciones y se evalúan 5 proporciones de calibración diferentes:

\begin{table}[htbp]
\centering
\caption{Proporciones de calibración evaluadas (N=240 fijo)}
\label{tab:proporciones_calib}
\begin{tabular}{lccc}
\toprule
\textbf{Proporción} & $n_{\text{train}}$ & $n_{\text{calib}}$ & \textbf{Ratio} \\
\midrule
10\% & 216 & 24 & 9:1 \\
20\% & 192 & 48 & 4:1 \\
30\% & 168 & 72 & 7:3 \\
40\% & 144 & 96 & 3:2 \\
50\% & 120 & 120 & 1:1 \\
\bottomrule
\end{tabular}
\end{table}

La estructura completa es:
\begin{equation}
\begin{split}
N_{\text{config}} = & \ 3 \text{ tipos} \times 7 \text{ configs} \times 5 \text{ proporciones} \\
& \times 5 \text{ distribuciones} \times 4 \text{ varianzas} \\
= & \ 2{,}100 \text{ configuraciones}
\end{split}
\end{equation}

Con 12 pasos de evaluación por configuración:
\begin{equation}
N_{\text{filas}} = 2{,}100 \times 12 = 25{,}200 \text{ evaluaciones}
\end{equation}

% ----------------------------------------------------------------------------


\subsection{Simulación 5: Predicción Multi-paso (Horizonte $h$)}
\label{subsec:sim5_multi_paso}

\subsubsection{Motivación}

El diseño principal evalúa la predicción a un paso adelante mediante un esquema de ventana rodante, donde el modelo se actualiza con cada nueva observación antes de realizar la siguiente predicción. Sin embargo, muchas aplicaciones prácticas requieren pronósticos para múltiples períodos futuros sin acceso a observaciones intermedias \parencite{Hyndman2021FPP3}. Esta simulación evalúa cómo se degrada la calidad de la distribución predictiva cuando el modelo debe proyectar $h$ pasos hacia adelante de forma recursiva, alimentándose exclusivamente de sus propias predicciones anteriores.

\subsubsection{Fundamento Metodológico: Predicción Recursiva}

Existen dos estrategias principales para pronóstico multi-paso \parencite{Taieb2012}:

\begin{enumerate}
\item \textbf{Estrategia Directa:} Entrenar modelos separados para cada horizonte $h$, cada uno estimando directamente $Y_{t+h}$ desde $Y_{1:t}$. Aunque conceptualmente simple, requiere entrenar $H$ modelos independientes y no aprovecha la estructura secuencial del problema.

\item \textbf{Estrategia Recursiva (Iterativa):} Utilizar un único modelo de predicción a un paso y aplicarlo recursivamente:
\begin{equation}
\hat{Y}_{t+h} = f(\hat{Y}_{t+h-1}, \hat{Y}_{t+h-2}, \ldots, Y_t), \quad h = 2, 3, \ldots, H
\end{equation}
Esta estrategia, aunque propaga errores de predicción, es más eficiente computacionalmente y refleja mejor la práctica operativa donde no hay acceso a observaciones futuras verdaderas \parencite{Bontempi2013}.
\end{enumerate}

La presente simulación implementa la estrategia recursiva, que es la más relevante para evaluar métodos conformales en horizontes extendidos. La propagación de incertidumbre en este contexto ha sido estudiada por \textcite{Gneiting2014} y \textcite{Stankeviciute2021}, quienes demuestran que la distribución predictiva en el horizonte $h$ debe considerar tanto la incertidumbre del modelo como la acumulación de errores de pasos previos.

\subsubsection{Generación de Trayectorias Estocásticas}

Para cada configuración y modelo, se generan $N_{\text{traj}} = 100$ trayectorias completas desde el mismo punto de origen $t$. Cada trayectoria $m$ se construye mediante muestreo recursivo de las distribuciones predictivas:

\begin{equation}
\begin{aligned}
\hat{Y}_{t+1}^{(m)} & \sim \hat{F}_{\text{modelo}}(\cdot \mid Y_{1:t}) \\
\hat{Y}_{t+2}^{(m)} & \sim \hat{F}_{\text{modelo}}(\cdot \mid Y_{1:t}, \hat{Y}_{t+1}^{(m)}) \\
& \vdots \\
\hat{Y}_{t+h}^{(m)} & \sim \hat{F}_{\text{modelo}}\left(\cdot \mid Y_{1:t}, \hat{Y}_{t+1}^{(m)}, \ldots, \hat{Y}_{t+h-1}^{(m)}\right)
\end{aligned}
\end{equation}

donde $m = 1, \ldots, 100$ indexa las trayectorias independientes. Este procedimiento genera una distribución empírica para cada horizonte $h$, formada por las 100 realizaciones $\{\hat{Y}_{t+h}^{(1)}, \ldots, \hat{Y}_{t+h}^{(100)}\}$.

\subsubsection{Diseño Experimental}

Se evalúan únicamente 4 modelos representativos (LSPM, DeepAR, Sieve Bootstrap, MCPS) debido al alto costo computacional de generar 100 trayectorias completas por escenario. La selección incluye:

\begin{itemize}
\item \textbf{LSPM:} Método conformal clásico basado en cuantiles empíricos
\item \textbf{DeepAR:} Método paramétrico de aprendizaje profundo que modela distribuciones completas
\item \textbf{Sieve Bootstrap:} Método no paramétrico basado en remuestreo de residuos
\item \textbf{MCPS:} Método conformal contemporáneo que particiona el espacio de calibración
\end{itemize}

Para cada tipo de proceso (ARMA, ARIMA, SETAR), la estructura es:
\begin{equation}
N_{\text{config}} = 7 \text{ configs} \times 5 \text{ distribuciones} \times 4 \text{ varianzas} = 140 \text{ configuraciones}
\end{equation}

Con 3 tipos de procesos y 12 horizontes de predicción:
\begin{equation}
N_{\text{filas}} = 3 \times 140 \times 12 = 5{,}040 \text{ evaluaciones}
\end{equation}

\subsubsection{Protocolo de Evaluación}

El protocolo sigue la estructura del diseño principal con las siguientes particularidades:

\begin{enumerate}
\item \textbf{Punto de origen fijo:} A diferencia de la ventana rodante, aquí todas las predicciones parten del mismo punto temporal $t = n_{\text{train}} + n_{\text{calib}}$.

\item \textbf{Sin actualización intermedia:} El modelo se ajusta una sola vez con los datos disponibles hasta $t$ y no se actualiza durante la proyección de los $H=12$ pasos.

\item \textbf{Evaluación por horizonte:} Para cada $h \in \{1, 2, \ldots, 12\}$, se calcula:
\begin{equation}
\text{ECRPS}_h = \text{ECRPS}\left(\{\hat{Y}_{t+h}^{(1)}, \ldots, \hat{Y}_{t+h}^{(100)}\}, \{Y_{t+h}^{(\text{true}, 1)}, \ldots, Y_{t+h}^{(\text{true}, 1000)}\}\right)
\end{equation}

\item \textbf{Análisis de degradación:} El experimento permite cuantificar la tasa de crecimiento de $\text{ECRPS}_h$ conforme $h$ aumenta, caracterizando la velocidad de degradación de la calidad predictiva.
\end{enumerate}

Esta simulación es particularmente relevante para aplicaciones donde las decisiones deben tomarse con base en pronósticos de mediano plazo sin posibilidad de actualización frecuente del modelo, como en planeación energética, gestión de inventarios o política monetaria \parencite{Hyndman2021FPP3}.

% ----------------------------------------------------------------------------


\subsection{Resumen de Evaluaciones Complementarias}
\label{subsec:resumen_complementarias}

La Tabla~\ref{tab:resumen_complementarias} consolida el alcance total de estos experimentos. El volumen combinado de estas simulaciones complementarias es comparable al del diseño principal, reflejando la importancia de estos aspectos metodológicos para una evaluación comprehensiva.

\begin{table}[htbp]
\centering
\caption{Resumen de la carga experimental de simulaciones complementarias}
\label{tab:resumen_complementarias}
\small
\begin{tabular}{lp{6cm}c}
\toprule
\textbf{Simulación} & \textbf{Factor Variado} & \textbf{Filas} \\
\midrule
1. Diferenciación ($d=1$) & 
    Modalidad (SIN\_DIFF vs CON\_DIFF) & 
    3{,}360 \\
\addlinespace
2. Multi-D & 
    Orden de integración $d \in \{1, \ldots, 10\}$ & 
    26{,}880 \\
\addlinespace
3. Tamaño Muestral & 
    Tamaño total con proporción fija (83:17) & 
    25{,}200 \\
\addlinespace
4. Proporciones & 
    Proporción de calibración ($n_{\text{total}} = 240$ fijo) & 
    25{,}200\\
\addlinespace
5. Multi-paso & 
    Horizonte de predicción $h \in \{1, \ldots, 12\}$ & 
    5{,}040 \\
\midrule
\textbf{Total} & & \textbf{85{,}680} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Protocolo Común a Todas las Simulaciones}

Todas las simulaciones complementarias mantienen consistencia metodológica con el diseño principal descrito en la Sección~\ref{sec:diseño_experimental}:

\begin{itemize}
\item \textbf{Período de inicialización (burn-in):} 100 observaciones descartadas antes del inicio efectivo de la serie para eliminar efectos transitorios de las condiciones iniciales.

\item \textbf{Métrica de evaluación:} ECRPS entre la distribución predictiva empírica del modelo (basada en 1,000 muestras bootstrap o generadas por el modelo) y la distribución teórica verdadera del proceso generador de datos (representada por 1,000 muestras de la densidad real), como se define en la Sección~\ref{subsec:ecrps}.

\item \textbf{Modelos evaluados:} Los 9 métodos conformales especificados en la Sección~\ref{sec:modelos_predictivos} (Block Bootstrapping, Sieve Bootstrap, LSPM, LSPMW, AREPD, MCPS, AV-MCPS, DeepAR, EnCQR-LSTM), excepto en la Simulación 5 donde por razones de eficiencia computacional se evalúan únicamente 4 modelos representativos (LSPM, DeepAR, Sieve Bootstrap, MCPS).

\item \textbf{Calibración y ajuste de hiperparámetros:} Cada modelo se optimiza siguiendo un procedimiento de dos etapas. Primero, los hiperparámetros del modelo base se seleccionan mediante validación cruzada temporal en el conjunto de entrenamiento, minimizando el ECRPS en una ventana de validación. Segundo, los parámetros conformales (como el nivel de cobertura o los pesos de calibración) se ajustan usando el conjunto de calibración dedicado. 

\item \textbf{Control de reproducibilidad:} Semillas aleatorias fijas y documentadas para cada escenario, con incrementos determinísticos según el índice del escenario ($\text{seed} = \text{seed}_{\text{base}} + \text{id}_{\text{escenario}}$), permitiendo la replicación exacta de todos los experimentos.

\item \textbf{Esquema de evaluación:} 
\begin{itemize}
    \item \textit{Ventana rodante} (Simulaciones 1--4): El modelo se actualiza con cada nueva observación antes de predecir el siguiente paso, generando 12 predicciones secuenciales donde el horizonte siempre es $h=1$ pero el conjunto de entrenamiento crece.
    \item \textit{Proyección desde origen fijo} (Simulación 5): El modelo se ajusta una sola vez en $t$ y proyecta recursivamente los horizontes $h \in \{1, 2, \ldots, 12\}$ sin actualización intermedia, propagando la incertidumbre a través de predicciones iteradas.
\end{itemize}

\item \textbf{Estructura completa:} Todas las simulaciones evalúan las 21 configuraciones de procesos (7 ARMA + 7 ARIMA + 7 SETAR) cruzadas con 5 distribuciones de ruido (normal, uniforme, exponencial, t-student, mezcla) y 4 niveles de varianza (0.2, 0.5, 1.0, 3.0), garantizando cobertura exhaustiva del espacio paramétrico.
\end{itemize}

La uniformidad en estos aspectos fundamentales garantiza que las diferencias observadas en el desempeño sean atribuibles exclusivamente a los factores experimentales bajo estudio (modalidad de diferenciación, orden de integración, tamaño muestral, proporción de calibración, u horizonte de predicción) y no a variaciones en el protocolo de evaluación.