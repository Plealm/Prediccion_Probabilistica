% !TeX root = ../main.tex
\chapter{Fundamentos de Pronóstico Probabilístico y Sistemas de Predicción Conformal}
\label{cap:marco_teorico}

En este capítulo se presentan los fundamentos teóricos que sustentan el desarrollo de esta investigación. Se inicia con una introducción al pronóstico probabilístico y su importancia en el análisis de series temporales, seguido de una discusión detallada sobre las métricas utilizadas para evaluar el desempeño predictivo. Posteriormente, se desarrollan los conceptos fundamentales de la predicción conformal y sus adaptaciones para series de tiempo.

\section{Pronóstico Probabilístico}
\label{sec:pronostico_probabilistico}

El pronóstico probabilístico representa un cambio de paradigma fundamental en la predicción estadística, pasando de estimaciones puntuales a distribuciones de probabilidad completas sobre cantidades futuras de interés \parencite{Gneiting2014}. A diferencia de las predicciones puntuales tradicionales, que proporcionan únicamente un valor esperado o una estimación central, el pronóstico probabilístico cuantifica la incertidumbre asociada a la predicción mediante la especificación de una distribución predictiva completa \parencite{GneitingBalabdaoui2007}.

\subsection{Definición y Objetivos}

Formalmente, sea $Y_{t+h}$ una variable aleatoria que representa el valor de una serie temporal en el tiempo $t+h$, donde $h>0$ denota el horizonte de predicción. Un pronóstico probabilístico es una distribución de probabilidad $F_{t+h|t}$ que caracteriza la incertidumbre sobre $Y_{t+h}$ dado el conjunto de información disponible hasta el tiempo $t$, denotado por $\mathcal{F}_t$ \parencite{Gneiting2014}.

Gneiting y Raftery explican que el objetivo fundamental del pronóstico probabilístico es maximizar la nitidez de las distribuciones predictivas sujeto a calibración. Estos dos conceptos son fundamentales para entender la calidad de un pronóstico probabilístico \parencite{GneitingRaftery2007, GneitingBalabdaoui2007}:

\begin{itemize}
\item \textbf{Calibración:} Se refiere a la consistencia estadística entre las distribuciones predictivas y las observaciones. Una predicción está calibrada si las realizaciones son estadísticamente indistinguibles de muestras aleatorias de las distribuciones predictivas \parencite{Thorarinsdottir2017}.

\item \textbf{Nitidez:} Se refiere a la concentración de las distribuciones predictivas y es una propiedad exclusiva de los pronósticos. Cuanto más concentradas sean las distribuciones predictivas, mejor, siempre que se mantenga la calibración \parencite{GneitingRaftery2007}.
\end{itemize}

La Figura~\ref{fig:tipos_prediccion} ilustra la diferencia entre una predicción puntual, un intervalo de predicción y una distribución predictiva completa.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{./Imagenes/Cap2_prediccion.png}
\caption{Tipos de predicciones y su relación con la incertidumbre.}
\label{fig:tipos_prediccion}
\end{figure}

\subsection{Ventajas del Pronóstico Probabilístico}

El pronóstico probabilístico ofrece múltiples ventajas sobre las predicciones puntuales tradicionales que justifican su adopción creciente en diversas aplicaciones \parencite{Gneiting2014}:

\begin{enumerate}
\item \textbf{Cuantificación Completa de Incertidumbre:} A diferencia de las predicciones puntuales, que proporcionan únicamente un valor esperado, el pronóstico probabilístico caracteriza la incertidumbre de manera exhaustiva mediante distribuciones de probabilidad. Esto permite a los tomadores de decisiones evaluar tanto la magnitud esperada de un evento como la variabilidad asociada, facilitando una comprensión más profunda de los riesgos y oportunidades \parencite{GneitingRaftery2007}.

\item \textbf{Soporte para Decisiones Óptimas:} En contextos donde las decisiones deben tomarse bajo incertidumbre, como la gestión de inventarios, la planificación de recursos energéticos, o la asignación de capital, las distribuciones predictivas completas son esenciales. Permiten la optimización de funciones de utilidad esperada y la implementación de estrategias que consideren explícitamente el trade-off entre riesgo y recompensa \parencite{Gneiting2014}.

\item \textbf{Evaluación de Eventos Extremos:} Las predicciones puntuales son inherentemente limitadas para caracterizar eventos raros o de cola. El pronóstico probabilístico, en cambio, permite estimar probabilidades de ocurrencia de eventos extremos, información crucial para la gestión de riesgos financieros y la planificación de infraestructura \parencite{Thorarinsdottir2017}.

\item \textbf{Flexibilidad en la Comunicación de Incertidumbre:} Las distribuciones predictivas permiten múltiples formas de comunicación adaptadas a diferentes audiencias: intervalos de predicción, probabilidades de excedencia de umbrales críticos o visualizaciones completas mediante fan charts \parencite{Gneiting2014}.

\end{enumerate}

Estas ventajas han motivado la transición hacia pronósticos probabilísticos en campos tan diversos como meteorología, finanzas, energía, epidemiología y gestión de cadenas de suministro \parencite{Gneiting2014, Salinas2020}.

\section{Métricas para Evaluación de Pronósticos Probabilísticos}
\label{sec:metricas_evaluacion}

La evaluación rigurosa del desempeño predictivo es fundamental para comparar metodologías de pronóstico y guiar mejoras en los modelos. En el contexto de pronósticos probabilísticos, las métricas de evaluación deben considerar tanto la calibración como la nitidez de las distribuciones predictivas \parencite{GneitingBalabdaoui2007, Thorarinsdottir2017}.

\subsection{Reglas de Puntuación Propias}
\label{subsec:scoring_rules}

Una \textit{regla de puntuación} (scoring rule) es una función $S: \mathcal{F} \times \mathbb{R} \to \mathbb{R} \cup \{\infty\}$ que asigna una penalización numérica $S(F,y)$ a cada par formado por una distribución predictiva $F$ y una observación realizada $y$ \parencite{GneitingRaftery2007}. En nuestra notación, valores más bajos de la puntuación indican mejor desempeño predictivo.

\subsubsection{Propriety y Strict Propriety}

La \textit{propriety} es una característica fundamental que debe satisfacer toda métrica de evaluación de pronósticos probabilísticos para garantizar que incentive predicciones honestas y bien calibradas \parencite{GneitingRaftery2007}.

\textbf{Definición (Regla de Puntuación Proper):} Una regla de puntuación $S$ es \textit{proper} relativa a una clase $\mathcal{F}$ de distribuciones de probabilidad si
\begin{equation}
\mathbb{E}_G[S(G, Y)] \leq \mathbb{E}_G[S(F, Y)]
\label{eq:proper_scoring}
\end{equation}
para todas las distribuciones $F, G \in \mathcal{F}$, donde $Y \sim G$ \parencite{GneitingRaftery2007, Thorarinsdottir2017}. 

\textbf{Definición (Regla de Puntuación Strictly Proper):} La regla de puntuación $S$ es \textit{strictly proper} si la desigualdad en \eqref{eq:proper_scoring} se cumple con igualdad únicamente cuando $F = G$ \parencite{GneitingRaftery2007}.

La importancia de la \textit{propriety} radica en que establece un principio de alineación de incentivos: si un pronosticador desea minimizar su puntuación esperada, su mejor estrategia es reportar sinceramente su verdadera distribución predictiva \parencite{GneitingRaftery2007}. 

\subsection{Continuous Ranked Probability Score (CRPS)}
\label{subsec:crps}

El \textit{Continuous Ranked Probability Score} (CRPS) es una de las reglas de puntuación estrictamente propias más utilizadas para evaluar pronósticos probabilísticos de variables continuas \parencite{Gneiting2014}. Su popularidad se debe a su sólida fundamentación teórica y su capacidad para evaluar simultáneamente calibración y nitidez \parencite{GneitingRaftery2007, Thorarinsdottir2017}.

\subsubsection{Definiciones y Representaciones}

El CRPS admite varias representaciones matemáticas equivalentes, cada una con sus propias ventajas conceptuales y computacionales.

\textbf{Representación integral:} La definición original del CRPS está dada por \parencite{GneitingRaftery2007}:
\begin{equation}
\text{CRPS}(F, y) = \int_{-\infty}^{\infty} \left(F(x) - \mathbbm{1}\{y \leq x\}\right)^2 dx
\label{eq:crps_integral}
\end{equation}
donde $F$ es la función de distribución acumulada (FDA) de la distribución predictiva y $y$ es la observación realizada. Esta representación muestra que el CRPS mide el área entre la FDA predictiva y la FDA de la observación \parencite{Thorarinsdottir2017}.

\textbf{Representación basada en esperanzas:} Una forma alternativa, más conveniente para cálculos, está dada por \parencite{GneitingRaftery2007}:
\begin{equation}
\text{CRPS}(F, y) = \mathbb{E}_F|X - y| - \frac{1}{2}\mathbb{E}_F|X - X'|
\label{eq:crps_expectation}
\end{equation}
donde $X$ y $X'$ son variables aleatorias independientes con distribución $F$. Esta representación revela una interpretación intuitiva del CRPS: el primer término mide la distancia esperada entre la predicción y la observación, mientras que el segundo término penaliza la dispersión de la distribución predictiva.

\subsubsection{Propiedades del CRPS}

El CRPS posee varias propiedades deseables que explican su amplia adopción en la literatura \parencite{GneitingRaftery2007, Thorarinsdottir2017}:

\begin{enumerate}
\item \textbf{Strictly proper:} El CRPS es \textit{strictly proper} relativo a la clase de todas las distribuciones de probabilidad en $\mathbb{R}$ con primer momento finito \parencite{GneitingRaftery2007}.

\item \textbf{Unidades consistentes:} El CRPS se expresa en las mismas unidades que la variable pronosticada \parencite{Gneiting2014}.

\item \textbf{Reducción al error absoluto:} Cuando $F$ es una distribución degenerada (predicción puntual), el CRPS se reduce al error absoluto $|x-y|$, permitiendo un marco de evaluación unificado \parencite{GneitingRaftery2007}.

\item \textbf{Sensibilidad dual:} El CRPS evalúa simultáneamente la calibración y la nitidez \parencite{Thorarinsdottir2017}.
\end{enumerate}

\subsection{Expected Continuous Ranked Probability Score (ECRPS)}
\label{subsec:ecrps}

El desempeño predictivo global de una secuencia de $n$ pares pronóstico-observación se cuantifica mediante el \textit{Expected Continuous Ranked Probability Score} (ECRPS), definido como la media aritmética de los CRPS individuales:

\begin{equation}
\text{ECRPS} = \frac{1}{n}\sum_{i=1}^n \text{CRPS}(F_i, y_i)
\label{eq:ecrps}
\end{equation}

El ECRPS hereda todas las propiedades deseables del CRPS y proporciona un resumen numérico único del desempeño predictivo sobre todo el conjunto de evaluación. En particular, la comparación mediante ECRPS permite establecer comparaciones robustas entre diferentes métodos de pronóstico probabilístico.

\section{Test de Diebold-Mariano para Comparación de Precisión Predictiva}
\label{sec:test_diebold_mariano}

La evaluación comparativa de distintas metodologías de pronóstico requiere herramientas estadísticas rigurosas que permitan determinar si las diferencias observadas en el desempeño predictivo son estadísticamente significativas o simplemente producto del azar. El test de Diebold-Mariano \parencite{DieboldMariano1995} constituye uno de los procedimientos más ampliamente utilizados para este propósito, ofreciendo un marco general y flexible para contrastar la hipótesis nula de igual precisión predictiva entre dos métodos de pronóstico competidores.

\subsection{Formulación del Test}

Sean $\hat{y}_{t+h}^{(1)}$ y $\hat{y}_{t+h}^{(2)}$ dos pronósticos $h$ pasos adelante para una variable $y_{t+h}$, producidos por dos metodologías diferentes. Los errores de pronóstico correspondientes son:
\begin{equation}
e_{t+h}^{(i)} = y_{t+h} - \hat{y}_{t+h}^{(i)}, \quad i = 1, 2
\end{equation}

El test de Diebold-Mariano se basa en una función de pérdida $L(\cdot)$ que cuantifica el costo asociado con cada error de pronóstico. Para un horizonte temporal de evaluación que abarca $n$ observaciones, se define el diferencial de pérdida en el tiempo $t$ como:
\begin{equation}
d_t = L(e_t^{(1)}) - L(e_t^{(2)}), \quad t = 1, \ldots, n
\label{eq:loss_differential}
\end{equation}

Tradicionalmente, el test de Diebold-Mariano se ha aplicado utilizando la pérdida cuadrática para evaluar estimaciones puntuales. Sin embargo, este marco es suficientemente general para acomodar cualquier función de pérdida \parencite{DieboldMariano1995}. En el presente trabajo, se utilizará principalmente el CRPS o el ECRPS, según corresponda como métrica de pérdida fundamental. Esto permite extender la comparación de Diebold-Mariano.

La hipótesis nula de igual precisión predictiva se formula como:
\begin{equation}
H_0: \mathbb{E}[d_t] = 0
\label{eq:dm_null}
\end{equation}

Esta hipótesis establece que la pérdida esperada es idéntica para ambos métodos de pronóstico. El estadístico de prueba se construye a partir de la media muestral del diferencial de pérdida:
\begin{equation}
\bar{d} = \frac{1}{n}\sum_{t=1}^n d_t
\end{equation}

\subsection{Distribución Asintótica y Estimación de la Varianza}

Bajo condiciones de regularidad que incluyen la estacionariedad débil y la existencia de momentos de orden finito, Diebold y Mariano demuestran que:
\begin{equation}
\sqrt{n}\,\bar{d} \xrightarrow{d} N(0, 2\pi f_d(0))
\end{equation}
donde $f_d(0)$ denota la densidad espectral de la serie $d_t$ evaluada en frecuencia cero, la cual equivale a la varianza de largo plazo:
\begin{equation}
\sigma^2 = \text{Var}(\sqrt{n}\,\bar{d}) = \gamma_0 + 2\sum_{k=1}^\infty \gamma_k
\label{eq:long_run_variance}
\end{equation}
siendo $\gamma_k = \text{Cov}(d_t, d_{t-k})$ la autocovarianza de orden $k$.

Un aspecto fundamental del test de Diebold-Mariano es que permite explícitamente la presencia de autocorrelación en el diferencial de pérdida $d_t$. Esta característica es especialmente relevante en el contexto de pronósticos a múltiples pasos adelante ($h > 1$), donde los errores de pronóstico exhiben típicamente estructura de autocorrelación hasta el orden $(h-1)$ \parencite{DieboldMariano1995}. Esta estructura surge porque pronósticos óptimos $h$ pasos adelante generan errores que siguen un proceso de media móvil MA$(h-1)$.

En la práctica, la varianza de largo plazo $\sigma^2$ debe ser estimada. Diebold y Mariano proponen utilizar un estimador basado en autocovarianzas ponderadas por kernel:
\begin{equation}
\hat{\sigma}^2 = \hat{\gamma}_0 + 2\sum_{k=1}^{M} k\left(\frac{k}{M}\right)\hat{\gamma}_k
\label{eq:kernel_variance_estimator}
\end{equation}
donde $\hat{\gamma}_k = n^{-1}\sum_{t=k+1}^n (d_t - \bar{d})(d_{t-k} - \bar{d})$ son las autocovarianzas muestrales, $k(\cdot)$ es una función kernel (por ejemplo, kernel de Bartlett o Parzen), y $M$ es el parámetro de ancho de banda o truncamiento que controla el número de autocovarianzas incluidas en la estimación.

Para el caso específico donde se conoce que el diferencial de pérdida sigue un proceso MA$(h-1)$, Diebold y Mariano sugieren simplificar el estimador utilizando $M = h-1$ con kernel rectangular:
\begin{equation}
\hat{\sigma}^2_{DM} = \hat{\gamma}_0 + 2\sum_{k=1}^{h-1} \hat{\gamma}_k
\label{eq:dm_variance_simple}
\end{equation}

El estadístico de prueba resultante es:
\begin{equation}
DM = \frac{\sqrt{n}\,\bar{d}}{\hat{\sigma}}
\label{eq:dm_statistic}
\end{equation}

Bajo la hipótesis nula, este estadístico converge en distribución a una normal estándar: $DM \xrightarrow{d} N(0,1)$. Para un test bilateral al nivel de significancia $\alpha$, se rechaza $H_0$ si $|DM| > z_{\alpha/2}$, donde $z_{\alpha/2}$ denota el cuantil $(1-\alpha/2)$ de la distribución normal estándar.

\subsection{Modificaciones para Muestras Pequeñas}

A pesar de la solidez teórica del test de Diebold-Mariano bajo asintótica estándar, diversos estudios han documentado distorsiones de tamaño en muestras finitas, particularmente cuando el número de observaciones de pronóstico es limitado. Harvey et al. \parencite{Harvey1997} demostraron mediante simulaciones Monte Carlo que el test original tiende a sobrerrechazar la hipótesis nula (es decir, presenta un tamaño empírico superior al nominal), especialmente para horizontes de pronóstico largos y muestras pequeñas.

Para abordar estas limitaciones, Harvey et al. proponen una corrección del estadístico que mejora sustancialmente el desempeño en muestras finitas. La modificación se fundamenta en el uso de un estimador aproximadamente insesgado de la varianza de $\bar{d}$. Partiendo de la expresión exacta:
\begin{equation}
\text{Var}(\bar{d}) = n^{-1}\left[\gamma_0 + 2n^{-1}\sum_{k=1}^{h-1}(n-k)\gamma_k\right]
\end{equation}
y calculando el valor esperado del estimador empleado en \eqref{eq:dm_variance_simple}, se obtiene que:
\begin{equation}
\mathbb{E}[\hat{\sigma}^2_{DM}] \approx \left[\frac{n+1-2h+n^{-1}h(h-1)}{n}\right]\text{Var}(\bar{d})
\end{equation}

Esta relación sugiere el estadístico modificado:
\begin{equation}
DM^* = \left[\frac{n+1-2h+n^{-1}h(h-1)}{n}\right]^{1/2} DM
\label{eq:dm_modified}
\end{equation}

Adicionalmente, Harvey et al. recomiendan comparar $DM^*$ con valores críticos de la distribución $t$ de Student con $(n-1)$ grados de libertad, en lugar de la distribución normal estándar. Esta segunda modificación reconoce implícitamente la incertidumbre adicional asociada con la estimación de la varianza en muestras finitas.

Los resultados de simulación reportados por Harvey et al. \parencite{Harvey1997} indican que el test modificado presenta un tamaño empírico considerablemente más cercano al nominal, especialmente para $n \leq 50$ y horizontes de pronóstico $h \geq 2$. Aunque el test modificado exhibe una ligera pérdida de potencia en comparación con el test original cuando ambos están correctamente calibrados, esta reducción es marginal y ampliamente compensada por la ganancia en confiabilidad inferencial.

\subsection{Enfoque de Asintótica de Suavizado Fijo}

Una alternativa más reciente para abordar las distorsiones de tamaño del test de Diebold-Mariano en muestras pequeñas es el enfoque de \textit{asintótica de suavizado fijo} (fixed-smoothing asymptotics), desarrollado por Coroneo e Iacone \parencite{CoroneoIacone2020}. Este marco teórico reconoce que en aplicaciones prácticas de evaluación de pronósticos, el tamaño muestral $n$ es frecuentemente limitado, haciendo que la aproximación asintótica estándar (que requiere $M/n \to 0$) sea inadecuada.

La idea fundamental es mantener constante la razón entre el parámetro de ancho de banda y el tamaño muestral conforme $n$ aumenta. Formalmente, bajo la \textit{asintótica fixed-b}, se asume que $M/n \to b$ para algún $b \in (0,1]$ fijo. Bajo este régimen asintótico alternativo, el estimador de varianza \eqref{eq:kernel_variance_estimator} ya no es consistente para $\sigma^2$. Sin embargo, Kiefer y Vogelsang (2005) demostraron que el estadístico resultante converge a una distribución no estándar que depende de $b$ y del kernel empleado.

Para el kernel de Bartlett, la distribución límite puede caracterizarse explícitamente, y sus cuantiles pueden aproximarse mediante fórmulas polinomiales. Específicamente, para un test bilateral al 5\% de significancia, el valor crítico $c_\alpha(b)$ satisface:
\begin{equation}
c_\alpha(b) \approx \alpha_0 + \alpha_1 b + \alpha_2 b^2 + \alpha_3 b^3
\end{equation}
donde los coeficientes $\{\alpha_i\}$ han sido tabulados por Kiefer y Vogelsang.

Coroneo e Iacone \parencite{CoroneoIacone2020} extienden este marco al contexto específico de evaluación de pronósticos, demostrando mediante simulaciones Monte Carlo que los tests basados en asintótica de suavizado fijo exhiben un tamaño empírico notablemente más preciso que el test de Diebold-Mariano estándar, incluso para muestras tan pequeñas como $n=40$. Los autores proponen utilizar anchos de banda $M = \lfloor n^{1/2} \rfloor$ para el estimador con kernel de Bartlett, encontrando que esta elección ofrece un equilibrio favorable entre tamaño y potencia del test.

Una segunda variante dentro del paradigma de suavizado fijo es la \textit{asintótica fixed-m}, que emplea un estimador de varianza basado en el periodograma suavizado con kernel de Daniell:
\begin{equation}
\hat{\sigma}^2_{DAN} = \frac{2\pi}{m}\sum_{j=1}^m I(\lambda_j)
\end{equation}
donde $I(\lambda_j)$ denota el periodograma de $d_t$ evaluado en la frecuencia de Fourier $\lambda_j = 2\pi j/n$, y $m$ es un parámetro de truncamiento mantenido fijo conforme $n \to \infty$. Bajo condiciones de regularidad, el estadístico resultante converge a una distribución $t$ con $2m$ grados de libertad \parencite{CoroneoIacone2020}.

\subsection{Consideraciones sobre la Generación de Pronósticos}

Un aspecto metodológico crucial del test de Diebold-Mariano es su tratamiento de los pronósticos como objetos dados o primitivos, sin considerar explícitamente el proceso de estimación de los modelos subyacentes. Esta perspectiva contrasta con marcos alternativos, como el de West (1996) y Clark y McCracken (2001), que desarrollan teoría asintótica específica para pronósticos derivados de modelos paramétricos estimados.

Cuando los pronósticos provienen de modelos con parámetros estimados, la incertidumbre asociada con la estimación puede afectar la distribución del estadístico de prueba. West \parencite{West1996} demostró que, bajo ciertas condiciones, esta incertidumbre de estimación es asintóticamente despreciable si el tamaño de la muestra de entrenamiento $R$ crece mucho más rápido que el tamaño de la muestra de evaluación $P$ (específicamente, si $P/R \to 0$).

Alternativamente, Giacomini y White \parencite{GiacominiWhite2006} proponen un marco donde la incertidumbre de estimación no desaparece asintóticamente (fijando $R < \infty$). Bajo este régimen, el test de Diebold-Mariano permanece válido, pero ahora evalúa el desempeño relativo de \textit{métodos de pronóstico} (incluyendo el procedimiento de estimación) en lugar de \textit{modelos de pronóstico} poblacionales. Este marco es particularmente apropiado cuando el objetivo es comparar estrategias de pronóstico que podrían implementarse en práctica, reconociendo que los modelos deben ser reestimados periódicamente con ventanas de datos finitas.

Para los propósitos de esta investigación, adoptamos la perspectiva de Giacomini y White, interpretando el test de Diebold-Mariano como una herramienta para evaluar el desempeño predictivo de métodos completos de pronóstico, incluyendo tanto la especificación del modelo como el procedimiento de estimación y actualización de parámetros.

\section{Predicción Conformal por Intervalos: El Enfoque IIE}
\label{sec:prediccion_conformal_intervalos_IIE}

La predicción conformal clásica, introducida por Vovk et al. \cite{Vovk2005}, se fundamenta en la capacidad de generar conjuntos de predicción $\Gamma^\epsilon$ que garantizan una cobertura de confianza exacta para cualquier nivel de significancia $\epsilon \in (0,1)$. A diferencia de los métodos estadísticos tradicionales que dependen de la asintótica (grandes muestras), la predicción conformal es válida para muestras finitas, siempre que se cumpla el supuesto de intercambiabilidad de los datos.

\subsection{El Concepto de No-Conformidad}

El núcleo de esta metodología es la \textit{medida de no-conformidad} (NCM, por sus siglas en inglés). Una NCM es una función $A(B, z)$ que cuantifica el grado de ``extrañeza" de un ejemplo $z$ en relación con un multiconjunto (o \textit{bag}) de ejemplos $B$. En el contexto de regresión, donde $z = (x, y)$, la medida de no-conformidad más común es el error absoluto de predicción, definido como:
\begin{equation}
    \alpha_i = |y_i - \hat{y}_i|
\end{equation}
donde $\hat{y}_i$ es la estimación producida por un algoritmo de aprendizaje subyacente (denominado \textit{underlying algorithm}). Es importante subrayar que la predicción conformal es agnóstica al modelo: puede envolver desde una regresión lineal simple hasta redes neuronales profundas, transformando sus predicciones puntuales en intervalos con validez estadística.

\subsection{Protocolo de Construcción de Intervalos}

Para construir un intervalo de predicción para un nuevo objeto $x_n$ basado en un conjunto de entrenamiento $z_1, \dots, z_{n-1}$, el método IIE (Inducida por Errores) sigue un proceso de prueba de hipótesis inversa. Para cada valor potencial $y \in \mathbb{R}$:

\begin{enumerate}
    \item \textbf{Aumentación del Conjunto:} Se asume hipotéticamente que la verdadera etiqueta de $x_n$ es $y$, formando el conjunto aumentado $z_1, z_2, \dots, z_{n-1}, z_n$, donde $z_n = (x_n, y)$.
    \item \textbf{Cálculo de Puntajes:} Se calculan los puntajes de no-conformidad $\alpha_1, \dots, \alpha_n$ para todos los elementos, incluyendo el ejemplo hipotético. 
    \item \textbf{Derivación del p-valor:} Se calcula la proporción de ejemplos que son ``al menos tan extraños'' como el nuevo ejemplo $z_n$:
    \begin{equation}
        p(y) = \frac{|\{i = 1, \dots, n : \alpha_i \geq \alpha_n\}|}{n}
        \label{eq:p_valor_clasico}
    \end{equation}
    \item \textbf{Inversión de la Región de Aceptación:} El intervalo de predicción $\Gamma^{1-\epsilon}$ se define como el conjunto de todos los valores $y$ que no pueden ser rechazados al nivel de significancia $\epsilon$:
    \begin{equation}
        \Gamma^\epsilon(x_1, y_1, \dots, x_{n-1}, y_{n-1}, x_n) = \{y \in \mathbb{R} : p(y) > \epsilon\}
    \end{equation}
\end{enumerate}

Este procedimiento garantiza que $P(y_n \notin \Gamma^\epsilon) \leq \epsilon$. Si los puntajes $\alpha_i$ tienen una distribución continua (sin empates), la probabilidad de error es exactamente $\epsilon$ \cite{Vovk2005}.

\section{Robustez ante la No-Intercambiabilidad: Aproximación de Barber}
\label{sec:barber_robustness_detail}

Uno de los desafíos críticos en el análisis de series temporales es que el supuesto de intercambiabilidad rara vez se sostiene. Fenómenos como la autocorrelación, la heterocedasticidad y la deriva de parámetros (drift) invalidan la asunción de que el pasado y el futuro son estadísticamente idénticos. Barber et al. \cite{Barber2023} proponen una extensión fundamental para estos escenarios.

\subsection{El Gap de Cobertura y Variación Total}

Barber et al. formalizan la degradación de la validez conformal mediante el uso de la \textit{Distancia de Variación Total} ($d_{TV}$). Si la distribución de los datos cambia en el tiempo, existe una brecha de cobertura (\textit{coverage gap}). El teorema principal de Barber establece que la pérdida de cobertura está acotada por la suma de las distancias entre la distribución de los datos de entrenamiento y la distribución del dato de prueba:
\begin{equation}
    \text{Error de Cobertura} \leq \epsilon + \sum_{i=1}^{n} w_i d_{TV}(Z_i, Z_{n+1})
\end{equation}

\subsection{Cuantiles Pesados y Decaimiento Temporal}

Para contrarrestar este efecto en series de tiempo, Barber et al. introducen los \textit{Weighted Conformal Predictors}. En lugar de asignar un peso uniforme de $1/n$ a cada residuo histórico, se asignan pesos $w_i$ que reflejan la relevancia del dato. En series no estacionarias, los datos más recientes son mejores predictores del futuro.

Se define comúnmente un decaimiento geométrico para los pesos:
\begin{equation}
    w_i = \rho^{n-i}, \quad \rho \in (0, 1)
\end{equation}
donde un $\rho$ cercano a 1 asume una estabilidad lenta, mientras que un $\rho$ menor reacciona rápidamente a cambios estructurales. El p-valor pesado se calcula como una suma ponderada de funciones indicadoras:
\begin{equation}
    p^y = \frac{\sum_{i=1}^{n-1} w_i \mathbbm{1}_{\alpha_i \geq \alpha_n} + w_n}{\sum_{j=1}^n w_j}
\end{equation}
Este enfoque permite que la predicción conformal sea ``adaptativa", manteniendo la cobertura cercana al nivel nominal incluso cuando la serie temporal experimenta cambios súbitos en su media o varianza \cite{Barber2023}.

\section{Sistemas de Predicción Conformal (CPS): De Intervalos a Densidades}
\label{sec:cps_densidades_LSPM}

El Capítulo 7 de la obra de Vovk \cite{Vovk2005} marca la transición de la predicción de conjuntos a la predicción de distribuciones completas. Un \textit{Sistema de Predicción Conformal} (CPS) no entrega un rango, sino una \textit{Distribución Predictiva Aleatorizada} (RPD), denotada como $\Pi_n(y, \tau)$, que representa la probabilidad de que la verdadera etiqueta sea menor o igual a $y$.

\subsection{Formalización de la RPD y el Suavizado ($\tau$)}

Para asegurar que la distribución resultante sea continua y cumpla con las propiedades de una FDA (Función de Distribución Acumulada), se introduce una variable de suavizado $\tau \sim U(0,1)$. La función $\Pi$ se define como:
\begin{equation}
\Pi_n(y, \tau) := \frac{|\{i : \alpha_i < \alpha_n^y\}| + \tau |\{i : \alpha_i = \alpha_n^y\}|}{n}
\end{equation}
Es vital notar que aquí $\alpha_i$ son puntajes de \textit{conformidad} (no de no-conformidad). Un ejemplo común en regresión es $\alpha_i = y_i - \hat{y}_i$. El uso de la variable $\tau$ garantiza la \textit{calibración fuerte en probabilidad}: los valores de la RPD evaluados en la verdadera etiqueta son independientes y uniformes en $[0,1]$, permitiendo una cuantificación exacta de la incertidumbre en cualquier punto de la distribución \cite{Vovk2017wp}.

\subsection{La Máquina de Predicción de Mínimos Cuadrados (LSPM)}

La \textit{Least Squares Prediction Machine} (LSPM) es la aplicación primordial de los CPS al ámbito de la regresión. La LSPM utiliza la estructura de la regresión lineal para optimizar la eficiencia de la distribución predictiva.

\subsubsection{Variantes de la LSPM}
Vovk distingue tres formas de calcular los residuos dentro de una LSPM:
\begin{enumerate}
    \item \textbf{LSPM Ordinaria:} Los puntajes son simplemente los residuos de entrenamiento. Sin embargo, este enfoque tiende a ser demasiado optimista (sobreajuste), ya que el modelo ya ha ``visto" los datos de entrenamiento.
    \item \textbf{LSPM Eliminada (Deleted):} Utiliza un esquema de validación cruzada interna (\textit{leave-one-out}). Para cada dato $i$, se entrena un modelo omitiendo ese dato específico, asegurando que el residuo sea una medida honesta de la capacidad de generalización.
    \item \textbf{LSPM Estudiantizada:} Es la variante más robusta y matemáticamente rigurosa. Ajusta cada residuo por su apalancamiento (\textit{leverage}), $h_i$, proveniente de la diagonal de la matriz \textit{hat}:
    \begin{equation}
        \alpha_i := \frac{y_i - \hat{y}_i}{\sigma \sqrt{1 - h_i}}
    \end{equation}
\end{enumerate}


\section{Sistemas de Predicción Conformal de Mondrian (MCPS)}
\label{sec:mcps_profundo}

A pesar de las sólidas garantías de validez marginal que ofrecen los Sistemas de Predicción Conformal (CPS) descritos en la sección~\ref{sec:cps_densidades_LSPM}, estos presentan una limitación teórica y práctica fundamental: la garantía de error es un promedio sobre todo el espacio de datos. Esto implica que el sistema puede ser extremadamente preciso en ciertas regiones del espacio de características y, simultáneamente, cometer errores sistemáticos en otras, siempre que el error global no supere el nivel $\epsilon$. Los \textit{Sistemas de Predicción Conformal de Mondrian} (MCPS, por sus siglas en inglés) introducen el concepto de \textit{validez condicional por categorías}, permitiendo que la calibración se mantenga exacta dentro de subconjuntos específicos de los datos \cite{Vovk2022}.

\subsection{Origen y Motivación: Validez Marginal vs. Condicional}

El apelativo ``Mondrian'' deriva del estilo geométrico del pintor neerlandés Piet Mondrian, cuya estética se fundamenta en la compartimentación del lienzo en rectángulos de colores puros delimitados por una cuadrícula, tal como se ilustra en la Figura~\ref{fig:analogia_mondrian}. Bajo esta analogía, un sistema de predicción conformal Mondriano particiona el espacio de ejemplos $\mathcal{Z}$ en categorías mutuamente excluyentes o taxonomías. Este enfoque permite que las garantías de cobertura sean válidas no solo de forma agregada, sino específicamente dentro de cada subgrupo definido, abordando así el problema de la validez condicional.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{./Imagenes/mondrian.png}
    \caption{Estética de Mondrian como analogía de la partición del espacio $\mathcal{Z}$.}
    \label{fig:analogia_mondrian}
\end{figure}


La necesidad de este enfoque surge cuando existen grupos de datos con dificultades predictivas heterogéneas. Por ejemplo, en una serie temporal de demanda eléctrica, predecir el consumo en un día festivo es intrínsecamente más difícil que en un día laboral. Un CPS global podría subestimar masivamente la incertidumbre en los días festivos, compensándola con una sobreestimación en los días laborales. El enfoque de Mondrian garantiza que la probabilidad de error sea exactamente $\epsilon$ tanto para los días laborales como para los festivos, de forma independiente \cite{Vovk2017wp}.

\subsection{La Taxonomía de Mondrian ($\kappa$)}

La base matemática de un MCPS es la \textit{taxonomía}. Una taxonomía es una función medible $\kappa: \mathbb{N} \times (\mathbf{X} \times \mathbf{Y}) \to K$, donde $K$ es un conjunto numerable de categorías. Para cada par de ejemplo $(x_i, y_i)$ y su posición en la secuencia $i$, la taxonomía asigna una categoría $\kappa_i$.

Existen tres tipos principales de taxonomías aplicables a series temporales:
\begin{enumerate}
    \item \textbf{Taxonomías de Objetos:} Dependen solo de las características $x_i$ (ej. agrupar por niveles de volatilidad observada).
    \item \textbf{Taxonomías de Etiquetas:} Dependen de la respuesta $y_i$. Esto da lugar a los \textit{Label-Conditional Conformal Predictors}, vitales cuando el impacto de un error depende de la magnitud del valor (ej. errores en valores extremos son más costosos).
    \item \textbf{Taxonomías Temporales:} Dependen del índice $i$. Este es el puente con el trabajo de Barber et al. \cite{Barber2023}, donde la categoría de Mondrian puede ser una ``ventana deslizante" de los datos más recientes para adaptarse a la no-intercambiabilidad.
\end{enumerate}

\subsection{Integración del Algoritmo MCPS}

La integración de la lógica de Mondrian en un Sistema de Predicción Conformal se realiza modificando el cálculo del p-valor o de la RPD (Distribución Predictiva Aleatorizada). En lugar de comparar el puntaje del nuevo ejemplo $\alpha_n$ con todos los puntajes históricos, solo se compara con aquellos que pertenecen a su misma categoría.

Sea $\sigma = \{z_1, \dots, z_{n-1}\}$ el conjunto de entrenamiento y $z_n = (x_n, y)$ el ejemplo de prueba con etiqueta hipotética $y$. El proceso para generar la RPD de Mondrian $\Pi_{M}$ es el siguiente:

\begin{enumerate}
    \item Se identifica la categoría del nuevo ejemplo: $k = \kappa(n, (x_n, y))$.
    \item Se filtran los índices de los ejemplos de entrenamiento que pertenecen a dicha categoría:
    \begin{equation}
        S_k = \{i \in \{1, \dots, n-1\} : \kappa(i, z_i) = k\}
    \end{equation}
    \item Se calculan los puntajes de conformidad $\alpha_i$ solo para $i \in S_k \cup \{n\}$.
    \item La RPD de Mondrian se define como:
    \begin{equation}
    \Pi_{M}(y, \tau) := \frac{|\{i \in S_k : \alpha_i < \alpha_n^y\}| + \tau |\{i \in S_k \cup \{n\} : \alpha_i = \alpha_n^y\}|}{|S_k| + 1}
    \label{eq:mondrian_rpd}
    \end{equation}
\end{enumerate}

El denominador $|S_k| + 1$ es clave: representa el tamaño de la ``muestra local". Si una categoría tiene pocos ejemplos, la distribución predictiva será naturalmente más dispersa (reflejando mayor incertidumbre), mientras que categorías ricas en datos producirán densidades más nítidas \cite{Vovk2022}.

\section{Análisis de la Consistencia Universal de Vovk}
\label{sec:sustento_teorico_vovk}

Para consolidar el marco teórico de esta investigación, es imperativo discutir el sustento matemático que garantiza que los Sistemas de Predicción Conformal (CPS) no solo son válidos en muestras finitas, sino también óptimos a medida que el volumen de datos aumenta. Este respaldo proviene de la demostración de la \textit{consistencia universal} de Vovk (\cite{Vovk2019}), formalizada en el Teorema 31 de su obra reciente.

\subsection{Definición de Consistencia Universal}

En el contexto de los CPS, la validez (propiedad R2) asegura que el sistema está calibrado independientemente de la distribución de los datos. Sin embargo, la validez por sí sola no garantiza que la distribución predictiva $\Pi_n$ sea una buena aproximación a la verdadera distribución condicional de las etiquetas $P(y|x)$.

Vovk define un sistema predictivo como \textit{universalmente consistente} si, para cualquier medida de probabilidad $P$ (bajo el modelo IID) y para cualquier función continua acotada $f$, se cumple que:
\begin{equation}
    \int f d\Pi_n - \mathbb{E}_P(f | x_{n+1}) \to 0 \quad \text{en probabilidad cuando } n \to \infty
\end{equation}
Esta propiedad implica que, asintóticamente, el CPS ``encuentra" la verdadera distribución de probabilidad generadora de los datos, eliminando la incertidumbre epistémica conforme el tamaño de la muestra $n$ tiende al infinito \cite{Vovk2019}.

\subsection{Mecanismo de la Demostración: El Enfoque de Histograma}

La prueba de Vovk sobre la existencia de un CPS universal se apoya en la construcción de un \textit{Histogram Conformal Predictive System}. El argumento se divide en dos pilares fundamentales que vinculan la teoría de martingalas con la ley de los grandes números:

\begin{enumerate}
    \item \textbf{Teorema de Convergencia de Martingalas de Lévy:} Vovk utiliza particiones anidadas del espacio de objetos $X$ (celdas de histograma que se encogen conforme $n$ crece). Según el teorema de Lévy, la esperanza condicional de la función sobre una celda que se reduce tiende al valor puntual de la esperanza condicional en el objeto de prueba $x_{n+1}$ \cite{Vovk2019}.
    
    \item \textbf{Ley de los Grandes Números (LGN):} Mientras las celdas se encogen para ganar resolución, el número de ejemplos dentro de cada celda debe tender a infinito ($nh_n \to \infty$, donde $h_n$ es el ancho de la celda). Esto permite que la frecuencia empírica de las etiquetas dentro de la categoría de Mondrian converja a la esperanza real en esa región del espacio \cite{Vovk2019}.
\end{enumerate}

\subsection{La Distancia de Lévy y la Convergencia Débil}

Un punto crítico de la demostración es el uso de la noción de Belyaev sobre secuencias de distribuciones que se aproximan débilmente. Vovk demuestra que bajo un CPS universal, la \textit{Distancia de Lévy} entre la distribución predictiva conformal y la verdadera distribución condicional converge a cero en probabilidad \cite{Vovk2019}. 

Este resultado es el que otorga rigor a la aplicación de CPS en problemas de alta criticidad, como el pronóstico de carga eléctrica o la gestión de riesgos financieros. Indica que el analista no tiene que elegir entre un modelo ``seguro" (conformal) y un modelo ``preciso" (bayesiando/paramétrico); el CPS universal ofrece ambas ventajas simultáneamente:
\begin{itemize}
    \item \textbf{A corto plazo:} Garantiza cobertura exacta mediante calibración fuerte.
    \item \textbf{A largo plazo:} Garantiza convergencia a la distribución real de los datos sin requerir asunciones paramétricas.
\end{itemize}

\subsection{Implicaciones para el LSPM y la Eficiencia}

Aunque el modelo de mínimos cuadrados (LSPM) estudiado en la sección~\ref{sec:cps_densidades_LSPM} es eficiente bajo ruido gaussiano, Vovk advierte que no es universalmente consistente si la relación real entre $X$ y $Y$ no es lineal \cite{Vovk2005}. Por ello, el desarrollo de CPS basados en kernels o en métodos de vecinos cercanos (como se discute en el capítulo 4 de su obra) es lo que permite alcanzar la consistencia universal en espacios de características complejos. Esta conclusión justifica el uso de arquitecturas no lineales conformizadas en la presente tesis, ya que heredan la solidez de la prueba de consistencia de Vovk.
\section{Hacia la Consistencia Universal en Series de Tiempo Ergódicas}
\label{sec:extension_ergodica}

Si bien el trabajo de Vovk (\cite{Vovk2019}) establece una base sólida para la consistencia universal bajo el modelo IID, las aplicaciones en entornos reales, como las series de tiempo, exigen una transición hacia modelos que capturen la dependencia temporal. Inspirado en el formalismo de Vovk, el presente marco teórico propone las bases para un Sistema de Predicción Conformal (CPS) adaptado a procesos estocásticos donde la suposición de intercambiabilidad no se cumple.

\subsection{Redefinición del Objetivo de Consistencia}

En el contexto de series de tiempo, el objetivo de un CPS universalmente consistente es que la distribución predictiva generada, $Q_n(y)$, converja débilmente en probabilidad a la verdadera distribución condicional $F_{Y|X}(\cdot|X_{n+1})$. A diferencia del caso IID, aquí la ``consistencia" implica que el sistema debe ser capaz de aprender la dinámica local y la estructura de dependencia del proceso a medida que la serie evoluciona.

Se plantea que, bajo este régimen, el sistema no solo debe ser asintóticamente válido, sino también \textit{eficiente}, adaptándose a la heterocedasticidad (volatilidad cambiante) intrínseca de los datos secuenciales.

\subsection{Supuestos Fundamentales del Marco Propuesto}

Para transitar de la teoría de Vovk a procesos dependientes, se han identificado los siguientes supuestos como pilares necesarios para el desarrollo de una prueba de consistencia futura:

\begin{enumerate}
    \item \textbf{Estacionariedad y Ergodicidad:} Se asume que el proceso $\{Z_t\}$ es estrictamente estacionario y ergódico. Esto garantiza que los promedios temporales observados en la ventana de datos converjan a los promedios del ensamble, permitiendo que el sistema ``aprenda" de la historia pasada.
    
    \item \textbf{Condición de $\alpha$-mixing (Mezcla Fuerte):} Para manejar la dependencia, se requiere que el proceso sea $\alpha$-mixing con coeficientes que decaigan algebraicamente ($\alpha(k) \leq Ck^{-\beta}, \beta > 2$). Este supuesto es crucial para aplicar teoremas límite central y asegurar que las observaciones lejanas en el tiempo sean casi independientes.
    
    \item \textbf{Regularidad de Lipschitz:} A diferencia del enfoque de histograma de celdas discretas, aquí se asume que tanto la función de regresión $\mu(x)$ como la distribución de los residuos $G(s|x)$ son Lipschitz continuas respecto al espacio de covariables. Esto asegura que puntos cercanos en el tiempo y espacio tengan comportamientos predictivos similares.
\end{enumerate}

\subsection{Mecanismo Propuesto: Transductor Conformal por Kernel}

En lugar del enfoque de histogramas anidados de Vovk, este marco propone una arquitectura adaptativa basada en dos componentes:

\begin{itemize}
    \item \textbf{Ventana Temporal Móvil ($L_n$):} Un mecanismo de truncamiento que selecciona las últimas $L_n$ observaciones. Para alcanzar la consistencia, el tamaño de esta ventana debe crecer con $n$ pero a un ritmo controlado ($L_n \to \infty$).
    \item \textbf{Suavizado Espacial por Kernel ($K, h_n$):} En lugar de asignar pesos uniformes a la celda (como en Mondrian), se propone el uso de pesos de relevancia espacial $w_i = K(\frac{d(X_i, X_{n+1})}{h_n})$. Esto permite que el sistema pondere los residuos pasados no solo por su cercanía temporal, sino por su similitud en el espacio de características.
\end{itemize}

\subsection{Discusión y Perspectivas Futuras}

Esta formulación plantea que la convergencia de la integral $\int f dQ_n$ hacia la esperanza condicional real depende del balance entre el sesgo del kernel y la varianza inducida por la dependencia de los datos. Mientras que Vovk utiliza el Teorema de Lévy para martingalas, el análisis en series de tiempo requiere el uso de técnicas de \textit{análisis de sesgo-varianza para estimadores no paramétricos en procesos mixing}.

Es importante notar que este planteamiento se presenta como una \textit{hoja de ruta teórica}. La validación de que este transductor conformal ergódico alcanza la consistencia universal bajo cualquier proceso mixing representaría una extensión significativa del trabajo original de Vovk, unificando la robustez de la predicción conformal con la flexibilidad de la estimación no paramétrica para datos secuenciales de alta complejidad.