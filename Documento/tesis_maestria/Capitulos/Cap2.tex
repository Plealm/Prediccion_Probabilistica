% !TeX root = ../main.tex
\chapter{Sistemas de Predicción Conformal}
\label{cap:marco_teorico}

En este capítulo se presentan los fundamentos teóricos que sustentan el desarrollo de esta investigación. Se inicia con una introducción al pronóstico probabilístico y su importancia en el análisis de series temporales, seguido de una discusión detallada sobre las métricas utilizadas para evaluar el desempeño predictivo. Posteriormente, se desarrollan los conceptos fundamentales de la predicción conformal y sus adaptaciones para series de tiempo.

\section{Pronóstico Probabilístico}
\label{sec:pronostico_probabilistico}

El pronóstico probabilístico representa un cambio de paradigma fundamental en la predicción estadística, pasando de estimaciones puntuales a distribuciones de probabilidad completas sobre cantidades futuras de interés \parencite{Gneiting2014}. A diferencia de las predicciones puntuales tradicionales, que proporcionan únicamente un valor esperado o una estimación central, el pronóstico probabilístico cuantifica la incertidumbre asociada a la predicción mediante la especificación de una distribución predictiva completa \parencite{GneitingBalabdaoui2007}.

\subsection{Definición y Objetivos}

Formalmente, sea $Y_{t+h}$ una variable aleatoria que representa el valor de una serie temporal en el tiempo $t+h$, donde $h>0$ denota el horizonte de predicción. Un pronóstico probabilístico es una distribución de probabilidad $F_{t+h|t}$ que caracteriza la incertidumbre sobre $Y_{t+h}$ dado el conjunto de información disponible hasta el tiempo $t$, denotado por $\mathcal{F}_t$ \parencite{Gneiting2014}.

Gneiting y Raftery explican que el objetivo fundamental del pronóstico probabilístico es maximizar la nitidez de las distribuciones predictivas sujeto a calibración. Estos dos conceptos son fundamentales para entender la calidad de un pronóstico probabilístico \parencite{GneitingRaftery2007, GneitingBalabdaoui2007}:

\begin{itemize}
\item \textbf{Calibración:} Se refiere a la concordancia estadística entre las distribuciones predictivas y las observaciones. Una predicción está calibrada si las realizaciones son estadísticamente indistinguibles de muestras aleatorias de las distribuciones predictivas \parencite{Thorarinsdottir2017}.

\item \textbf{Nitidez (\textit{sharpness}):} Se refiere a la concentración de las distribuciones predictivas y es una propiedad exclusiva de los pronósticos. Cuanto más concentradas sean las distribuciones predictivas, mejor, siempre que se mantenga la calibración \parencite{GneitingRaftery2007}.
\end{itemize}

La Figura~\ref{fig:tipos_prediccion} ilustra la diferencia entre una predicción puntual, un intervalo de predicción y una distribución predictiva completa.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{./Imagenes/Cap2_prediccion.png}
\caption{Tipos de predicciones y su relación con la incertidumbre.}
\label{fig:tipos_prediccion}
\end{figure}

\subsection{Ventajas del Pronóstico Probabilístico}

El pronóstico probabilístico ofrece múltiples ventajas sobre las predicciones puntuales tradicionales que justifican su adopción creciente en diversas aplicaciones \parencite{Gneiting2014}. A diferencia de las predicciones puntuales que proporcionan únicamente un valor representativo (típicamente el valor esperado, aunque también puede ser alguna otra estadística de centralidad o de interés para el tomador de decisiones), el pronóstico probabilístico caracteriza la incertidumbre de manera exhaustiva mediante distribuciones de probabilidad completas \parencite{GneitingRaftery2007}. 

Una ventaja fundamental del pronóstico probabilístico sobre los intervalos de predicción es que proporciona la distribución completa en lugar de solo límites de confianza. Mientras que un intervalo de predicción al 95\% indica únicamente un rango donde se espera que caiga el valor futuro, la distribución completa permite evaluar tanto la magnitud esperada de un evento como su variabilidad asociada en cualquier región de interés. Esto facilita la optimización de funciones de utilidad esperada en contextos como gestión de inventarios, planificación energética o asignación de capital, donde las decisiones deben considerar explícitamente el trade-off entre riesgo y recompensa. 

Además, mientras las predicciones puntuales e intervalos de predicción son inherentemente limitados para caracterizar eventos raros, las distribuciones predictivas permiten estimar probabilidades de eventos extremos, información crucial para la gestión de riesgos financieros y planificación de infraestructura \parencite{Thorarinsdottir2017}. Finalmente, las distribuciones predictivas ofrecen flexibilidad comunicativa adaptable a diferentes audiencias mediante intervalos de predicción con diversos niveles de confianza o probabilidades de excedencia de umbrales críticos \parencite{Gneiting2014}.

Estas ventajas han motivado la transición hacia pronósticos probabilísticos en campos tan diversos como meteorología, finanzas, energía, epidemiología y gestión de cadenas de suministro \parencite{Gneiting2014, Salinas2020}.

\section{Métricas para Evaluación de Pronósticos Probabilísticos}
\label{sec:metricas_evaluacion}

La evaluación rigurosa del desempeño predictivo es fundamental para comparar metodologías de pronóstico y guiar mejoras en los modelos. En el contexto de pronósticos probabilísticos, las métricas de evaluación deben considerar tanto la calibración como la nitidez de las distribuciones predictivas \parencite{GneitingBalabdaoui2007, Thorarinsdottir2017}.

\subsection{Reglas de Puntuación Propias}
\label{subsec:scoring_rules}

Una \textit{regla de puntuación} (\textit{scoring rule}) es una función $S: \mathcal{F} \times \mathbb{R} \to \mathbb{R} \cup \{-\infty, \infty\}$ que asigna una penalización numérica $S(F,y)$ a cada par formado por una distribución predictiva $F \in \mathcal{F}$ y una observación realizada $y$ \parencite{GneitingRaftery2007}. En nuestra notación, valores más bajos de la puntuación indican mejor desempeño predictivo.

\subsubsection{Propiedad (Propriety) y Propiedad estricta (Strict Propriety)}

La propiedad (\textit{propriety}) es una característica fundamental que debe satisfacer toda métrica de evaluación de pronósticos probabilísticos para garantizar que incentive predicciones honestas y bien calibradas \parencite{GneitingRaftery2007}.

\textbf{Definición (Regla de Puntuación Propia (Proper)):} Una regla de puntuación $S$ es propia relativamente a una clase $\mathcal{F}$ de distribuciones de probabilidad si
\begin{equation}
\mathbb{E}_G[S(G, Y)] \leq \mathbb{E}_G[S(F, Y)]
\label{eq:proper_scoring}
\end{equation}
para todas las distribuciones $F, G \in \mathcal{F}$, donde $Y \sim G$ \parencite{GneitingRaftery2007, Thorarinsdottir2017}. 

Intuitivamente, la condición \eqref{eq:proper_scoring} establece que una regla de puntuación propia alcanza su valor mínimo esperado cuando la distribución pronosticada coincide con la distribución verdadera de los datos. En otras palabras, si los datos provienen de $G$, entonces el pronóstico que minimiza la puntuación esperada es precisamente $G$ mismo.

\textbf{Definición (Regla de Puntuación estrictamente propia (Strictly Proper)):} La regla de puntuación $S$ es estrictamente propia si la desigualdad en \eqref{eq:proper_scoring} se cumple con igualdad únicamente cuando $F = G$ \parencite{GneitingRaftery2007}.

La importancia de la propiedad estricta radica en que establece un principio de alineación de incentivos: si un pronosticador desea minimizar su puntuación esperada, su mejor estrategia es reportar sinceramente su verdadera distribución predictiva \parencite{GneitingRaftery2007}. 

\subsection{Continuous Ranked Probability Score (CRPS)}
\label{subsec:crps}

El \textit{Continuous Ranked Probability Score} (CRPS) es una de las reglas de puntuación estrictamente propias más utilizadas para evaluar pronósticos probabilísticos de variables continuas \parencite{Gneiting2014}. Su popularidad se debe a su sólida fundamentación teórica y su capacidad para evaluar simultáneamente calibración y nitidez \parencite{GneitingRaftery2007, Thorarinsdottir2017}.

\subsubsection{Definiciones y Representaciones}

El CRPS admite varias representaciones matemáticas equivalentes, cada una con sus propias ventajas conceptuales y computacionales.

\textbf{Representación integral:} La definición original del CRPS está dada por \parencite{GneitingRaftery2007}:
\begin{equation}
\text{CRPS}(F, y) = \int_{-\infty}^{\infty} \left(F(x) - \mathbbm{1}\{y \leq x\}\right)^2 dx
\label{eq:crps_integral}
\end{equation}
donde $F$ es la función de distribución acumulada (FDA) de la distribución predictiva y $y$ es la observación realizada. Esta representación muestra que el CRPS es proporcional a el área entre la FDA predictiva y la FDA de la observación \parencite{Thorarinsdottir2017}.

\textbf{Representación basada en esperanzas:} Una forma alternativa, más conveniente para cálculos, está dada por \parencite{GneitingRaftery2007}:
\begin{equation}
\text{CRPS}(F, y) = \mathbb{E}_F|X - y| - \frac{1}{2}\mathbb{E}_F|X - X'|
\label{eq:crps_expectation}
\end{equation}
donde $X$ y $X'$ son variables aleatorias independientes con distribución $F$. Esta representación revela una interpretación intuitiva del CRPS: el primer término mide la distancia esperada entre la distribución predictiva, representada por la variable aleatoria X, y la observación; mientras que el segundo término penaliza la dispersión de la distribución predictiva.

\subsubsection{Propiedades del CRPS}

El CRPS posee varias propiedades deseables que explican su amplia adopción en la literatura \parencite{GneitingRaftery2007, Thorarinsdottir2017}:

\begin{enumerate}
\item \textbf{Estrictamente propia:} El CRPS es \textit{estrictamente propia} relativo a la clase de todas las distribuciones de probabilidad en $\mathbb{R}$ con primer momento finito \parencite{GneitingRaftery2007}.

\item \textbf{Unidades consistentes:} El CRPS se expresa en las mismas unidades que la variable pronosticada \parencite{Gneiting2014}.

\item \textbf{Reducción al error absoluto:} Cuando $F$ es una distribución degenerada (predicción puntual), el CRPS se reduce al error absoluto $|x-y|$, permitiendo un marco de evaluación unificado \parencite{GneitingRaftery2007}.

\item \textbf{Sensibilidad dual:} El CRPS evalúa simultáneamente la calibración y la nitidez \parencite{Thorarinsdottir2017}.
\end{enumerate}

\subsection{Expected Continuous Ranked Probability Score (ECRPS)}
\label{subsec:ecrps}

El \textit{Expected Continuous Ranked Probability Score} (ECRPS) extiende el CRPS al caso donde se desea cuantificar la discrepancia entre dos distribuciones probabilísticas completas: una distribución predictiva $F$ y una distribución de referencia $G$.

\subsubsection{Definición del ECRPS}

\textbf{Representación integral:} Siguiendo la forma integral del CRPS, el ECRPS se define como:
\begin{equation}
\text{ECRPS}(F, G) = \int_{-\infty}^{\infty} \left(F(x) - G(x)\right)^2 dx
\label{eq:ecrps_integral}
\end{equation}
donde $F$ y $G$ son las funciones de distribución acumulada de las distribuciones predictiva y de referencia, respectivamente. Esta representación mide el área cuadrática entre ambas FDAs.

\textbf{Representación basada en esperanzas:} De forma análoga a la ecuación \eqref{eq:crps_expectation}, el ECRPS puede expresarse como:
\begin{equation}
\text{ECRPS}(F, G) = \mathbb{E}_{F,G}|X - Y| - \frac{1}{2}\mathbb{E}_F|X - X'| - \frac{1}{2}\mathbb{E}_G|Y - Y'|
\label{eq:ecrps_expectation}
\end{equation}
donde $X, X' \sim F$ son variables aleatorias independientes con distribución $F$, y $Y, Y' \sim G$ son variables aleatorias independientes con distribución $G$. Esta forma revela la estructura del ECRPS: el primer término mide la distancia esperada entre ambas distribuciones, mientras que los dos últimos términos penalizan la dispersión de cada distribución.

\subsubsection{Estimación empírica}

En la práctica, cuando se dispone de muestras $\vec{x} = \{x_1, \ldots, x_n\}$ de $F$ y $\vec{y} = \{y_1, \ldots, y_m\}$ de $G$, el ECRPS puede estimarse mediante:
\begin{equation}
\widehat{\text{ECRPS}}(F, G) = \frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^m |x_i - y_j| - \frac{1}{2n^2}\sum_{i=1}^n\sum_{k=1}^n |x_i - x_k| - \frac{1}{2m^2}\sum_{j=1}^m\sum_{\ell=1}^m |y_j - y_\ell|
\label{eq:ecrps_empirical}
\end{equation}

El ECRPS hereda las propiedades deseables del CRPS, siendo estrictamente propio y sensible tanto a sesgos sistemáticos como a diferencias en la dispersión entre las distribuciones comparadas.



\subsection{Z-Score Normalizado}

Para facilitar la comparación del desempeño entre diferentes escenarios de simulación con escalas y niveles de dificultad heterogéneos, se emplea la normalización mediante Z-score del ECRPS. Para cada método de pronóstico $m$ evaluado en un conjunto de escenarios $\mathcal{S}$, el Z-score se define como:
\begin{equation}
Z_m = \frac{\text{ECRPS}_m - \mu_{\mathcal{S}}}{\sigma_{\mathcal{S}}}
\label{eq:zscore}
\end{equation}
donde $\mu_{\mathcal{S}}$ y $\sigma_{\mathcal{S}}$ representan la media y desviación estándar del ECRPS calculados sobre todos los métodos en el conjunto de escenarios $\mathcal{S}$.

Esta normalización permite identificar de manera clara en qué escenarios particulares un método presenta dificultades (valores positivos altos) o se desempeña excepcionalmente bien (valores negativos). Por ejemplo, un método conformal con ponderación temporal podría mostrar Z-scores bajos en escenarios estacionarios pero Z-scores elevados ante cambios estructurales abruptos, revelando limitaciones específicas del esquema de ponderación empleado.

\subsection{Transformación Integral de Probabilidad (PIT)}

La Transformación Integral de Probabilidad (PIT, por sus siglas en inglés) constituye una herramienta fundamental para evaluar la calibración probabilística de pronósticos \parencite{Dawid1984, DieboldGunther1998}. Para una distribución predictiva $F_t$ y una observación realizada $y_t$, el valor PIT se define como:
\begin{equation}
p_t = F_t(y_t)
\label{eq:pit}
\end{equation}

Si los pronósticos están perfectamente calibrados y $F_t$ es continua, entonces $p_t \sim \text{Uniform}(0,1)$ \parencite{GneitingBalabdaoui2007}. La evaluación de calibración se realiza mediante histogramas de los valores PIT acumulados sobre el horizonte de evaluación. La forma del histograma revela tipos específicos de descalibración \parencite{Thorarinsdottir2017}:

\begin{itemize}
\item \textbf{Histograma en forma de U:} Indica distribuciones predictivas subdispersas (intervalos de predicción demasiado estrechos).
\item \textbf{Histograma en forma de $\cap$:} Indica distribuciones predictivas sobredispersas (intervalos de predicción demasiado amplios).
\item \textbf{Histograma triangular:} Sugiere sesgo sistemático en las predicciones.
\item \textbf{Histograma uniforme:} Evidencia calibración probabilística adecuada.
\end{itemize}

Es importante notar que la uniformidad del PIT es una condición necesaria pero no suficiente para pronósticos ideales, como lo demuestran los ejemplos contraintuitivos de Hamill \parencite{Hamill2001} y la discusión en \textcite{GneitingBalabdaoui2007}. Por esta razón, el análisis PIT debe complementarse con evaluaciones de nitidez.

\subsection{Curvas de Confiabilidad (Reliability Diagrams)}

Las curvas de confiabilidad proporcionan una evaluación visual de la calibración marginal al comparar las probabilidades pronosticadas con las frecuencias observadas \parencite{GneitingBalabdaoui2007}. Para un conjunto de umbrales $\{u_1, \ldots, u_K\}$, se define:

\begin{itemize}
\item \textbf{Probabilidad pronosticada:} $\hat{p}_k = \frac{1}{T}\sum_{t=1}^T F_t(u_k)$, la probabilidad media de que la observación sea menor o igual al umbral $u_k$.
\item \textbf{Frecuencia observada:} $\bar{o}_k = \frac{1}{T}\sum_{t=1}^T \mathbbm{1}\{y_t \leq u_k\}$, la proporción de observaciones que efectivamente no excedieron $u_k$.
\end{itemize}

La curva de confiabilidad grafica $\bar{o}_k$ versus $\hat{p}_k$. Para pronósticos perfectamente calibrados, los puntos deberían alinearse sobre la diagonal de 45 grados. Desviaciones sistemáticas indican problemas de calibración:

\begin{itemize}
\item Puntos por debajo de la diagonal: el método subestima las probabilidades de no-excedencia (sobreestima eventos extremos).
\item Puntos por encima de la diagonal: el método sobreestima las probabilidades de no-excedencia (subestima eventos extremos).
\end{itemize}

Esta herramienta es particularmente útil para identificar si la descalibración es uniforme a lo largo de toda la distribución predictiva o si se concentra en regiones específicas como las colas \parencite{Thorarinsdottir2017}.


\section{Test de Diebold-Mariano}
\label{sec:test_diebold_mariano}

La evaluación comparativa de distintas metodologías de pronóstico requiere herramientas estadísticas que permitan determinar si las diferencias observadas en el desempeño predictivo son estadísticamente significativas o simplemente producto del azar. El test de Diebold-Mariano \parencite{DieboldMariano1995} constituye uno de los procedimientos más ampliamente utilizados para este propósito, ofreciendo un marco general y flexible para contrastar la hipótesis nula de igual precisión predictiva entre dos métodos de pronóstico competidores.

\subsection{Formulación del Test}

Sean $\hat{y}_{t+h}^{(1)}$ y $\hat{y}_{t+h}^{(2)}$ dos pronósticos $h$ pasos adelante para una variable $y_{t+h}$, producidos por dos metodologías diferentes. Los errores de pronóstico correspondientes son:
\begin{equation}
e_{t+h}^{(i)} = y_{t+h} - \hat{y}_{t+h}^{(i)}, \quad i = 1, 2
\end{equation}

El test de Diebold-Mariano se basa en una función de pérdida $L(\cdot)$ que cuantifica el costo asociado con cada error de pronóstico. Para un horizonte temporal de evaluación que abarca $n$ observaciones, se define el diferencial de pérdida en el tiempo $t$ como:
\begin{equation}
d_t = L(e_t^{(1)}) - L(e_t^{(2)}), \quad t = 1, \ldots, n
\label{eq:loss_differential}
\end{equation}

Tradicionalmente, el test de Diebold-Mariano se ha aplicado utilizando la pérdida cuadrática para evaluar estimaciones puntuales. Sin embargo, este marco es suficientemente general para acomodar cualquier función de pérdida \parencite{DieboldMariano1995}. En el presente trabajo, se utilizará principalmente el CRPS o el ECRPS, según corresponda como métrica de pérdida fundamental. Esto permite extender la comparación de Diebold-Mariano.

La hipótesis nula de igual precisión predictiva se formula como:
\begin{equation}
H_0: \mathbb{E}[d_t] = 0
\label{eq:dm_null}
\end{equation}

Esta hipótesis establece que la pérdida esperada es idéntica para ambos métodos de pronóstico. El estadístico de prueba se construye a partir de la media muestral del diferencial de pérdida:
\begin{equation}
\bar{d} = \frac{1}{n}\sum_{t=1}^n d_t
\end{equation}

\subsection{Distribución Asintótica y Estimación de la Varianza}

Bajo condiciones de regularidad de procesos temporales que incluyen la estacionariedad débil y la existencia de momentos de orden finito, Diebold y Mariano demuestran que:
\begin{equation}
\sqrt{n}\,\bar{d} \xrightarrow{d} N(0, 2\pi f_d(0))
\end{equation}
donde $f_d(0)$ denota la densidad espectral de la serie $d_t$ evaluada en frecuencia cero, la cual equivale a la varianza de largo plazo:
\begin{equation}
\sigma^2 = \text{Var}(\sqrt{n}\,\bar{d}) = \gamma_0 + 2\sum_{k=1}^\infty \gamma_k
\label{eq:long_run_variance}
\end{equation}
siendo $\gamma_k = \text{Cov}(d_t, d_{t-k})$ la autocovarianza de orden $k$.

Un aspecto fundamental del test de Diebold-Mariano es que permite explícitamente la presencia de autocorrelación en el diferencial de pérdida $d_t$. Esta característica es especialmente relevante en el contexto de pronósticos a múltiples pasos adelante ($h > 1$), donde los errores de pronóstico exhiben típicamente estructura de autocorrelación hasta el orden $(h-1)$ \parencite{DieboldMariano1995}. Esta estructura surge porque pronósticos óptimos $h$ pasos adelante generan errores que siguen un proceso de media móvil MA$(h-1)$.

En la práctica, la varianza de largo plazo $\sigma^2$ debe ser estimada. Diebold y Mariano proponen utilizar un estimador basado en autocovarianzas ponderadas por kernel:
\begin{equation}
\hat{\sigma}^2 = \hat{\gamma}_0 + 2\sum_{k=1}^{M} k\left(\frac{k}{M}\right)\hat{\gamma}_k
\label{eq:kernel_variance_estimator}
\end{equation}
donde $\hat{\gamma}_k = n^{-1}\sum_{t=k+1}^n (d_t - \bar{d})(d_{t-k} - \bar{d})$ son las autocovarianzas muestrales, $k(\cdot)$ es una función kernel (por ejemplo, kernel de Bartlett o Parzen), y $M$ es el parámetro de ancho de banda o truncamiento que controla el número de autocovarianzas incluidas en la estimación.

Para el caso específico donde se conoce que el diferencial de pérdida sigue un proceso MA$(h-1)$, Diebold y Mariano sugieren simplificar el estimador utilizando $M = h-1$ con kernel rectangular:
\begin{equation}
\hat{\sigma}^2_{DM} = \hat{\gamma}_0 + 2\sum_{k=1}^{h-1} \hat{\gamma}_k
\label{eq:dm_variance_simple}
\end{equation}

El estadístico de prueba resultante es:
\begin{equation}
DM = \frac{\sqrt{n}\,\bar{d}}{\hat{\sigma}}
\label{eq:dm_statistic}
\end{equation}

Bajo la hipótesis nula, este estadístico converge en distribución a una normal estándar: $DM \xrightarrow{d} N(0,1)$. Para un test bilateral al nivel de significancia $\alpha$, se rechaza $H_0$ si $|DM| > z_{\alpha/2}$, donde $z_{\alpha/2}$ denota el cuantil $(1-\alpha/2)$ de la distribución normal estándar.

\subsection{Modificaciones para Muestras Pequeñas}

A pesar de la solidez teórica del test de Diebold-Mariano bajo la teoría asintótica estándar, diversos estudios han documentado distorsiones de tamaño en muestras finitas, particularmente cuando el número de observaciones de pronóstico es limitado. Harvey et al. \parencite{Harvey1997} demostraron mediante simulaciones Monte Carlo que el test original tiende a rechazar la hipótesis nula (es decir, presenta un tamaño empírico superior al nominal), especialmente para horizontes de pronóstico largos y muestras pequeñas.

Para abordar estas limitaciones, Harvey et al. proponen una corrección del estadístico que mejora sustancialmente el desempeño en muestras finitas. La modificación se fundamenta en el uso de un estimador aproximadamente insesgado de la varianza de $\bar{d}$. Partiendo de la expresión exacta:
\begin{equation}
\text{Var}(\bar{d}) = n^{-1}\left[\gamma_0 + 2n^{-1}\sum_{k=1}^{h-1}(n-k)\gamma_k\right]
\end{equation}
y calculando el valor esperado del estimador empleado en \eqref{eq:dm_variance_simple}, se obtiene que:
\begin{equation}
\mathbb{E}[\hat{\sigma}^2_{DM}] \approx \left[\frac{n+1-2h+n^{-1}h(h-1)}{n}\right]\text{Var}(\bar{d})
\end{equation}

Esta relación sugiere el estadístico modificado:
\begin{equation}
DM^* = \left[\frac{n+1-2h+n^{-1}h(h-1)}{n}\right]^{1/2} DM
\label{eq:dm_modified}
\end{equation}

Adicionalmente, Harvey et al. recomiendan comparar $DM^*$ con valores críticos de la distribución $t$ de Student con $(n-1)$ grados de libertad, en lugar de la distribución normal estándar. Esta segunda modificación reconoce implícitamente la incertidumbre adicional asociada con la estimación de la varianza en muestras finitas.

Los resultados de simulación reportados por Harvey et al. \parencite{Harvey1997} indican que el test modificado presenta un tamaño empírico considerablemente más cercano al nominal, especialmente para $n \leq 50$ y horizontes de pronóstico $h \geq 2$. Aunque el test modificado exhibe una ligera pérdida de potencia en comparación con el test original cuando ambos están correctamente calibrados, esta reducción es marginal y ampliamente compensada por la ganancia en confiabilidad inferencial.

\subsection{Enfoque de la Teoria Asintótica de Suavizado Fijo}

Una alternativa más reciente para abordar las distorsiones de tamaño del test de Diebold-Mariano en muestras pequeñas es el enfoque de \textit{asintótica de suavizado fijo} (fixed-smoothing asymptotics), desarrollado por Coroneo e Iacone \parencite{CoroneoIacone2020}. Este marco teórico reconoce que en aplicaciones prácticas de evaluación de pronósticos, el tamaño muestral $n$ es frecuentemente limitado, haciendo que la aproximación asintótica estándar (que requiere $M/n \to 0$) sea inadecuada.

La idea fundamental es mantener constante la razón entre el parámetro de ancho de banda y el tamaño muestral conforme $n$ aumenta. Formalmente, bajo la \textit{asintótica fixed-b}, se asume que $M/n \to b$ para algún $b \in (0,1]$ fijo. Bajo este régimen asintótico alternativo, el estimador de varianza \eqref{eq:kernel_variance_estimator} ya no es consistente para $\sigma^2$. Sin embargo, Kiefer y Vogelsang (2005) \parencite{kiefer2005} demostraron que el estadístico resultante converge a una distribución no estándar que depende de $b$ y del kernel empleado.

Para el kernel de Bartlett, la distribución límite puede caracterizarse explícitamente, y sus cuantiles pueden aproximarse mediante fórmulas polinomiales. Específicamente, para un test bilateral al 5\% de significancia, el valor crítico $c_\alpha(b)$ satisface:
\begin{equation}
c_\alpha(b) \approx \alpha_0 + \alpha_1 b + \alpha_2 b^2 + \alpha_3 b^3
\end{equation}
donde los coeficientes $\{\alpha_i\}$ han sido tabulados por Kiefer y Vogelsang.

Coroneo e Iacone \parencite{CoroneoIacone2020} extienden este marco al contexto específico de evaluación de pronósticos, demostrando mediante simulaciones Monte Carlo que los tests basados en asintótica de suavizado fijo exhiben un tamaño empírico notablemente más preciso que el test de Diebold-Mariano estándar, incluso para muestras tan pequeñas como $n=40$. Los autores proponen utilizar anchos de banda $M = \lfloor n^{1/2} \rfloor$ para el estimador con kernel de Bartlett, encontrando que esta elección ofrece un equilibrio favorable entre tamaño y potencia del test.

Una segunda variante dentro del paradigma de suavizado fijo es la \textit{asintótica fixed-m}, que emplea un estimador de varianza basado en el periodograma suavizado con kernel de Daniell:
\begin{equation}
\hat{\sigma}^2_{DAN} = \frac{2\pi}{m}\sum_{j=1}^m I(\lambda_j)
\end{equation}
donde $I(\lambda_j)$ denota el periodograma de $d_t$ evaluado en la frecuencia de Fourier $\lambda_j = 2\pi j/n$, y $m$ es un parámetro de truncamiento mantenido fijo conforme $n \to \infty$. Bajo condiciones de regularidad, el estadístico resultante converge a una distribución $t$ con $2m$ grados de libertad \parencite{CoroneoIacone2020}.


\section{Predicción Conformal por Intervalos: El Enfoque IID}
\label{sec:prediccion_conformal_intervalos_IIE}

La predicción conformal clásica, introducida por Vovk et al. \cite{Vovk2005}, se fundamenta en la capacidad de generar conjuntos de predicción $\Gamma^\epsilon$ que garantizan una cobertura de confianza exacta para cualquier nivel de significancia $\epsilon \in (0,1)$. A diferencia de los métodos estadísticos tradicionales que dependen de la teoría asintótica (grandes muestras), la predicción conformal es válida para muestras finitas, siempre que se cumpla el supuesto de intercambiabilidad de los datos.

\subsection{El Concepto de No-Conformidad}

El núcleo de esta metodología es la \textit{medida de no-conformidad} (NCM, por sus siglas en inglés). Una NCM es una función $A(B, z)$ que cuantifica el grado de ``extrañeza" de un ejemplo $z$ en relación con un multiconjunto (o \textit{bag}) de ejemplos $B$. En el contexto de regresión, donde $z = (x, y)$, la medida de no-conformidad más común es el error absoluto de predicción, definido como:
\begin{equation}
    \alpha_i = |y_i - \hat{y}_i|
\end{equation}
donde $\hat{y}_i$ es la estimación producida por un algoritmo de aprendizaje subyacente (denominado \textit{underlying algorithm}). Es importante subrayar que la predicción conformal es agnóstica al modelo: puede envolver desde una regresión lineal simple hasta redes neuronales profundas, transformando sus predicciones puntuales en intervalos con validez estadística.

\subsection{Protocolo de Construcción de Intervalos}

Para construir un intervalo de predicción para un nuevo objeto $x_n$ , se define $z_i = (x_i, y_i)$ que permite escribir el un conjunto de entrenamiento como $z_1, \dots, z_{n-1}$, sobre el cual se realiza un proceso de prueba de hipótesis inversa. Para cada valor potencial $y \in \mathbb{R}$:

\begin{enumerate}
    \item \textbf{Conjunto aumentado:} Se asume hipotéticamente que la verdadera etiqueta de $x_n$ es $y$, formando el conjunto aumentado $z_1, z_2, \dots, z_{n-1}, z_n$, donde $z_n = (x_n, y)$.
    \item \textbf{Cálculo de Puntajes:} Se calculan los puntajes de no-conformidad $\alpha_1, \dots, \alpha_n$ para todos los elementos, incluyendo el ejemplo hipotético. 
    \item \textbf{Derivación del p-valor:} Se calcula la proporción de ejemplos que son ``al menos tan extraños'' como el nuevo ejemplo $z_n$:
    \begin{equation}
        p(y) = \frac{|\{i = 1, \dots, n : \alpha_i \geq \alpha_n\}|}{n}
        \label{eq:p_valor_clasico}
    \end{equation}
    \item \textbf{Inversión de la Región de Aceptación:} El intervalo de predicción $\Gamma^{\epsilon}$ se define como el conjunto de todos los valores $y$ que no pueden ser rechazados al nivel de significancia $\epsilon$:
    \begin{equation}
        \Gamma^\epsilon(x_1, y_1, \dots, x_{n-1}, y_{n-1}, x_n) = \{y \in \mathbb{R} : p(y) > \epsilon\}
    \end{equation}
\end{enumerate}

Este procedimiento garantiza que $P(y_n \notin \Gamma^\epsilon) \leq \epsilon$. Si los puntajes $\alpha_i$ tienen una distribución continua (sin empates), la probabilidad de error es exactamente $\epsilon$ \parencite{Vovk2005}.

La validez de cobertura mencionadas dependen fundamentalmente del supuesto de \textit{intercambiabilidad}. Esta propiedad es la que permite afirmar que, bajo la hipótesis nula de que $y$ es la etiqueta verdadera, el par $(x_n, y)$ es estadísticamente indistinguible de los datos de entrenamiento $z_1, \dots, z_{n-1}$. En términos prácticos, la intercambiabilidad asegura que el puntaje de no-conformidad del nuevo ejemplo, $\alpha_n$, tiene la misma probabilidad de ocupar cualquier posición en el ranking respecto a los puntajes previos $\alpha_1, \dots, \alpha_{n-1}$. Es precisamente esta distribución uniforme de los rangos de los puntajes lo que transforma el cálculo del p-valor en la ecuación \eqref{eq:p_valor_clasico} en una prueba de hipótesis válida, permitiendo que la probabilidad de error esté acotada por $\epsilon$ de manera exacta, independientemente de la distribución subyacente de los datos \parencite{Vovk2022}.

\section{Robustez ante la No-Intercambiabilidad: Aproximación de Barber}
\label{sec:barber_robustness_detail}

Uno de los desafíos críticos en el análisis de series temporales es que el supuesto de intercambiabilidad rara vez se sostiene. Fenómenos como la autocorrelación, la heterocedasticidad y la deriva de parámetros (drift) invalidan la asunción de que el pasado y el futuro son estadísticamente idénticos. Barber et al. proponen una extensión fundamental para estos escenarios.

\subsection{La brecha (gap) de Cobertura y Variación Total}


Barber et al. formalizan la degradación de la validez conformal mediante el uso de la \textit{Distancia de Variación Total} ($d_{TV}$). Si la distribución de los datos cambia en el tiempo, existe una brecha de cobertura (\textit{coverage gap}). El teorema principal de Barber establece que la pérdida de cobertura está acotada por la suma ponderada de las distancias entre la distribución de los datos de entrenamiento y la distribución del dato de prueba:

\begin{equation}
    \text{Coverage gap} \leq \frac{\sum_{i=1}^{n} w_i \cdot d_{TV}(Z, Z^i)}{1 + \sum_{i=1}^{n} w_i}
\end{equation}

donde:
\begin{itemize}
    \item $Z = (Z_1, \ldots, Z_{n+1})$ representa la secuencia completa de datos (entrenamiento y prueba)
    \item $Z^i$ denota la secuencia después de intercambiar el punto de prueba $(X_{n+1}, Y_{n+1})$ con el $i$-ésimo punto de entrenamiento $(X_i, Y_i)$
    \item $w_i \in [0,1]$ son pesos pre-especificados que reflejan la confianza en cada punto de datos
    \item $d_{TV}(Z, Z^i)$ es la distancia de variación total entre las distribuciones de $Z$ y $Z^i$
    \item $\epsilon$ representa el nivel de error nominal (típicamente $\alpha$ en predicción conformal estándar)
\end{itemize}


\subsection{Cuantiles Ponderados y Decaimiento Temporal}

Para contrarrestar este efecto en series de tiempo, Barber et al. introducen los \textit{Weighted Conformal Predictors}. En lugar de asignar un peso uniforme de $1/n$ a cada residuo histórico, se asignan pesos $w_i$ que reflejan la relevancia del dato. En series no estacionarias, los datos más recientes son mejores predictores del futuro.

Se define comúnmente un decaimiento geométrico para los pesos:
\begin{equation}
    w_i = \rho^{n-i}, \quad \rho \in (0, 1)
\end{equation}
donde un $\rho$ cercano a 1 asume una estabilidad lenta, mientras que un $\rho$ menor reacciona rápidamente a cambios estructurales. El p-valor pesado se calcula como una suma ponderada de funciones indicadoras:
\begin{equation}
    p^y = \frac{\sum_{i=1}^{n-1} w_i \mathbbm{1}_{\alpha_i \geq \alpha_n} + w_n}{\sum_{j=1}^n w_j}
\end{equation}
Este enfoque permite que la predicción conformal sea ``adaptativa", manteniendo la cobertura cercana al nivel nominal incluso cuando la serie temporal experimenta cambios súbitos en su media o varianza \parencite{Barber2023}.

\section{Sistemas de Predicción Conformal (CPS): De Intervalos a Densidades}
\label{sec:cps_densidades_LSPM}

El Capítulo 7 de la obra de \cite{Vovk2005} marca la transición de la predicción de intervalos a la predicción de distribuciones completas. Un \textit{Sistema de Predicción Conformal} (CPS) no entrega un rango, sino una \textit{Distribución Predictiva Aleatorizada} (RPD), denotada como $\Pi_n(y, \tau)$, que representa la función de distribución acumulada de la variable de interés.

\subsection{Formalización de la RPD y el Suavizado ($\tau$)}

Para asegurar que la distribución resultante sea continua y cumpla con las propiedades de una FDA (Función de Distribución Acumulada), se introduce una variable de suavizado $\tau \sim U(0,1)$. La función $\Pi$ se define como:
\begin{equation}
\Pi_n(y, \tau) := \frac{|\{i : \alpha_i < \alpha_n^y\}| + \tau |\{i : \alpha_i = \alpha_n^y\}|}{n}
\end{equation}

Es fundamental precisar que la validez de la RPD radica en que, bajo el supuesto de intercambiabilidad, la función $\Pi_n$ evaluada en la variable aleatoria subyacente genera una distribución uniforme. Específicamente, al evaluar la RPD en las etiquetas verdaderas $y_n$ junto a sus respectivas variables de suavizado $\tau_n$, los valores resultantes $P_n = \Pi_n(y_n, \tau_n)$ constituyen una muestra aleatoria de la distribución $U(0,1)$, propiedad conocida como \textit{calibración fuerte en probabilidad} \cite{Vovk2022}. En este proceso, el rol de $\tau \sim U(0,1)$ es resolver la falta de continuidad producida por los posibles empates en los puntajes de conformidad ($\alpha_i = \alpha_n^y$), permitiendo que una función de saltos se transforme en una medida aleatorizada continua y asegurando así una cuantificación exacta de la incertidumbre.


\subsection{La Máquina de Predicción de Mínimos Cuadrados (LSPM)}

LSPM (\textit{Least Squares Prediction Machine})  es la aplicación primordial de los CPS al ámbito de la regresión. La LSPM utiliza la estructura de la regresión lineal para optimizar la eficiencia de la distribución predictiva, aprovechando las propiedades geométricas del método de mínimos cuadrados.

Consideramos el problema de regresión con $p$ atributos. El espacio de objetos es $\mathbf{X} := \mathbb{R}^p$ y el espacio de ejemplos es $\mathbf{Z} := \mathbb{R}^{p+1} = \mathbb{R}^p \times \mathbb{R}$. La matriz de datos $\bar{X}$ es de dimensión $n \times p$, y la matriz sombrero (hat) se define como:
\begin{equation}
    \bar{H} = \bar{X}(\bar{X}^{\top}\bar{X})^{-1}\bar{X}^{\top}
\end{equation}

Los elementos de esta matriz se denotan como $\bar{h}_{i,j}$, siendo $\bar{h}_{i,i}$ el apalancamiento (\textit{leverage}) del $i$-ésimo punto de datos.

\subsubsection{Variantes de la LSPM}

Vovk distingue tres formas de calcular las medidas de no conformidad dentro de una LSPM, cada una con diferentes propiedades teóricas y prácticas:

\begin{enumerate}
    \item \textbf{LSPM Ordinaria:} Define la medida de no conformidad como el residuo ordinario:
    \begin{equation}
        A([z_1, \ldots, z_n], z_n) := y_n - \hat{y}_n
    \end{equation}
    donde $\hat{y}_n$ es la predicción para $y_n$ usando regresión de mínimos cuadrados sobre los objetos $x_n$ con $z_1, \ldots, z_{n-1}$ como conjunto de entrenamiento. Este enfoque tiende a ser demasiado optimista (sobreajuste), ya que el modelo ya ha ``visto'' los datos de entrenamiento.
    
    \item \textbf{LSPM Eliminada (\textit{Deleted}):} Utiliza un esquema de validación cruzada interna (\textit{leave-one-out}):
    \begin{equation}
        A([z_1, \ldots, z_n], z_n) := y_n - \hat{y}_{(n)}
    \end{equation}
    donde $\hat{y}_{(n)}$ es la predicción para $y_n$ usando mínimos cuadrados sobre $x_n$ con $z_1, \ldots, z_{n-1}$ como conjunto de entrenamiento (excluyendo $z_n$ del test). Esto asegura que el residuo sea una medida honesta de la capacidad de generalización.
    
    \item \textbf{LSPM Estudiantizada:} Es la variante más robusta y matemáticamente rigurosa. Ajusta cada residuo por su apalancamiento (\textit{leverage}), $\bar{h}_n$:
    \begin{equation}
        A([z_1, \ldots, z_n], z_n) := \frac{y_n - \hat{y}_n}{\sqrt{1 - \bar{h}_n}}
    \end{equation}
\end{enumerate}

\subsubsection{Forma Explícita de la Distribución Predictiva}

Para la LSPM estudiantizada, la distribución predictiva toma una forma particularmente elegante. Dado un conjunto de entrenamiento y un objeto de prueba $x_n$, los residuos estudiantizados $\alpha_i^y = \phi_i^y$ se calculan como:

\begin{equation}
    \alpha_i^y - \phi_i^y = B_i y - A_i, \quad i = 1, \ldots, n-1
\end{equation}

donde, en la notación de Vovk, $y$ es la etiqueta del $n$-ésimo objeto $x_n$ y:

\begin{align}
    B_i &:= \sqrt{1 - \bar{h}_n + \frac{\bar{h}_{i,n}}{\sqrt{1 - \bar{h}_i}}} \\
    A_i &:= \frac{\sum_{j=1}^{n-1} \bar{h}_{j,n} x_j}{{\sqrt{1 - \bar{h}_n}}} + \frac{y_i - \sum_{j=1}^{n-1} \bar{h}_{i,j} y_j}{\sqrt{1 - \bar{h}_i}}
\end{align}

Ordenando la secuencia de conformidad $C_1, \ldots, C_{n-1}$ en orden ascendente como $C_{(1)} \leq \cdots \leq C_{(n-1)}$, y definiendo $C_{(0)} := -\infty$ y $C_{(n)} := \infty$, la distribución predictiva se expresa como:

\begin{equation}
    \bar{\Pi}_n(y) := 
    \begin{cases}
        \left[\frac{i'}{n}, \frac{i'+1}{n}\right] & \text{si } y \in (C_{(i)}, C_{(i+1)}) \text{ para } i \in \{0, 1, \ldots, n-1\} \\
        \left[\frac{i'}{n}, \frac{i''}{n}\right] & \text{si } y = C_{(i)} \text{ para } i \in \{1, \ldots, n-1\}
    \end{cases}
\end{equation}

donde $i' := \min\{j : C_{(j)} = C_{(i)}\}$ y $i'' := \max\{j : C_{(j)} = C_{(i)}\}$.

Esta representación explícita revela que la distribución predictiva de la LSPM estudiantizada es una función escalonada con a lo sumo $n-1$ puntos de discontinuidad, cada uno correspondiente a un valor único de los puntajes de conformidad ordenados. El grosor de cada intervalo es igual a $1/n$, excepto en los puntos de empate donde varios puntajes de conformidad coinciden.

\section{Sistemas de Predicción Conformal de Mondrian (MCPS)}
\label{sec:mcps_profundo}

A pesar de las sólidas garantías de validez marginal que ofrecen los Sistemas de Predicción Conformal (CPS) descritos en la sección~\ref{sec:cps_densidades_LSPM}, estos presentan una limitación teórica y práctica fundamental: la garantía de error es un promedio sobre todo el espacio de datos. Esto implica que el sistema puede ser extremadamente preciso en ciertas regiones del espacio de características y, simultáneamente, cometer errores sistemáticos en otras, siempre que el error global no supere el nivel $\epsilon$. Los \textit{Sistemas de Predicción Conformal de Mondrian} (MCPS, por sus siglas en inglés) introducen el concepto de \textit{validez condicional por categorías}, permitiendo que la calibración se mantenga exacta dentro de subconjuntos específicos de los datos \parencite{Vovk2022}.

\subsection{Origen y Motivación: Validez Marginal vs. Condicional}

El apelativo ``Mondrian'' deriva del estilo geométrico del pintor neerlandés Piet Mondrian, cuya estética se fundamenta en la compartimentación del lienzo en rectángulos de colores puros delimitados por una cuadrícula, tal como se ilustra en la Figura~\ref{fig:analogia_mondrian}. Bajo esta analogía, un sistema de predicción conformal Mondriano particiona el espacio de ejemplos $\mathcal{Z}$ en categorías mutuamente excluyentes o taxonomías. Este enfoque permite que las garantías de cobertura sean válidas no solo de forma agregada, sino específicamente dentro de cada subgrupo definido, abordando así el problema de la validez condicional.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Imagenes/mondrian.png}
    \caption{Piet Mondrian (1872-1944). Composición con rojo, amarillo y azul. Dominio público.}
    \label{fig:mondrian}
\end{figure}


La necesidad de este enfoque surge cuando existen grupos de datos con dificultades predictivas heterogéneas. Por ejemplo, en una serie temporal de demanda eléctrica, predecir el consumo en un día festivo es intrínsecamente más difícil que en un día laboral. Un CPS global podría subestimar masivamente la incertidumbre en los días festivos, compensándola con una sobreestimación en los días laborales. El enfoque de Mondrian garantiza que la probabilidad de error sea exactamente $\epsilon$ tanto para los días laborales como para los festivos, de forma independiente \parencite{Vovk2017wp}.

\subsection{La Taxonomía de Mondrian ($\kappa$)}

La base matemática de un MCPS es la \textit{taxonomía}. Una taxonomía es una función medible $\kappa: \mathbb{N} \times (\mathbf{X} \times \mathbf{Y}) \to K$, donde $K$ es un conjunto numerable de categorías. Para cada par de ejemplo $(x_i, y_i)$ y su posición en la secuencia $i$, la taxonomía asigna una categoría $\kappa_i$.

Existen tres tipos principales de taxonomías aplicables a series temporales:
\begin{enumerate}
    \item \textbf{Taxonomías de Objetos:} dependen solo de las características $x_i$ (ej. agrupar por niveles de volatilidad observada).
    \item \textbf{Taxonomías de Etiquetas:} dependen de la respuesta $y_i$. Esto da lugar a los \textit{Label-Conditional Conformal Predictors}, vitales cuando el impacto de un error depende de la magnitud del valor (ej. errores en valores extremos son más costosos).
    \item \textbf{Taxonomías Temporales:} dependen del índice $i$. Este es el puente con el trabajo de Barber et al. \cite{Barber2023}, donde la categoría de Mondrian puede ser una ``ventana deslizante" de los datos más recientes para adaptarse a la no-intercambiabilidad.
\end{enumerate}

\subsection{Integración del Algoritmo MCPS}

La integración de la lógica de Mondrian en un Sistema de Predicción Conformal se realiza modificando el cálculo del p-valor o de la RPD (Distribución Predictiva Aleatorizada). En lugar de comparar el puntaje del nuevo ejemplo $\alpha_n$ con todos los puntajes históricos, solo se compara con aquellos que pertenecen a su misma categoría.

Sea $\sigma = \{z_1, \dots, z_{n-1}\}$ el conjunto de entrenamiento y $z_n = (x_n, y)$ el ejemplo de prueba con etiqueta hipotética $y$. El proceso para generar la RPD de Mondrian $\Pi_{M}$ es el siguiente:

\begin{enumerate}
    \item Se identifica la categoría del nuevo ejemplo: $k = \kappa(n, (x_n, y))$.
    \item Se filtran los índices de los ejemplos de entrenamiento que pertenecen a dicha categoría:
    \begin{equation}
        S_k = \{i \in \{1, \dots, n-1\} : \kappa(i, z_i) = k\}
    \end{equation}
    \item Se calculan los puntajes de conformidad $\alpha_i$ solo para $i \in S_k \cup \{n\}$.
    \item La RPD de Mondrian se define como:
    \begin{equation}
    \Pi_{M}(y, \tau) := \frac{|\{i \in S_k : \alpha_i < \alpha_n^y\}| + \tau |\{i \in S_k \cup \{n\} : \alpha_i = \alpha_n^y\}|}{|S_k| + 1}
    \label{eq:mondrian_rpd}
    \end{equation}
\end{enumerate}

El denominador $|S_k| + 1$ es clave: representa el tamaño de la ``muestra local". Si una categoría tiene pocos ejemplos, la distribución predictiva será naturalmente más dispersa (reflejando mayor incertidumbre), mientras que categorías ricas en datos producirán densidades más nítidas \cite{Vovk2022}.

\section{Vacío Metodológico: De Intervalos a Distribuciones en Series Temporales}
\label{sec:vacio_metodologico}

A pesar de los avances en predicción conformal y sus extensiones para dependencia temporal, existe un vacío metodológico fundamental: la mayoría de adaptaciones de CP para series temporales se concentran exclusivamente en intervalos de predicción, dejando prácticamente inexplorada la generación de distribuciones predictivas completas en este contexto.

\subsection{Estado Actual y Dimensiones del Vacío}

La predicción conformal clásica \parencite{Vovk2005} fue desarrollada para el contexto i.i.d. Las extensiones para series temporales, como el enfoque de ponderación de Barber et al. \parencite{Barber2023}, se enfocan en mantener garantías de cobertura para intervalos mediante esquemas adaptativos. Los Sistemas de Predicción Conformal (CPS) de la Sección~\ref{sec:cps_densidades_LSPM} y las taxonomías de Mondrian (Sección~\ref{sec:mcps_profundo}) representan el marco teórico más completo para distribuciones conformales, pero su aplicación se restringe casi exclusivamente a datos intercambiables.

Actualmente no existe una metodología consolidada que combine: (i) generación de distribuciones predictivas completas, (ii) adaptación a dependencia temporal, y (iii) garantías teóricas sobre calibración. Los métodos existentes presentan compensaciones: el \textit{Sieve Bootstrapping} \parencite{Lahiri2003} ofrece flexibilidad pero carece de garantías formales de calibración; \textit{DeepAR} \parencite{Salinas2020} muestra excelente desempeño empírico pero opera como caja negra sin fundamento teórico sobre cobertura.

El vacío identificado se manifiesta en tres dimensiones críticas. Primero, los intervalos de predicción proporcionan información limitada: en problemas de decisión bajo incertidumbre como optimización de inventarios o gestión de riesgos financieros, conocer únicamente los límites de un intervalo resulta insuficiente. La distribución predictiva completa permite evaluar probabilidades de eventos específicos, calcular valores esperados de funciones de pérdida asimétricas, y cuantificar riesgos extremos. Segundo, existe una desconexión entre fundamento teórico y práctica: mientras la teoría conformal proporciona garantías exactas bajo intercambiabilidad, las adaptaciones para dependencia temporal carecen de un marco unificado que caracterice rigurosamente el comportamiento bajo correlación temporal. Tercero, hay una carencia de evaluación comparativa sistemática: prácticamente no existen estudios que evalúen distribuciones conformales completas frente a distribuciones generadas por métodos establecidos utilizando métricas probabilísticas apropiadas.

\subsection{Contribución y Relevancia}

Esta tesis aborda directamente el vacío metodológico mediante tres contribuciones principales. Primero, desarrolla una extensión sistemática de los Sistemas de Predicción Conformal desde el contexto i.i.d. hacia series temporales, incorporando esquemas de ponderación temporal que manejan la no-intercambiabilidad mientras generan distribuciones predictivas completas. Segundo, establece un marco de evaluación comparativa riguroso basado en métricas probabilísticas como el \textit{Continuous Ranked Probability Score} (CRPS) y pruebas de significancia estadística, permitiendo comparar distribuciones conformales con métodos establecidos bajo criterios objetivos. Tercero, proporciona evidencia empírica mediante simulaciones controladas y aplicaciones a datos reales sobre el desempeño relativo de CP frente a alternativas como \textit{Sieve Bootstrapping} y \textit{DeepAR}.

La relevancia práctica de cerrar esta brecha es significativa. En pronóstico de demanda, la distribución completa permite optimizar niveles de stock considerando costos asimétricos. En gestión de riesgos energéticos, conocer la distribución predictiva de generación renovable facilita estrategias de cobertura. En meteorología operacional, distribuciones calibradas son fundamentales para sistemas de alerta temprana. Al ofrecer métodos conformales que generan distribuciones completas con propiedades de calibración controlables, este trabajo proporciona a los practicantes una alternativa fundamentada que combina garantías teóricas con aplicabilidad en problemas reales de pronóstico temporal.

\section{Análisis de la Consistencia Universal de Vovk}
\label{sec:sustento_teorico_vovk}

Para consolidar el marco teórico de esta investigación, es imperativo discutir el sustento matemático que garantiza que los Sistemas de Predicción Conformal (CPS) no solo son válidos en muestras finitas, sino también óptimos a medida que el volumen de datos aumenta. Este respaldo proviene de la demostración de la \textit{consistencia universal} de Vovk (\cite{Vovk2019}), formalizada en el Teorema 31 de su obra reciente.

\subsection{Definición de Consistencia Universal}

\subsection{Definición de Consistencia Universal}

Antes de proceder con la definición formal, es crucial aclarar la distinción entre dos propiedades fundamentales de los CPS: \textit{validez} y \textit{consistencia}. En el marco de predicción conformal, el término \textbf{validez} se refiere específicamente a la propiedad R2 establecida por \cite{Vovk2005}, que garantiza calibración probabilística automática en muestras finitas. Formalmente, un sistema predictivo aleatorizado cumple la propiedad de validez si, para cualquier nivel $\alpha \in [0,1]$ y cualquier medida de probabilidad $P$, la distribución de $Q$ evaluada en datos aleatorios $(z_1, \ldots, z_{n+1}) \sim P$ y $\tau \sim U[0,1]$ es uniforme:
\begin{equation}
    \forall \alpha \in [0,1]: \quad P\big(Q(z_1, \ldots, z_n, z_{n+1}, \tau) \leq \alpha\big) = \alpha
\end{equation}
Esta propiedad es más fuerte que la simple calibración marginal, pues no solo garantiza que $\mathbb{E}[Q(\cdot, y)] = P(y_{n+1} \leq y)$, sino que asegura que los p-valores conformales son uniformes e independientes para diferentes $n$, permitiendo aplicar la ley de los grandes números \parencite{Vovk2019}. A lo largo de este trabajo, usaremos ``validez'' exclusivamente en este sentido técnico.

Sin embargo, la validez por sí sola no garantiza que la distribución predictiva $\Pi_n$ sea una buena aproximación de la verdadera distribución condicional $P(y|x)$. Aquí es donde entra el concepto de \textit{consistencia universal}. Vovk define un sistema predictivo como \textit{universalmente consistente} si, para cualquier medida de probabilidad $P$ bajo el modelo IID y para cualquier función continua acotada $f$, se cumple que:
\begin{equation}
    \int f \, d\Pi_n - \mathbb{E}_P(f \mid x_{n+1}) \to 0 \quad \text{en probabilidad cuando } n \to \infty
\end{equation}

Esta propiedad implica que, asintóticamente, el CPS aproxima tan precisamente como se quiera la verdadera distribución generadora de los datos, eliminando la incertidumbre epistémica conforme el tamaño de la muestra $n$ tiende al infinito \parencite{Vovk2019}. 

\subsection{Mecanismo de la Demostración: El Enfoque de Histograma}

La prueba de Vovk sobre la existencia de un CPS consistente universalmente se apoya en la construcción de un \textit{Histogram Conformal Predictive System}. El argumento se divide en dos pilares fundamentales que vinculan la teoría de martingalas con la ley de los grandes números:

\begin{enumerate}
    \item \textbf{Teorema de Convergencia de Martingalas de Lévy:} Vovk utiliza particiones anidadas del espacio de objetos $X$ (celdas de histograma que se encogen conforme $n$ crece). Según el teorema de Lévy, la esperanza condicional de la función sobre una celda que se reduce tiende al valor puntual de la esperanza condicional en el objeto de prueba $x_{n+1}$ \parencite{Vovk2019}.
    
    \item \textbf{Ley de los Grandes Números (LGN):} Mientras las celdas se encogen para ganar resolución, el número de ejemplos dentro de cada celda debe tender a infinito ($nh_n \to \infty$, donde $h_n$ es el ancho de la celda). Esto permite que la frecuencia empírica de las etiquetas dentro de la categoría de Mondrian converja a la esperanza real en esa región del espacio \parencite{Vovk2019}.
\end{enumerate}

\subsection{La Distancia de Lévy y la Convergencia Débil}

Un punto crítico de la demostración es el uso de la noción de Belyaev sobre secuencias de distribuciones que se aproximan débilmente. Vovk demuestra que bajo un CPS universal, la \textit{Distancia de Lévy} entre la distribución predictiva conformal y la verdadera distribución condicional converge a cero en probabilidad \cite{Vovk2019}. 

Este resultado es el que otorga rigor a la aplicación de CPS en problemas de alta criticidad, como el pronóstico de carga eléctrica o la gestión de riesgos financieros. Indica que el analista no tiene que elegir entre un modelo ``seguro" (conformal) y un modelo ``preciso" (bayesiando/paramétrico); el CPS universal ofrece ambas ventajas simultáneamente:
\begin{itemize}
    \item \textbf{A corto plazo:} Garantiza cobertura exacta mediante calibración fuerte.
    \item \textbf{A largo plazo:} Garantiza convergencia a la distribución real de los datos sin requerir asunciones paramétricas.
\end{itemize}

\subsection{Implicaciones para el LSPM y la Eficiencia}

Aunque el modelo de mínimos cuadrados (LSPM) estudiado en la sección~\ref{sec:cps_densidades_LSPM} es eficiente bajo ruido gaussiano, Vovk advierte que no es universalmente consistente si la relación real entre $X$ y $Y$ no es lineal \parencite{Vovk2005}. Por ello, el desarrollo de CPS basados en kernels o en métodos de vecinos cercanos (como se discute en el capítulo 4 de su obra) es lo que permite alcanzar la consistencia universal en espacios de características complejos. Esta conclusión justifica el uso de arquitecturas no lineales conformizadas en la presente tesis, ya que heredan la solidez de la prueba de consistencia de Vovk.

\section{Hacia la Consistencia Universal en Series de Tiempo Ergódicas}
\label{sec:extension_ergodica}

Si bien el trabajo de Vovk (\cite{Vovk2019}) establece una base sólida para la consistencia universal bajo el modelo IID, las aplicaciones en entornos reales, como las series de tiempo, exigen una transición hacia modelos que capturen la dependencia temporal. Durante el desarrollo de esta investigación, se intentó abordar el problema teórico de probar las condiciones formales bajo las cuales un Sistema de Predicción Conformal (CPS) podría alcanzar consistencia universal en el contexto de procesos estocásticos dependientes, donde la suposición de intercambiabilidad no se cumple.

Sin embargo, por cuestiones de alcance y limitaciones temporales, este esfuerzo resultó en la identificación de una ruta teórica que plantea los supuestos fundamentales, la arquitectura propuesta y las direcciones necesarias para una demostración futura. Los detalles completos de este marco conceptual —incluyendo la redefinición del objetivo de consistencia, los supuestos de estacionariedad y $\alpha$-mixing, y el mecanismo propuesto de transductor conformal por kernel— se presentan en el Apéndice~\ref{apx:consistencia_ergodica} como una guía para investigaciones posteriores que busquen extender los resultados de Vovk al dominio de series de tiempo ergódicas.

A pesar de no contar con una prueba formal completa, la identificación de estas condiciones ideales (ergodicidad, mezcla fuerte con decaimiento algebraico, y regularidad de Lipschitz) constituye un aporte conceptual que orienta el diseño empírico de los métodos implementados en los capítulos subsecuentes de esta tesis, particularmente en la selección de ventanas temporales móviles y esquemas de ponderación espacial por kernel.